{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment on Generated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to design an experiment to check if multi-user and multi-label (which is what our dataset looks like) is a problem for SSVM.   \n",
    "To chieve this goal,\n",
    "1. a trained SSVM $\\mathcal{M}_0$ (on Glasgow dataset $\\mathcal{D}_0$) is used to generate a single user, single label dataset $\\mathcal{D}_1$. Concretely, we predict a trajectory for every query $(p, l), p \\in \\mathcal{P}, l \\in \\{3,4,5,6,7\\}$ use $\\mathcal{M}_0$, where $\\mathcal{P}$ is from $\\mathcal{D}_0$.\n",
    "1. train a new SSVM $\\mathcal{M}_1$ using features (POI and transition features) computed from $\\mathcal{D}_0$ and labels from $\\mathcal{D}_1$, and check the performance on training set (i.e., $\\mathcal{D}_1$).\n",
    "1. perform leave-one-out cross validation on $\\mathcal{D}_1$. Hyperparameter (i.e., $C$) is determined by trying some numbers when holding one label in $\\mathcal{D}_1$ as test example and using all other labels in $\\mathcal{D}_1$ as training set (POI and transition features are computed from $\\mathcal{D}_0$), then fix the $C$ for all leave-one-out cross validations.\n",
    "1. we noted that POI and transition features are computed from $\\mathcal{D}_0$ and labels are from $\\mathcal{D}_1$, as we can't compute the duration related features (i.e., avgDuration for POI, and log transition probability between discretized duration buckets) on $\\mathcal{D}_1$ as no duration information is generated. So we try to disable duration related features one-by-one, and perform step $3$ to check whether duration related features help.\n",
    "1. if duration related features don't help, we can turn off them and then compute POI and transition features from $\\mathcal{D}_1$ and use labels in $\\mathcal{D}_1$, then we want to compare the performance of RankSVM and SSVM on $\\mathcal{D}_1$ (using leave-one-out cross validation), if SSVM performans better than RankSVM, it means multi-user and multi-label in our dataset is a problem for SSVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, pickle, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cvxopt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(1234554321)\n",
    "np.random.seed(123456789)\n",
    "cvxopt.base.setseed(123456789)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run notebook ```ssvm.ipynb```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 'ssvm_ml.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dump_vars = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Generate new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load trained parameters and prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fname = os.path.join(data_dir, 'ssvm-listViterbi-Glas.pkl')\n",
    "fname = os.path.join(data_dir, 'ssvm-listViterbi-Osak.3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssvm_lv = pickle.load(open(fname, 'rb'))  # a dict: query -> {'PRED': trajectory, 'C': ssvm-c, 'W': model_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = (2, 5)\n",
    "#query = (5, 4)\n",
    "W = ssvm_lv[query]['W']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssvm_lv[query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#W = np.random.randn(W.shape[0])  # Use a random weight vector to generate trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script false\n",
    "trajid_set = set(trajid_set_all) - TRAJ_GROUP_DICT[query]\n",
    "poi_set = {p for tid in trajid_set for p in traj_dict[tid] if len(traj_dict[tid]) >= 2}\n",
    "poi_list = sorted(poi_set)\n",
    "n_states = len(poi_set)\n",
    "n_edge_features = 5\n",
    "n_node_features = (len(W) - n_states * n_states * n_edge_features) // n_states\n",
    "#print(len(W), n_states, n_node_features)\n",
    "#unary_params = W[:n_states * n_node_features].reshape(n_states, n_node_features)\n",
    "#pw_params = W[n_states * n_node_features:].reshape((n_states, n_states, n_edge_features)) \n",
    "unary_params = W[:-n_edge_features]\n",
    "pw_params = W[-n_edge_features:].reshape(n_edge_features)\n",
    "# duplicate params so that inference procedures work the same way no matter params shared or not\n",
    "unary_params = np.tile(unary_params, (n_states, 1))\n",
    "pw_params = np.tile(pw_params, (n_states, n_states, 1))\n",
    "\n",
    "poi_id_dict, poi_id_rdict = dict(), dict()\n",
    "for idx, poi in enumerate(poi_list):\n",
    "    poi_id_dict[poi] = idx\n",
    "    poi_id_rdict[idx] = poi\n",
    "    \n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(poi_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute feature scaling parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script false\n",
    "poi_info = calc_poi_info(sorted(trajid_set), traj_all, poi_all)\n",
    "\n",
    "traj_list = [traj_dict[k] for k in sorted(trajid_set) if len(traj_dict[k]) >= 2]\n",
    "node_features_list = Parallel(n_jobs=N_JOBS)\\\n",
    "                     (delayed(calc_node_features)\\\n",
    "                      (tr[0], len(tr), poi_list, poi_info.copy(), poi_clusters=POI_CLUSTERS, \\\n",
    "                       cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST) for tr in traj_list)\n",
    "edge_features = calc_edge_features(list(trajid_set), poi_list, traj_dict, poi_info.copy())\n",
    "fdim = node_features_list[0].shape\n",
    "X_node_all = np.vstack(node_features_list)\n",
    "#scaler = MaxAbsScaler(copy=False)\n",
    "scaler = MinMaxScaler(feature_range=(-1,1), copy=False)\n",
    "scaler.fit(X_node_all)\n",
    "\n",
    "# edge feature scaling\n",
    "scaler_edge = MinMaxScaler(feature_range=(-1,1), copy=False)\n",
    "fdim_edge = edge_features.shape\n",
    "edge_features = scaler_edge.fit_transform(edge_features.reshape(fdim_edge[0]*fdim_edge[1], -1))\n",
    "edge_features = edge_features.reshape(fdim_edge)\n",
    "    \n",
    "\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(poi_info.shape)\n",
    "print(edge_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi, L = query\n",
    "X_node_test = calc_node_features(poi, L, poi_list, poi_info.copy(), poi_clusters=POI_CLUSTERS, \n",
    "                                 cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "X_node_test = scaler.transform(X_node_test)  # feature scaling\n",
    "unary_features = X_node_test\n",
    "pw_features = edge_features.copy()\n",
    "y_pred = do_inference_listViterbi(poi_id_dict[poi], L, len(poi_set), \n",
    "                                  unary_params, pw_params, unary_features, pw_features)\n",
    "print([poi_id_rdict[p] for p in y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat = [2, 1, 6, 21, 20]\n",
    "y_hat = [2, 1, 6, 20, 21]\n",
    "#y_hat = [5, 7, 8, 6]\n",
    "#y_hat = [5, 8, 7, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 0\n",
    "y = [poi_id_dict[x] for x in y_hat]\n",
    "for j in range(len(y)-1):\n",
    "    ss = y[j]\n",
    "    tt = y[j+1]\n",
    "    score += np.dot(pw_params[ss, tt], pw_features[ss, tt])\n",
    "    score += np.dot(unary_params[tt], unary_features[tt])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[traj_dict[x] for x in TRAJ_GROUP_DICT[query]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script false\n",
    "lengthes = [3, 4, 5, 6]#, 7]\n",
    "fake_labels = []\n",
    "for poi in sorted(poi_list):\n",
    "    for L in lengthes:\n",
    "        X_node_test = calc_node_features(poi, L, poi_list, poi_info.copy(), poi_clusters=POI_CLUSTERS, \\\n",
    "                                         cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "        X_node_test = scaler.transform(X_node_test)  # feature scaling\n",
    "        unary_features = X_node_test\n",
    "        pw_features = edge_features.copy()\n",
    "        y_pred = do_inference_listViterbi(poi_id_dict[poi], L, len(poi_set), \n",
    "                                          unary_params, pw_params, unary_features, pw_features)\n",
    "        fake_labels.append([poi_id_rdict[p] for p in y_pred])\n",
    "\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fake_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fname = 'fake_labels.pkl'\n",
    "#if dump_vars == True: pickle.dump(fake_labels, open(fname, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vars_equal(pickle.load(open(fname, 'rb')), fake_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Train SSVM on generated dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing scaling parameters and training features/labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_train_data(train_labels, poi_list, poi_info, edge_features, poi_id_dict):\n",
    "    node_features_all = Parallel(n_jobs=N_JOBS)\\\n",
    "                        (delayed(calc_node_features)\\\n",
    "                         (tr[0], len(tr), poi_list, poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                          cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST) for tr in train_labels)\n",
    "    fdim_train = node_features_all[0].shape\n",
    "    X_node_train = np.vstack(node_features_all)\n",
    "    scaler_node = MinMaxScaler(feature_range=(-1,1), copy=False)\n",
    "    X_node_train = scaler_node.fit_transform(X_node_train)\n",
    "    X_node_train = X_node_train.reshape(-1, fdim_train[0], fdim_train[1])\n",
    "    \n",
    "    assert(len(train_labels) == X_node_train.shape[0])\n",
    "    X_train = [(X_node_train[k, :, :], edge_features.copy(), \n",
    "                (poi_id_dict[train_labels[k][0]], len(train_labels[k]))) for k in range(len(train_labels))]\n",
    "    y_train = [np.array([poi_id_dict[k] for k in tr]) for tr in train_labels]\n",
    "    assert(len(X_train) == len(y_train))\n",
    "    \n",
    "    return X_train, y_train, scaler_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ssvm(X_train, y_train, C):\n",
    "    sm = MyModel(inference_fun=do_inference_listViterbi)\n",
    "    osssvm = OneSlackSSVM(model=sm, C=C, n_jobs=N_JOBS, verbose=0)\n",
    "    try:\n",
    "        osssvm.fit(X_train, y_train, initialize=True)\n",
    "        print('SSVM training finished.')\n",
    "    except:\n",
    "        sys.stderr.write('SSVM training FAILED.\\n')\n",
    "    return osssvm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the primal dual objective value curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_obj_curve(ssvm):\n",
    "    plt.plot(ssvm.objective_curve_, label='dual')\n",
    "    plt.plot(ssvm.primal_objective_curve_, label='primal')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(ssvm, ps, L, poi_list, poi_info, edge_features, scaler_node, poi_id_dict, poi_id_rdict):\n",
    "    X_node_test = calc_node_features(ps, L, poi_list, poi_info, poi_clusters=POI_CLUSTERS, \n",
    "                                     cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "    X_node_test = scaler_node.transform(X_node_test)\n",
    "    X_test = [(X_node_test, edge_features, (poi_id_dict[ps], L))]\n",
    "    y_hat = ssvm.predict(X_test)\n",
    "    return np.array([poi_id_rdict[p] for p in y_hat[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(predictions):\n",
    "    F1_ssvm = []; pF1_ssvm = []; tau_ssvm = []\n",
    "    for key in sorted(predictions.keys()):\n",
    "        F1 = calc_F1(predictions[key]['REAL'], predictions[key]['PRED'])\n",
    "        pF1 = calc_pairsF1(predictions[key]['REAL'], predictions[key]['PRED'])\n",
    "        tau = calc_kendalltau(predictions[key]['REAL'], predictions[key]['PRED'])\n",
    "        F1_ssvm.append(F1); pF1_ssvm.append(pF1); tau_ssvm.append(tau)\n",
    "    F1_mean = np.mean(F1_ssvm); pF1_mean = np.mean(pF1_ssvm); tau_mean = np.mean(tau_ssvm)\n",
    "    print('F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f), Tau (%.3f, %.3f)' % \\\n",
    "          (F1_mean, np.std(F1_ssvm)/np.sqrt(len(F1_ssvm)), \\\n",
    "           pF1_mean, np.std(pF1_ssvm)/np.sqrt(len(pF1_ssvm)), \\\n",
    "           tau_mean, np.std(tau_ssvm)/np.sqrt(len(tau_ssvm))))\n",
    "    return F1_mean, pF1_mean, tau_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#C = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "train_labels = fake_labels.copy()\n",
    "X_train, y_train, scaler_node = calc_train_data(train_labels, poi_list, poi_info.copy(), \n",
    "                                                edge_features.copy(), poi_id_dict.copy())\n",
    "ssvm = train_ssvm(X_train, y_train, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_obj_curve(ssvm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Evaluate on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "predictions = dict()\n",
    "for label in train_labels:\n",
    "    y_pred = predict(ssvm, label[0], len(label), poi_list, poi_info.copy(), edge_features.copy(),\n",
    "                     scaler_node, poi_id_dict, poi_id_rdict)\n",
    "    predictions[(label[0], len(label))] = {'PRED': y_pred, 'REAL': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "ret = evaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Leave-one-out evaluation on generated dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FOR STEP 4:** Turn off duration related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poi_info['avgDuration'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition features: [poiCat, popularity, nVisit, avgDuration, clusterID]\n",
    "#edge_features = edge_features[:, :, [0,1,2,4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose hyper-parameter C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose hyper-parameter C using Monte-Carlo cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "num_test = int(len(fake_labels) * MC_PORTION)\n",
    "best_tau = 0; best_C = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#edge_features = np.zeros_like(edge_features)  # Turn off transition features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "np.random.seed(0)\n",
    "for C in C_SET:\n",
    "    print('\\n--------------- try_C: %f ---------------\\n' % C); sys.stdout.flush() \n",
    "    F1_test = []; pF1_test = []; tau_test = []\n",
    "    for t in range(MC_NITER):\n",
    "        while True:\n",
    "            indices = np.arange(len(fake_labels))\n",
    "            np.random.shuffle(indices)\n",
    "            test_ix = indices[:num_test]\n",
    "            train_ix = indices[num_test:]\n",
    "            train_labels = [fake_labels[ix] for ix in train_ix]\n",
    "            test_labels  = [fake_labels[ix] for ix in test_ix]\n",
    "            poi_set_ = {p for x in train_labels for p in x}\n",
    "            if len(poi_set_) == len(poi_list): break\n",
    "        X_train, y_train, scaler_node = calc_train_data(train_labels, poi_list, poi_info.copy(), \n",
    "                                                        edge_features.copy(), poi_id_dict.copy())\n",
    "        ssvm = train_ssvm(X_train, y_train, C)\n",
    "        predictions = dict()\n",
    "        for label in test_labels:\n",
    "            y_pred = predict(ssvm, label[0], len(label), poi_list, poi_info.copy(), edge_features.copy(), \n",
    "                             scaler_node, poi_id_dict, poi_id_rdict)\n",
    "            predictions[(label[0], len(label))] = {'PRED': y_pred, 'REAL': label}\n",
    "        F1, pF1, tau = evaluation(predictions)\n",
    "        F1_test.append(F1); pF1_test.append(pF1); tau_test.append(tau)\n",
    "    mean_tau = np.mean(tau_test)\n",
    "    print('mean_tau: %.3f' % mean_tau)\n",
    "    if mean_tau > best_tau:\n",
    "        best_tau = mean_tau\n",
    "        best_C = C\n",
    "print('\\nbest_tau: %.3f, best_C: %.3f' % (best_tau, best_C))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-one-out cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "predictions = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "for i in range(len(fake_labels)):\n",
    "    sys.stdout.write('%s ' % str(i+1))\n",
    "    train_labels = fake_labels[:i] + fake_labels[i+1:]\n",
    "    X_train, y_train, scaler_node = calc_train_data(train_labels, poi_list, poi_info.copy(), \n",
    "                                                    edge_features.copy(), poi_id_dict.copy())\n",
    "    ssvm = train_ssvm(X_train, y_train, best_C)\n",
    "    test_label = fake_labels[i]\n",
    "    y_pred = predict(ssvm, test_label[0], len(test_label), poi_list, poi_info.copy(), edge_features.copy(), \n",
    "                     scaler_node, poi_id_dict, poi_id_rdict)\n",
    "    predictions[(test_label[0], len(test_label))] = {'PRED': y_pred, 'REAL': test_label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "ret = evaluation(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "fname = 'ssvm-orig-feature.pkl'\n",
    "if dump_vars == True: pickle.dump(predictions, open(fname, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "vars_equal(pickle.load(open(fname, 'rb')), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Check the informative of duration related features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn off duration related features one-by-one, and perform [step 3](#Step-3---Leave-one-out-evaluation-on-generated-dataset) to check whether duration related features help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concretely, disable duration related POI and transition features in [step 1](#Step-1---Generate-new-dataset) one-by-one, and run step 1 to step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Compute POI and transition features on the generated data: SSVM vs. RankSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- turn off duration related features.\n",
    "- recall that POI popularity is the number of distinct users that visited the POI, and we have only one user in $\\mathcal{D}_1$, which means all POIs have a popularity $1$ and transition between different popularity buckets (only one bucket here) is meaningless.\n",
    "- recompute the transition probabilities between nVisit buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transmat_visit0, logbins_visit0 = gen_transmat_visit(trajid_set, traj_dict, poi_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "poi_info_new = calc_poi_info(sorted(trajid_set), traj_all, poi_all)\n",
    "edge_features_new = calc_edge_features(list(trajid_set), poi_list, traj_dict, poi_info_new.copy())\n",
    "\n",
    "# set POI popularity and nvisit\n",
    "poi_info_new['avgDuration'] = 0.0\n",
    "poi_info_new['popularity'] = 1  # only a single user\n",
    "poi_info_new['nVisit'] = 0\n",
    "for label in fake_labels:\n",
    "    for p in label: poi_info_new.loc[p, 'nVisit'] += 1\n",
    "        \n",
    "# set popularity (drop it) and nvisit based transition features\n",
    "\n",
    "# compute binning boundaries\n",
    "poi_visits = poi_info_new.loc[poi_list, 'nVisit']\n",
    "expo_visit1 = np.log10(max(1, min(poi_visits)))\n",
    "expo_visit2 = np.log10(max(poi_visits))\n",
    "nbins_visit = BIN_CLUSTER\n",
    "logbins_visit = np.logspace(np.floor(expo_visit1), np.ceil(expo_visit2), nbins_visit+1)\n",
    "logbins_visit[0] = 0  # deal with underflow\n",
    "if not (logbins_visit[-1] > poi_info_new['nVisit'].max()):\n",
    "    logbins_visit[-1] = poi_info_new['nVisit'].max() + 1\n",
    "\n",
    "# compute transition matrix between different nVist buckets\n",
    "nbins = len(logbins_visit) - 1\n",
    "transmat_visit_cnt = pd.DataFrame(data=np.zeros((nbins, nbins), dtype=np.float),\n",
    "                                  columns=np.arange(1, nbins+1), index=np.arange(1, nbins+1))\n",
    "for t in fake_labels:\n",
    "    for pi in range(len(t)-1):\n",
    "        p1, p2 = t[pi], t[pi+1]\n",
    "        assert(p1 in poi_info_new.index and p2 in poi_info_new.index)\n",
    "        visit1 = poi_info_new.loc[p1, 'nVisit']\n",
    "        visit2 = poi_info_new.loc[p2, 'nVisit']\n",
    "        vc1, vc2 = np.digitize([visit1, visit2], logbins_visit)\n",
    "        if vc1 > 5 or vc2 > 5: print(p1, visit1, p2, visit2)\n",
    "        transmat_visit_cnt.loc[vc1, vc2] += 1\n",
    "transmat_visit = normalise_transmat(transmat_visit_cnt)\n",
    "\n",
    "# compute nvisit based transition features\n",
    "poi_features = pd.DataFrame(data=np.zeros((len(poi_list), 1)), columns=['nVisit'], index=poi_list)\n",
    "poi_features.index.name = 'poiID'\n",
    "poi_features['nVisit'] = np.digitize(poi_info_new.loc[poi_list, 'nVisit'], logbins_visit)\n",
    "for j in range(len(poi_list)): # NOTE: POI order\n",
    "    pj = poi_list[j]\n",
    "    visit = poi_features.loc[pj, 'nVisit']    \n",
    "    for k in range(len(poi_list)): # NOTE: POI order\n",
    "        pk = poi_list[k]\n",
    "        #edge_features_new[j, k, 2] = np.log10(transmat_visit.loc[visit, poi_features.loc[pk, 'nVisit']])\n",
    "        edge_features_new[j, k, 2] = transmat_visit.loc[visit, poi_features.loc[pk, 'nVisit']]\n",
    "\n",
    "# transition features: [poiCat, popularity, nVisit, avgDuration, clusterID]\n",
    "edge_features_new = edge_features_new[:, :, [0, 1, 2, 4]]\n",
    "\n",
    "# edge feature scaling\n",
    "scaler_edge = MinMaxScaler(feature_range=(-1,1), copy=False)\n",
    "fdim_edge = edge_features_new.shape\n",
    "edge_features_new = scaler_edge.fit_transform(edge_features_new.reshape(fdim_edge[0]*fdim_edge[1], -1))\n",
    "edge_features_new = edge_features_new.reshape(fdim_edge)\n",
    "    \n",
    "\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.heatmap(np.log10(transmat_visit0), cmap='BuGn')#, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.heatmap(np.log10(transmat_visit), cmap='BuGn')#, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histograms of the number of visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(logbins_visit0)\n",
    "#poi_info['nVisit'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(logbins_visit)\n",
    "#poi_info_new['nVisit'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose hyper-parameter C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "edge_features_new = np.zeros_like(edge_features_new) # Turn off transition features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "num_test = int(len(fake_labels) * MC_PORTION)\n",
    "best_tau = 0; best_C = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "np.random.seed(0)\n",
    "for C in C_SET:\n",
    "    print('\\n--------------- try_C: %f ---------------\\n' % C); sys.stdout.flush() \n",
    "    F1_test = []; pF1_test = []; tau_test = []\n",
    "    for t in range(MC_NITER):\n",
    "        while True:\n",
    "            indices = np.arange(len(fake_labels))\n",
    "            np.random.shuffle(indices)\n",
    "            test_ix = indices[:num_test]\n",
    "            train_ix = indices[num_test:]\n",
    "            train_labels = [fake_labels[ix] for ix in train_ix]\n",
    "            test_labels  = [fake_labels[ix] for ix in test_ix]\n",
    "            poi_set_ = {p for x in train_labels for p in x}\n",
    "            if len(poi_set_) == len(poi_list): break\n",
    "        X_train, y_train, scaler_node = calc_train_data(train_labels, poi_list, poi_info_new.copy(), \n",
    "                                                        edge_features_new.copy(), poi_id_dict.copy())\n",
    "        ssvm = train_ssvm(X_train, y_train, C)\n",
    "        predictions = dict()\n",
    "        for label in test_labels:\n",
    "            y_pred = predict(ssvm, label[0], len(label), poi_list, poi_info_new.copy(), edge_features_new.copy(), \n",
    "                             scaler_node, poi_id_dict, poi_id_rdict)\n",
    "            predictions[(label[0], len(label))] = {'PRED': y_pred, 'REAL': label}\n",
    "        F1, pF1, tau = evaluation(predictions)\n",
    "        F1_test.append(F1); pF1_test.append(pF1); tau_test.append(tau)\n",
    "    tau_mean = np.mean(tau_test)\n",
    "    print('mean_tau: %.3f' % tau_mean)\n",
    "    if tau_mean > best_tau:\n",
    "        best_tau = tau_mean\n",
    "        best_C = C\n",
    "print('\\nbest_tau: %.3f, best_C: %.3f' % (best_tau, best_C))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-one-out cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "predictions = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "for i in range(len(fake_labels)):\n",
    "    sys.stdout.write('%s ' % str(i+1))\n",
    "    train_labels = fake_labels[:i] + fake_labels[i+1:]\n",
    "    X_train, y_train, scaler_node = calc_train_data(train_labels, poi_list, poi_info_new.copy(), \n",
    "                                                    edge_features_new.copy(), poi_id_dict.copy())\n",
    "    ssvm = train_ssvm(X_train, y_train, best_C)\n",
    "    test_label = fake_labels[i]\n",
    "    y_pred = predict(ssvm, test_label[0], len(test_label), poi_list, poi_info_new.copy(), edge_features_new.copy(), \n",
    "                     scaler_node, poi_id_dict, poi_id_rdict)\n",
    "    predictions[(test_label[0], len(test_label))] = {'PRED': y_pred, 'REAL': test_label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "ret = evaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of RankSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run 'baseline.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_train_df_new(train_labels, poi_list, poi_info, query_id_dict, poi_clusters, cats, clusters, n_jobs=-1):    \n",
    "    columns = DF_COLUMNS\n",
    "    poi_distmat = POI_DISTMAT\n",
    "    train_trajs = train_labels    \n",
    "    \n",
    "    qid_set = sorted(set(query_id_dict.values()))    \n",
    "    query_id_rdict = dict()\n",
    "    for k, v in query_id_dict.items(): \n",
    "        query_id_rdict[v] = k  # qid --> (start, length)\n",
    "    \n",
    "    train_df_list = Parallel(n_jobs=n_jobs)\\\n",
    "                            (delayed(gen_train_subdf_new)(poi, qid_set, poi_info, poi_clusters,\n",
    "                                                          cats,clusters,query_id_rdict) for poi in poi_list)\n",
    "                        \n",
    "    assert(len(train_df_list) > 0)\n",
    "    df_ = train_df_list[0]\n",
    "    for j in range(1, len(train_df_list)):\n",
    "        df_ = df_.append(train_df_list[j], ignore_index=True)            \n",
    "        \n",
    "    # set label\n",
    "    df_.set_index(['queryID', 'poiID'], inplace=True)\n",
    "    df_['label'] = 0\n",
    "    for t in train_trajs:\n",
    "        qid = query_id_dict[(t[0], len(t))]\n",
    "        for poi in t[1:]:  # do NOT count if the POI is startPOI/endPOI\n",
    "            df_.loc[(qid, poi), 'label'] += 1\n",
    "\n",
    "    df_.reset_index(inplace=True)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_train_subdf_new(poi_id, query_id_set, poi_info, poi_clusters, cats, clusters, query_id_rdict):\n",
    "    assert(isinstance(cats, list))\n",
    "    assert(isinstance(clusters, list))\n",
    "    \n",
    "    columns = DF_COLUMNS\n",
    "    poi_distmat = POI_DISTMAT\n",
    "    df_ = pd.DataFrame(index=np.arange(len(query_id_set)), columns=columns)\n",
    "    \n",
    "    pop, nvisit = poi_info.loc[poi_id, 'popularity'], poi_info.loc[poi_id, 'nVisit']\n",
    "    cat, cluster = poi_info.loc[poi_id, 'poiCat'], poi_clusters.loc[poi_id, 'clusterID'] \n",
    "    duration = poi_info.loc[poi_id, 'avgDuration']\n",
    "    \n",
    "    for j in range(len(query_id_set)):\n",
    "        qid = query_id_set[j]\n",
    "        assert(qid in query_id_rdict) # qid --> (start, end, length)\n",
    "        (p0, trajLen) = query_id_rdict[qid]\n",
    "        idx = df_.index[j]\n",
    "        df_.loc[idx, 'poiID'] = poi_id\n",
    "        df_.loc[idx, 'queryID'] = qid\n",
    "        df_.set_value(idx, 'category', tuple((cat == np.array(cats)).astype(np.int) * 2 - 1))\n",
    "        df_.set_value(idx, 'neighbourhood', tuple((cluster == np.array(clusters)).astype(np.int) * 2 - 1))\n",
    "        df_.loc[idx, 'popularity'] = LOG_SMALL if pop < 1 else np.log10(pop)\n",
    "        df_.loc[idx, 'nVisit'] = LOG_SMALL if nvisit < 1 else np.log10(nvisit)\n",
    "        df_.loc[idx, 'avgDuration'] = LOG_SMALL if duration < 1 else np.log10(duration)\n",
    "        df_.loc[idx, 'trajLen'] = trajLen\n",
    "        df_.loc[idx, 'sameCatStart'] = 1 if cat == poi_info.loc[p0, 'poiCat'] else -1\n",
    "        df_.loc[idx, 'distStart'] = poi_distmat.loc[poi_id, p0]\n",
    "        df_.loc[idx, 'diffPopStart'] = pop - poi_info.loc[p0, 'popularity']\n",
    "        df_.loc[idx, 'diffNVisitStart'] = nvisit - poi_info.loc[p0, 'nVisit']\n",
    "        df_.loc[idx, 'diffDurationStart'] = duration - poi_info.loc[p0, 'avgDuration']\n",
    "        df_.loc[idx, 'sameNeighbourhoodStart'] = 1 if cluster == poi_clusters.loc[p0, 'clusterID'] else -1\n",
    "        \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_test_df_new(startPOI, nPOI, poi_info, query_id_dict, poi_clusters, cats, clusters):\n",
    "    assert(isinstance(cats, list))\n",
    "    assert(isinstance(clusters, list))\n",
    "    \n",
    "    columns = DF_COLUMNS\n",
    "    poi_distmat = POI_DISTMAT\n",
    "    \n",
    "    key = (p0, trajLen) = (startPOI, nPOI)\n",
    "    assert(key in query_id_dict)\n",
    "    assert(p0 in poi_info.index)\n",
    "    \n",
    "    df_ = pd.DataFrame(index=np.arange(poi_info.shape[0]), columns=columns)\n",
    "    poi_list = sorted(poi_info.index)\n",
    "    \n",
    "    qid = query_id_dict[key]\n",
    "    df_['queryID'] = qid\n",
    "    df_['label'] = np.random.rand(df_.shape[0]) # label for test data is arbitrary according to libsvm FAQ\n",
    "\n",
    "    for i in range(df_.index.shape[0]):\n",
    "        poi = poi_list[i]\n",
    "        lon, lat = poi_info.loc[poi, 'poiLon'], poi_info.loc[poi, 'poiLat']\n",
    "        pop, nvisit = poi_info.loc[poi, 'popularity'], poi_info.loc[poi, 'nVisit']\n",
    "        cat, cluster = poi_info.loc[poi, 'poiCat'], poi_clusters.loc[poi, 'clusterID']\n",
    "        duration = poi_info.loc[poi, 'avgDuration']\n",
    "        idx = df_.index[i]\n",
    "        df_.loc[idx, 'poiID'] = poi\n",
    "        df_.set_value(idx, 'category', tuple((cat == np.array(cats)).astype(np.int) * 2 - 1))\n",
    "        df_.set_value(idx, 'neighbourhood', tuple((cluster == np.array(clusters)).astype(np.int) * 2 - 1))\n",
    "        df_.loc[idx, 'popularity'] = LOG_SMALL if pop < 1 else np.log10(pop)\n",
    "        df_.loc[idx, 'nVisit'] = LOG_SMALL if nvisit < 1 else np.log10(nvisit)\n",
    "        df_.loc[idx, 'avgDuration'] = LOG_SMALL if duration < 1 else np.log10(duration)\n",
    "        df_.loc[idx, 'trajLen'] = trajLen\n",
    "        df_.loc[idx, 'sameCatStart'] = 1 if cat == poi_all.loc[p0, 'poiCat'] else -1\n",
    "        df_.loc[idx, 'distStart'] = poi_distmat.loc[poi, p0]\n",
    "        df_.loc[idx, 'diffPopStart'] = pop - poi_info.loc[p0, 'popularity']\n",
    "        df_.loc[idx, 'diffNVisitStart'] = nvisit - poi_info.loc[p0, 'nVisit']\n",
    "        df_.loc[idx, 'diffDurationStart'] = duration - poi_info.loc[p0, 'avgDuration']\n",
    "        df_.loc[idx, 'sameNeighbourhoodStart'] = 1 if cluster == poi_clusters.loc[p0, 'clusterID'] else -1\n",
    "        \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "num_test = int(len(fake_labels) * 0.2)\n",
    "best_tau = 0; best_C = 0\n",
    "query_id_dict = {(tr[0], len(tr)): ix for ix, tr in enumerate(fake_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#poi_info_new = calc_poi_info(sorted(trajid_set), traj_all, poi_all)  # Compute features on the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "np.random.seed(0)\n",
    "for C in C_SET:\n",
    "    print('\\n--------------- try_C: %f ---------------\\n' % C); sys.stdout.flush() \n",
    "    F1_test = []; pF1_test = []; tau_test = []\n",
    "    for t in range(MC_NITER):\n",
    "        while True:\n",
    "            indices = np.arange(len(fake_labels))\n",
    "            np.random.shuffle(indices)\n",
    "            test_ix = indices[:num_test]\n",
    "            train_ix = indices[num_test:]\n",
    "            train_labels = [fake_labels[ix] for ix in train_ix]\n",
    "            test_labels  = [fake_labels[ix] for ix in test_ix]\n",
    "            poi_set_ = {p for x in train_labels for p in x}\n",
    "            if len(poi_set_) == len(poi_list): break\n",
    "        train_df = gen_train_df_new(train_labels, poi_list, poi_info_new.copy(), query_id_dict.copy(), \n",
    "                                    poi_clusters=POI_CLUSTERS,cats=POI_CAT_LIST,clusters=POI_CLUSTER_LIST,n_jobs=N_JOBS)\n",
    "        ranksvm = RankSVM(ranksvm_dir, useLinear=True)\n",
    "        ranksvm.train(train_df, cost=C)\n",
    "        \n",
    "        predictions = dict()\n",
    "        for label in test_labels:\n",
    "            test_df = gen_test_df_new(label[0], len(label), poi_info_new.copy(), query_id_dict.copy(), \n",
    "                                      poi_clusters=POI_CLUSTERS, cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "            rank_df = ranksvm.predict(test_df)\n",
    "            rank_df.sort_values(by='rank', ascending=False, inplace=True)\n",
    "            y_pred = [label[0]] + [p for p in rank_df.index.tolist() if p != label[0]][:len(label)-1]\n",
    "            predictions[(label[0], len(label))] = {'PRED': y_pred, 'REAL': label}\n",
    "            \n",
    "        F1, pF1, tau = evaluation(predictions)\n",
    "        F1_test.append(F1); pF1_test.append(pF1); tau_test.append(tau)\n",
    "    tau_mean = np.mean(tau_test)\n",
    "    print('mean_tau: %.3f' % tau_mean)\n",
    "    if tau_mean > best_tau:\n",
    "        best_tau = tau_mean\n",
    "        best_C = C\n",
    "print('\\nbest_tau: %.3f, best_C: %.3f' % (best_tau, best_C))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predictions = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "for i in range(len(fake_labels)):\n",
    "    sys.stdout.write('%s ' % str(i+1))\n",
    "    train_labels = fake_labels[:i] + fake_labels[i+1:]\n",
    "    train_df = gen_train_df_new(train_labels, poi_list, poi_info_new.copy(), query_id_dict.copy(),\n",
    "                                poi_clusters=POI_CLUSTERS, cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST, n_jobs=N_JOBS)\n",
    "    ranksvm = RankSVM(ranksvm_dir, useLinear=True)\n",
    "    ranksvm.train(train_df, cost=best_C)\n",
    "    test_label = fake_labels[i]\n",
    "    test_df = gen_test_df_new(test_label[0], len(test_label), poi_info_new.copy(), query_id_dict.copy(),\n",
    "                              poi_clusters=POI_CLUSTERS, cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "    rank_df = ranksvm.predict(test_df)\n",
    "    rank_df.sort_values(by='rank', ascending=False, inplace=True)\n",
    "    y_pred = [test_label[0]] + [p for p in rank_df.index.tolist() if p != test_label[0]][:len(test_label)-1]\n",
    "    predictions[(test_label[0], len(test_label))] = {'PRED': y_pred, 'REAL': test_label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ret = evaluation(predictions)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
