%!TEX root = main.tex

%\secmoveup
\section{Introduction}
\label{sec:intro}
%\textmoveup

Content recommendation has been the subject of a rich body of literature~\citep{Goldberg:1992,Sarwar:2001,Koren:2010},
with established techniques seeing widespread adoption in industry~\citep{Linden:2003,Agarwal:2013,Amatriain:2015,Gomez-Uribe:2015}.
The success of these methods is explained by both the explosion in availability of users' explicit and implicit preferences for content,
as well as the design of methods that can suitably exploit these to make useful recommendations~\citep{Koren:2009}.

Classical recommendation systems have focused on a fixed set of individual items such as books or movies.
This does not however capture scenarios where
the content %possesses some \emph{structure},
%for example being
is
naturally organised as a \emph{sequence} or {\em graph}.
There are several examples of such \emph{sequence recommendation} problems, such as
recommending
a playlist of songs~\citep{McFee:2011,chen2012playlist,hidasi2015session,choi2016towards},
a chemical compound~\cite{dehaspe1998finding},
or linked websites for e-commerce~\cite{antikacioglu2015recommendation}.
In this paper, we focus on the \emph{trajectory recommendation} problem,
which is a particular instance of sequence recommendation
where we wish to
recommend a \emph{trajectory} of points-of-interest (POIs) in a city to a visitor~\citep{lu2010photo2trip,lu2012personalized,ijcai15,cikm16paper},
\ie a sequence of POIs without repeats.

% In scenarios where there is a concept of a session,
% one needs to recommend

% We consider the problem of recommending trajectories (sequences of items
% without repeats) given a seed item.
% We propose a way to train the recommender system based on session data that may
% have multiple possible ground truth trajectories for a particular seed item.

Trajectory recommendation brings several challenges,
the most immediate of which is the need to ensure \emph{global cohesion} of predictions.
To illustrate, consider a na\"{i}ve approach
%(applicable to sequence recommendation tasks more generally)
which ignores all sequential structure:
we could learn a user's preference for individual POIs,
and create a trajectory based on the top ranked items.
Such an approach may be sub-optimal,
as
it is unlikely \eg a user will want to visit three restaurants in a row;
more generally,
while a user's two favourite songs might be in
the metal and country genres,
a playlist featuring these songs in succession may be jarring.

%The above raises the question of how one can effectively learn from such sequential content to ensure global cohesion.
%In this paper,
% we cast trajectory recommendation as a special case of a more general
% structured recommendation problem, where the structure is a sequence with no repeats.
To effectively %learn from such sequential content to
ensure such global cohesion,
we propose to attack trajectory recommendation
via \emph{structured prediction}, leveraging the toolkit of structured
SVMs~\citep{taskar2004max,tsochantaridis2004support}.
However, a vanilla application of such methods does not suffice,
owing to two further challenges:
in trajectory recommendation,
%as they do not account for the fact that
each input can have multiple ground truths,
since multiple trajectories may be reasonable for a single query;
and %that 
one needs to constrain predictions to avoid repeated elements, since users are unlikely to wish to visit the same POI twice.
%repeated POIs in predicted sequences are undesirable.
%predicted sequences must have no repeats.
We nonetheless show how to extend SSVMs to address both these challenges,
via a novel application of the \emph{list Viterbi} algorithm.
To summarise, our contributions are as follows:
\begin{itemize}[noitemsep,leftmargin=12pt]%\itemmoveup
    \item We formalise trajectory recommendation as a special case of the sequence recommendation problem.

	\item We solve three key challenges in this problem:
	\begin{itemize}
		\item to ensure global cohesion, we attack the problem using structured SVMs (SSVMs)
		\item to account for the existence of multiple ground truths for each input, we show how
		to extend SSVM training via the list Viterbi algorithm, 
        which is an extension of the classic Viterbi algorithm that sequentially finds the list of highest scored sequences under some model
		\item to predict sequences without repeated elements, we show how to use either the list Viterbi algorithm, or alternately an integer linear program.
	\end{itemize}

	\item We present experiments on real-world trajectory datasets, and demonstrate our structured prediction approaches improve over existing, non-structured baselines. % (\S\ref{sec:experiment}).\itemmoveup

	%\item We formalise the problem of sequence recommendation, %(\S\ref{sec:seqrec-defn}),
    %      and show how trajectory recommendation can be seen as a special case. % (\S\ref{sec:trajrec}).
	%%cast it as a structured prediction task (\S\ref{sec:recseq}), and .

	%\item We show how sequence recommendation may be attacked using structured SVMs. % (\S\ref{sec:recseq}).
	%We propose one improvement of structured SVMs %to the recommendation problem, so as
    %to account for the existence of multiple ground truths for each input. % (\S\ref{ssec:sr}).
	%%Following \citep{joachims2009cutting}, we propose both $n$-slack and 1-slack versions of the structured recommender.%\footnote{This new structured recommender can in principle be applied to any problem where loss augmented inference can be efficiently computed. We focus on sequence recommendation in this paper.}

    %\item We propose two novel applications of the list Viterbi algorithm -- an extension of the classic Viterbi algorithm that sequentially finds the list of highest scored sequences under some model --
%to exclude multiple ground truths for model learning, % (\S\ref{ssec:training}),
%and to predict sequences without repeated elements, i.e. {\em path}s in state-space. % (\S\ref{ssec:testing}).

	%%\item We show how one can avoid recommending sequences with loops or repetitions via integer linear programming and list Viterbi;
%an extension of the classic Viterbi algorithm that returns a list of the highest scored sequences under some model;
%%	We show that these algorithms may be incorporated during both the training %(via loss augmented inference)
%%	and prediction steps of our structured recommendation (\S\ref{ssec:training}, \S\ref{ssec:testing}).
	%LX - now we have only four bullet points, i'm happier with this -- 5 is too many!
	%LX - in addition, i do not think we can claim ILP as a contribution. it's in Lim'2015
\end{itemize}

We emphasise that while
our focus is
on recommending a travel trajectory, our formulation is abstract and in principle could be
used for other sequence recommendation tasks, such as recommending a playlist of songs.


%We show how to overcome this by
%ncorporating multiple ground truth sequences into a structured prediction formulation,
%suitably normalising the loss function for the model,
%LX - what's the right word here?  i feel normalising is only summarizing part of what is done??
%and by two novel applications of the list Viterbi algorithm that sequentially finds the list of top-scoring sequences.
%LX - marketing listViterbi
%modifying the inference and prediction steps using a variant of the Viterbi algorithm.


%We begin with an overview of related work, before presenting our model.
%We begin with an overview of the sequence recommendation problem, before presenting our model.
