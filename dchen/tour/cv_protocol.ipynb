{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Trajectory Recommendation - Test Evaluation Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import os, sys, time\n",
    "import math, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run notebook ```ssvm.ipynb```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run 'ssvm.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_protocol = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check for evaluation protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70/30 split for trajectories conform to each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traj_group_test = dict()\n",
    "test_ratio = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key in sorted(TRAJ_GROUP_DICT.keys()):\n",
    "    group = sorted(TRAJ_GROUP_DICT[key])\n",
    "    num = int(test_ratio * len(group))\n",
    "    if num > 0:\n",
    "        np.random.shuffle(group)\n",
    "        traj_group_test[key] = set(group[:num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if check_protocol == True:\n",
    "    nnrand_dict = dict()\n",
    "    ssvm_dict = dict()\n",
    "    \n",
    "    # train set\n",
    "    trajid_set_train = set(trajid_set_all)\n",
    "    for key in traj_group_test.keys():\n",
    "        trajid_set_train = trajid_set_train - traj_group_test[key]\n",
    "    \n",
    "    # train ssvm\n",
    "    poi_info = calc_poi_info(list(trajid_set_train), traj_all, poi_all)\n",
    "                \n",
    "    # build POI_ID <--> POI__INDEX mapping for POIs used to train CRF\n",
    "    # which means only POIs in traj such that len(traj) >= 2 are included\n",
    "    poi_set = set()\n",
    "    for x in trajid_set_train:\n",
    "        if len(traj_dict[x]) >= 2:\n",
    "            poi_set = poi_set | set(traj_dict[x])\n",
    "    poi_ix = sorted(poi_set)\n",
    "    poi_id_dict, poi_id_rdict = dict(), dict()\n",
    "    for idx, poi in enumerate(poi_ix):\n",
    "        poi_id_dict[poi] = idx\n",
    "        poi_id_rdict[idx] = poi\n",
    "\n",
    "    # generate training data\n",
    "    train_traj_list = [traj_dict[x] for x in trajid_set_train if len(traj_dict[x]) >= 2]\n",
    "    node_features_list = Parallel(n_jobs=N_JOBS)\\\n",
    "                         (delayed(calc_node_features)\\\n",
    "                          (tr[0], len(tr), poi_ix, poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                           cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST) for tr in train_traj_list)\n",
    "    edge_features = calc_edge_features(list(trajid_set_train), poi_ix, traj_dict, poi_info)\n",
    "\n",
    "    assert(len(train_traj_list) == len(node_features_list))\n",
    "    X_train = [(node_features_list[x], edge_features.copy(), \\\n",
    "                (poi_id_dict[train_traj_list[x][0]], len(train_traj_list[x]))) for x in range(len(train_traj_list))]\n",
    "    y_train = [np.array([poi_id_dict[x] for x in tr]) for tr in train_traj_list]\n",
    "    assert(len(X_train) == len(y_train))\n",
    "\n",
    "    # train\n",
    "    sm = MyModel()\n",
    "    verbose = 0 #5\n",
    "    ssvm = OneSlackSSVM(model=sm, C=SSVM_C, n_jobs=N_JOBS, verbose=verbose)\n",
    "    ssvm.fit(X_train, y_train, initialize=True)\n",
    "    \n",
    "    print('SSVM training finished, start predicting.'); sys.stdout.flush()\n",
    "\n",
    "    # predict for each query\n",
    "    for query in sorted(traj_group_test.keys()):\n",
    "        ps, L = query\n",
    "        \n",
    "        # start should be in training set\n",
    "        if ps not in poi_set: continue\n",
    "        assert(L <= poi_info.shape[0])\n",
    "        \n",
    "        # prediction of ssvm\n",
    "        node_features = calc_node_features(ps, L, poi_ix, poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                                           cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)        \n",
    "        # normalise test features\n",
    "        unaries, pw = scale_features_linear(node_features, edge_features, node_max=sm.node_max, node_min=sm.node_min, \\\n",
    "                                            edge_max=sm.edge_max, edge_min=sm.edge_min)\n",
    "        X_test = [(unaries, pw, (poi_id_dict[ps], L))]\n",
    "\n",
    "        # test\n",
    "        y_pred = ssvm.predict(X_test)\n",
    "        rec = [poi_id_rdict[x] for x in y_pred[0]] # map POIs back\n",
    "        rec1 = [ps] + rec[1:]\n",
    "        ssvm_dict[query] = rec1\n",
    "        \n",
    "        # prediction of nearest neighbour\n",
    "        candidates_id = sorted(TRAJ_GROUP_DICT[query] - traj_group_test[query])\n",
    "        assert(len(candidates_id) > 0)\n",
    "        np.random.shuffle(candidates_id)\n",
    "        nnrand_dict[query] = traj_dict[candidates_id[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if check_protocol == True:\n",
    "    F1_ssvm = []; pF1_ssvm = []; Tau_ssvm = []\n",
    "    F1_nn   = []; pF1_nn   = []; Tau_nn   = []\n",
    "    for key in sorted(ssvm_dict.keys()):\n",
    "        assert(key in nnrand_dict)\n",
    "        F1, pF1, tau = evaluate(ssvm_dict[key], traj_group_test[key])\n",
    "        F1_ssvm.append(F1); pF1_ssvm.append(pF1); Tau_ssvm.append(tau)\n",
    "        F1, pF1, tau = evaluate(nnrand_dict[key], traj_group_test[key])\n",
    "        F1_nn.append(F1); pF1_nn.append(pF1); Tau_nn.append(tau)\n",
    "    print('SSVM: F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f) Tau (%.3f, %.3f)' % \\\n",
    "          (np.mean(F1_ssvm), np.std(F1_ssvm)/np.sqrt(len(F1_ssvm)), \\\n",
    "           np.mean(pF1_ssvm), np.std(pF1_ssvm)/np.sqrt(len(pF1_ssvm)),\n",
    "           np.mean(Tau_ssvm), np.std(Tau_ssvm)/np.sqrt(len(Tau_ssvm))))\n",
    "    print('NNRAND: F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f), Tau (%.3f, %.3f)' % \\\n",
    "          (np.mean(F1_nn), np.std(F1_nn)/np.sqrt(len(F1_nn)), \\\n",
    "           np.mean(pF1_nn), np.std(pF1_nn)/np.sqrt(len(pF1_nn)), \\\n",
    "           np.mean(Tau_nn), np.std(Tau_nn)/np.sqrt(len(Tau_nn))))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
