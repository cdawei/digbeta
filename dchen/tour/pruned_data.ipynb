{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment on Pruned Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning a real dataset such as there is only one trajectory for each query, then evaluate SSVM and RankSVM on the pruned dataset via leave-one-out cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os, pickle, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cvxopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(1234554321)\n",
    "np.random.seed(123456789)\n",
    "cvxopt.base.setseed(123456789)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run notebook `generated_data.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run 'generated_data.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compute_new_features = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Pruning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pruned_labels = []\n",
    "pruned_trajid_set = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning dataset such that only the trajectory with maximum total POI popularity for each query is keeped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for query in sorted(TRAJ_GROUP_DICT.keys()):\n",
    "    max_pop = 0\n",
    "    max_tid = -1\n",
    "    for tid in TRAJ_GROUP_DICT[query]:\n",
    "        pop = np.sum([poi_info_all.loc[p, 'popularity'] for p in traj_dict[tid]])\n",
    "        if pop > max_pop:\n",
    "            max_pop = pop\n",
    "            max_tid = tid\n",
    "    assert(max_tid != -1)\n",
    "    pruned_labels.append(traj_dict[max_tid]) \n",
    "    pruned_trajid_set.append(max_tid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pruned_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POI and transition features are computed from the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if compute_new_features == True:\n",
    "    trajid_set = pruned_trajid_set.copy()  # Use only the pruned dataset to compute POI/transition features\n",
    "else:\n",
    "    trajid_set = trajid_set_all.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poi_info = calc_poi_info(sorted(trajid_set), traj_all, poi_all)\n",
    "poi_list = sorted({p for t in pruned_labels for p in t})\n",
    "poi_id_dict = {poi: ix for ix, poi in enumerate(poi_list)}\n",
    "poi_id_rdict = {ix: poi for ix, poi in enumerate(poi_list)}\n",
    "query_id_dict = {(t[0], len(t)): ix for ix, t in enumerate(pruned_labels)}\n",
    "edge_features = calc_edge_features(list(trajid_set), poi_list, traj_dict, poi_info.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(poi_info.shape[0], len(poi_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Train SSVM on pruned dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on pruned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_labels = pruned_labels.copy()\n",
    "X_train, y_train, scaler_node = calc_train_data(train_labels, poi_list, poi_info.copy(), \n",
    "                                                edge_features.copy(), poi_id_dict.copy())\n",
    "ssvm = train_ssvm(X_train, y_train, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_obj_curve(ssvm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = dict()\n",
    "for label in train_labels:\n",
    "    y_pred = predict(ssvm, label[0], len(label), poi_list, poi_info.copy(), edge_features.copy(),\n",
    "                     scaler_node, poi_id_dict, poi_id_rdict)\n",
    "    predictions[(label[0], len(label))] = {'PRED': y_pred, 'REAL': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ret = evaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Leave-one-out evaluation on pruned dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate SSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edge_features = np.zeros_like(edge_features)  # turn off transition features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose hyper-parameter C**\n",
    "\n",
    "Choose hyper-parameter C using Monte-Carlo cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_test = int(len(pruned_labels) * MC_PORTION)\n",
    "best_tau = 0; best_C = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "for C in C_SET:\n",
    "    print('\\n--------------- try_C: %f ---------------\\n' % C); sys.stdout.flush() \n",
    "    F1_test = []; pF1_test = []; tau_test = []\n",
    "    for t in range(MC_NITER):\n",
    "        while True:\n",
    "            indices = np.arange(len(pruned_labels))\n",
    "            np.random.shuffle(indices)\n",
    "            test_ix = indices[:num_test]\n",
    "            train_ix = indices[num_test:]\n",
    "            train_labels = [pruned_labels[ix] for ix in train_ix]\n",
    "            test_labels  = [pruned_labels[ix] for ix in test_ix]\n",
    "            poi_set_ = {p for x in train_labels for p in x}\n",
    "            if len(poi_set_) == len(poi_list): break\n",
    "        X_train, y_train, scaler_node = calc_train_data(train_labels, poi_list, poi_info.copy(), \n",
    "                                                        edge_features.copy(), poi_id_dict.copy())\n",
    "        ssvm = train_ssvm(X_train, y_train, C)\n",
    "        predictions = dict()\n",
    "        for label in test_labels:\n",
    "            y_pred = predict(ssvm, label[0], len(label), poi_list, poi_info.copy(), edge_features.copy(), \n",
    "                             scaler_node, poi_id_dict, poi_id_rdict)\n",
    "            predictions[(label[0], len(label))] = {'PRED': y_pred, 'REAL': label}\n",
    "        F1, pF1, tau = evaluation(predictions)\n",
    "        F1_test.append(F1); pF1_test.append(pF1); tau_test.append(tau)\n",
    "    mean_tau = np.mean(tau_test)\n",
    "    print('mean_tau: %.3f' % mean_tau)\n",
    "    if mean_tau > best_tau:\n",
    "        best_tau = mean_tau\n",
    "        best_C = C\n",
    "print('\\nbest_tau: %.3f, best_C: %.3f' % (best_tau, best_C))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leave-one-out cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# best_C: 30  when using the original dataset to compute features\n",
    "# best_C: 0.1 when using the original dataset to compute features and turn off transition features\n",
    "# best_C: 0.3 when using the pruned dataset to compute features \n",
    "# best_C: 0.1 when using the pruned dataset to compute features and turn off transition features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make sure the POI features are the same for training and test\n",
    "poi_info_ = poi_info\n",
    "edge_features_ = edge_features\n",
    "\n",
    "for i in range(len(pruned_labels)):\n",
    "    sys.stdout.write('%s ' % str(i+1))\n",
    "    train_labels = pruned_labels[:i] + pruned_labels[i+1:]\n",
    "    test_label = pruned_labels[i]\n",
    "    #trajid_set_ = trajid_set[:i] + trajid_set[i+1:]\n",
    "    poi_list_ = sorted({p for x in train_labels for p in x})\n",
    "    if test_label[0] not in poi_list_: continue\n",
    "    poi_id_dict_ = {poi: ix for ix, poi in enumerate(poi_list_)}\n",
    "    poi_id_rdict_ = {ix: poi for ix, poi in enumerate(poi_list_)}\n",
    "    #poi_info_ = calc_poi_info(trajid_set_, traj_all, poi_all)\n",
    "    #edge_features_ = calc_edge_features(trajid_set_, poi_list, traj_dict, poi_info_.copy())\n",
    "    X_train, y_train, scaler_node = calc_train_data(train_labels, poi_list_, poi_info_.copy(), \n",
    "                                                    edge_features_.copy(), poi_id_dict_.copy())\n",
    "    ssvm = train_ssvm(X_train, y_train, best_C)\n",
    "    y_pred = predict(ssvm, test_label[0], len(test_label), poi_list_, poi_info_.copy(), edge_features_.copy(), \n",
    "                     scaler_node, poi_id_dict_, poi_id_rdict_)\n",
    "    predictions[(test_label[0], len(test_label))] = {'PRED': y_pred, 'REAL': test_label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ret = evaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate RankSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose hyper-parameter C**\n",
    "\n",
    "Choose hyper-parameter C using Monte-Carlo cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_test = int(len(pruned_labels) * MC_PORTION)\n",
    "best_tau = 0; best_C = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "for C in C_SET:\n",
    "    print('\\n--------------- try_C: %f ---------------\\n' % C); sys.stdout.flush() \n",
    "    F1_test = []; pF1_test = []; tau_test = []\n",
    "    for t in range(MC_NITER):\n",
    "        while True:\n",
    "            indices = np.arange(len(pruned_labels))\n",
    "            np.random.shuffle(indices)\n",
    "            test_ix = indices[:num_test]\n",
    "            train_ix = indices[num_test:]\n",
    "            train_labels = [pruned_labels[ix] for ix in train_ix]\n",
    "            test_labels  = [pruned_labels[ix] for ix in test_ix]\n",
    "            poi_set_ = {p for x in train_labels for p in x}\n",
    "            if len(poi_set_) == len(poi_list): break\n",
    "        train_df = gen_train_df_new(train_labels, poi_list, poi_info.copy(), query_id_dict.copy(), \n",
    "                                    poi_clusters=POI_CLUSTERS,cats=POI_CAT_LIST,clusters=POI_CLUSTER_LIST,n_jobs=N_JOBS)\n",
    "        ranksvm = RankSVM(ranksvm_dir, useLinear=True)\n",
    "        ranksvm.train(train_df, cost=C)\n",
    "        \n",
    "        predictions = dict()\n",
    "        for label in test_labels:\n",
    "            test_df = gen_test_df_new(label[0], len(label), poi_info.copy(), query_id_dict.copy(), \n",
    "                                      poi_clusters=POI_CLUSTERS, cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "            rank_df = ranksvm.predict(test_df)\n",
    "            rank_df.sort_values(by='rank', ascending=False, inplace=True)\n",
    "            y_pred = [label[0]] + [p for p in rank_df.index.tolist() if p != label[0]][:len(label)-1]\n",
    "            predictions[(label[0], len(label))] = {'PRED': y_pred, 'REAL': label}\n",
    "            \n",
    "        F1, pF1, tau = evaluation(predictions)\n",
    "        F1_test.append(F1); pF1_test.append(pF1); tau_test.append(tau)\n",
    "    tau_mean = np.mean(tau_test)\n",
    "    print('mean_tau: %.3f' % tau_mean)\n",
    "    if tau_mean > best_tau:\n",
    "        best_tau = tau_mean\n",
    "        best_C = C\n",
    "print('\\nbest_tau: %.3f, best_C: %.3f' % (best_tau, best_C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leave-one-out cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# best_C: 0.030 when compute the features on the original dataset\n",
    "# best_C: 0.010 when compute the features on the pruned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make sure the POI features are the same for training and test\n",
    "poi_info_ = poi_info\n",
    "query_id_dict_ = query_id_dict\n",
    "\n",
    "for i in range(len(pruned_labels)):\n",
    "    sys.stdout.write('%s ' % str(i+1))\n",
    "    train_labels = pruned_labels[:i] + pruned_labels[i+1:]\n",
    "    #trajid_set_ = trajid_set[:i] + trajid_set[i+1:]\n",
    "    test_label = pruned_labels[i]\n",
    "    poi_list_ = sorted({p for x in train_labels for p in x})\n",
    "    if test_label[0] not in poi_list_: continue\n",
    "    poi_id_dict_ = {poi: ix for ix, poi in enumerate(poi_list_)}\n",
    "    poi_id_rdict_ = {ix: poi for ix, poi in enumerate(poi_list_)}\n",
    "    #poi_info_ = calc_poi_info(trajid_set_, traj_all, poi_all)\n",
    "    train_df = gen_train_df_new(train_labels, poi_list_, poi_info_.copy(), query_id_dict_.copy(),\n",
    "                                poi_clusters=POI_CLUSTERS, cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST, n_jobs=N_JOBS)\n",
    "    ranksvm = RankSVM(ranksvm_dir, useLinear=True)\n",
    "    ranksvm.train(train_df, cost=best_C)\n",
    "    test_df = gen_test_df_new(test_label[0], len(test_label), poi_info_.copy(), query_id_dict_.copy(),\n",
    "                              poi_clusters=POI_CLUSTERS, cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "    rank_df = ranksvm.predict(test_df)\n",
    "    rank_df.sort_values(by='rank', ascending=False, inplace=True)\n",
    "    y_pred = [test_label[0]] + [p for p in rank_df.index.tolist() if p != test_label[0]][:len(test_label)-1]\n",
    "    predictions[(test_label[0], len(test_label))] = {'PRED': y_pred, 'REAL': test_label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ret = evaluation(predictions)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
