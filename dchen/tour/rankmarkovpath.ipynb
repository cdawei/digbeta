{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#% matplotlib inline\n",
    "\n",
    "import os, sys, time, pickle, tempfile\n",
    "import math, random, itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.linalg import kron\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import pulp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('src/src_cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from shared import TrajData, evaluate, DF_COLUMNS, LOG_SMALL, LOG_ZERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(1234567890)\n",
    "np.random.seed(1234567890)\n",
    "ranksvm_dir = '$HOME/work/ranksvm'  # directory that contains rankSVM binaries: train, predict, svm-scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_ix = 0\n",
    "data_dir = 'data/data-new'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_obj = TrajData(dat_ix, data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_JOBS = 6         # number of parallel jobs\n",
    "USE_GUROBI = False # whether to use GUROBI as ILP solver\n",
    "C_SET = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000, 3000]  # regularisation parameter\n",
    "ALPHA_SET = [0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99]  # trade-off parameter\n",
    "MC_PORTION = 0.1   # the portion of data that sampled by Monte-Carlo cross-validation\n",
    "MC_NITER = 5       # number of iterations for Monte-Carlo cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_train_subdf(poi_id, poi_info, query_id_set, query_id_rdict, dat_obj):\n",
    "    assert(type(dat_obj) == TrajData)\n",
    "    \n",
    "    columns = DF_COLUMNS\n",
    "    poi_distmat = dat_obj.POI_DISTMAT\n",
    "    poi_clusters = dat_obj.POI_CLUSTERS\n",
    "    cats = dat_obj.POI_CAT_LIST\n",
    "    clusters = dat_obj.POI_CLUSTER_LIST\n",
    "    \n",
    "    df_ = pd.DataFrame(index=np.arange(len(query_id_set)), columns=columns)\n",
    "    \n",
    "    pop, nvisit = poi_info.loc[poi_id, 'popularity'], poi_info.loc[poi_id, 'nVisit']\n",
    "    cat, cluster = poi_info.loc[poi_id, 'poiCat'], poi_clusters.loc[poi_id, 'clusterID'] \n",
    "    duration = poi_info.loc[poi_id, 'avgDuration']\n",
    "    \n",
    "    for j in range(len(query_id_set)):\n",
    "        qid = query_id_set[j]\n",
    "        assert(qid in query_id_rdict) # qid --> (start, end, length)\n",
    "        (p0, trajLen) = query_id_rdict[qid]\n",
    "        idx = df_.index[j]\n",
    "        df_.loc[idx, 'poiID'] = poi_id\n",
    "        df_.loc[idx, 'queryID'] = qid\n",
    "        df_.set_value(idx, 'category', tuple((cat == np.array(cats)).astype(np.int) * 2 - 1))\n",
    "        df_.set_value(idx, 'neighbourhood', tuple((cluster == np.array(clusters)).astype(np.int) * 2 - 1))\n",
    "        df_.loc[idx, 'popularity'] = LOG_SMALL if pop < 1 else np.log10(pop)\n",
    "        df_.loc[idx, 'nVisit'] = LOG_SMALL if nvisit < 1 else np.log10(nvisit)\n",
    "        df_.loc[idx, 'avgDuration'] = LOG_SMALL if duration < 1 else np.log10(duration)\n",
    "        df_.loc[idx, 'trajLen'] = trajLen\n",
    "        df_.loc[idx, 'sameCatStart'] = 1 if cat == poi_info.loc[p0, 'poiCat'] else -1\n",
    "        df_.loc[idx, 'distStart'] = poi_distmat.loc[poi_id, p0]\n",
    "        df_.loc[idx, 'diffPopStart'] = pop - poi_info.loc[p0, 'popularity']\n",
    "        df_.loc[idx, 'diffNVisitStart'] = nvisit - poi_info.loc[p0, 'nVisit']\n",
    "        df_.loc[idx, 'diffDurationStart'] = duration - poi_info.loc[p0, 'avgDuration']\n",
    "        df_.loc[idx, 'sameNeighbourhoodStart'] = 1 if cluster == poi_clusters.loc[p0, 'clusterID'] else -1\n",
    "        \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_train_df(trajid_list, poi_info, dat_obj, n_jobs=-1):    \n",
    "    assert(type(dat_obj) == TrajData)\n",
    "    \n",
    "    columns = DF_COLUMNS\n",
    "    query_id_dict = dat_obj.QUERY_ID_DICT\n",
    "    train_trajs = [dat_obj.traj_dict[x] for x in trajid_list if len(dat_obj.traj_dict[x]) >= 2]\n",
    "    qid_set = sorted(set([query_id_dict[(t[0], len(t))] for t in train_trajs]))\n",
    "    poi_set = {poi for tr in train_trajs for poi in tr}\n",
    "    \n",
    "    query_id_rdict = dict()\n",
    "    for k, v in query_id_dict.items(): \n",
    "        query_id_rdict[v] = k  # qid --> (start, length)\n",
    "    \n",
    "    train_df_list = Parallel(n_jobs=n_jobs)\\\n",
    "                    (delayed(gen_train_subdf)(poi, poi_info, qid_set, query_id_rdict, dat_obj) for poi in poi_set)\n",
    "                        \n",
    "    assert(len(train_df_list) > 0)\n",
    "    df_ = train_df_list[0]\n",
    "    for j in range(1, len(train_df_list)):\n",
    "        df_ = df_.append(train_df_list[j], ignore_index=True)            \n",
    "        \n",
    "    # set label\n",
    "    df_.set_index(['queryID', 'poiID'], inplace=True)\n",
    "    df_['label'] = 0\n",
    "    for t in train_trajs:\n",
    "        qid = query_id_dict[(t[0], len(t))]\n",
    "        for poi in t[1:]:  # do NOT count if the POI is startPOI/endPOI\n",
    "            df_.loc[(qid, poi), 'label'] += 1\n",
    "\n",
    "    df_.reset_index(inplace=True)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_test_df(startPOI, nPOI, poi_info, dat_obj):\n",
    "    assert(type(dat_obj) == TrajData)\n",
    "    \n",
    "    columns = DF_COLUMNS\n",
    "    poi_distmat = dat_obj.POI_DISTMAT\n",
    "    poi_clusters = dat_obj.POI_CLUSTERS\n",
    "    cats = dat_obj.POI_CAT_LIST\n",
    "    clusters = dat_obj.POI_CLUSTER_LIST\n",
    "    query_id_dict = dat_obj.QUERY_ID_DICT\n",
    "    key = (p0, trajLen) = (startPOI, nPOI)\n",
    "    assert(key in query_id_dict)\n",
    "    assert(p0 in poi_info.index)\n",
    "    \n",
    "    df_ = pd.DataFrame(index=np.arange(poi_info.shape[0]), columns=columns)\n",
    "    poi_list = sorted(poi_info.index)\n",
    "    \n",
    "    qid = query_id_dict[key]\n",
    "    df_['queryID'] = qid\n",
    "    df_['label'] = np.random.rand(df_.shape[0]) # label for test data is arbitrary according to libsvm FAQ\n",
    "\n",
    "    for i in range(df_.index.shape[0]):\n",
    "        poi = poi_list[i]\n",
    "        lon, lat = poi_info.loc[poi, 'poiLon'], poi_info.loc[poi, 'poiLat']\n",
    "        pop, nvisit = poi_info.loc[poi, 'popularity'], poi_info.loc[poi, 'nVisit']\n",
    "        cat, cluster = poi_info.loc[poi, 'poiCat'], poi_clusters.loc[poi, 'clusterID']\n",
    "        duration = poi_info.loc[poi, 'avgDuration']\n",
    "        idx = df_.index[i]\n",
    "        df_.loc[idx, 'poiID'] = poi\n",
    "        df_.set_value(idx, 'category', tuple((cat == np.array(cats)).astype(np.int) * 2 - 1))\n",
    "        df_.set_value(idx, 'neighbourhood', tuple((cluster == np.array(clusters)).astype(np.int) * 2 - 1))\n",
    "        df_.loc[idx, 'popularity'] = LOG_SMALL if pop < 1 else np.log10(pop)\n",
    "        df_.loc[idx, 'nVisit'] = LOG_SMALL if nvisit < 1 else np.log10(nvisit)\n",
    "        df_.loc[idx, 'avgDuration'] = LOG_SMALL if duration < 1 else np.log10(duration)\n",
    "        df_.loc[idx, 'trajLen'] = trajLen\n",
    "        df_.loc[idx, 'sameCatStart'] = 1 if cat == poi_info.loc[p0, 'poiCat'] else -1\n",
    "        df_.loc[idx, 'distStart'] = poi_distmat.loc[poi, p0]\n",
    "        df_.loc[idx, 'diffPopStart'] = pop - poi_info.loc[p0, 'popularity']\n",
    "        df_.loc[idx, 'diffNVisitStart'] = nvisit - poi_info.loc[p0, 'nVisit']\n",
    "        df_.loc[idx, 'diffDurationStart'] = duration - poi_info.loc[p0, 'avgDuration']\n",
    "        df_.loc[idx, 'sameNeighbourhoodStart'] = 1 if cluster == poi_clusters.loc[p0, 'clusterID'] else -1\n",
    "\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_data_str(df_, df_columns=DF_COLUMNS):\n",
    "    for col in df_columns:\n",
    "        assert(col in df_.columns)\n",
    "        \n",
    "    lines = []\n",
    "    for idx in df_.index:\n",
    "        slist = [str(df_.loc[idx, 'label'])]\n",
    "        slist.append(' qid:')\n",
    "        slist.append(str(int(df_.loc[idx, 'queryID'])))\n",
    "        fid = 1\n",
    "        for j in range(3, len(df_columns)):\n",
    "            values_ = df_.get_value(idx, df_columns[j])\n",
    "            values_ = values_ if isinstance(values_, tuple) else [values_]\n",
    "            for v in values_:\n",
    "                slist.append(' ')\n",
    "                slist.append(str(fid)); fid += 1\n",
    "                slist.append(':')\n",
    "                slist.append(str(v))\n",
    "        slist.append('\\n')\n",
    "        lines.append(''.join(slist))\n",
    "    return ''.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x1 = x.copy()\n",
    "    x1 -= np.max(x1)  # numerically more stable, REF: http://cs231n.github.io/linear-classify/#softmax\n",
    "    expx = np.exp(x1)\n",
    "    return expx / np.sum(expx, axis=0) # column-wise sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# python wrapper of rankSVM\n",
    "class RankSVM:\n",
    "    def __init__(self, bin_dir, useLinear=True, debug=False):\n",
    "        dir_ = !echo $bin_dir  # deal with environmental variables in path\n",
    "        assert(os.path.exists(dir_[0]))\n",
    "        self.bin_dir = dir_[0]\n",
    "        \n",
    "        self.bin_train = 'svm-train'\n",
    "        self.bin_predict = 'svm-predict'\n",
    "        if useLinear:\n",
    "            self.bin_train = 'train'\n",
    "            self.bin_predict = 'predict'\n",
    "        \n",
    "        assert(isinstance(debug, bool))\n",
    "        self.debug = debug\n",
    "        \n",
    "        # create named tmp files for model and feature scaling parameters\n",
    "        self.fmodel = None\n",
    "        self.fscale = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            self.fmodel = fd.name\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            self.fscale = fd.name\n",
    "        \n",
    "        if self.debug:\n",
    "            print('model file:', self.fmodel)\n",
    "            print('feature scaling parameter file:', self.fscale)\n",
    "    \n",
    "    \n",
    "    def __del__(self):\n",
    "        # remove tmp files\n",
    "        if self.debug == False:\n",
    "            if self.fmodel is not None and os.path.exists(self.fmodel):\n",
    "                os.unlink(self.fmodel)\n",
    "            if self.fscale is not None and os.path.exists(self.fscale):\n",
    "                os.unlink(self.fscale)\n",
    "\n",
    "    \n",
    "    def train(self, train_df, cost=1):\n",
    "        # cost is parameter C in SVM\n",
    "        # write train data to file\n",
    "        ftrain = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftrain = fd.name\n",
    "            datastr = gen_data_str(train_df)\n",
    "            fd.write(datastr)\n",
    "        \n",
    "        # feature scaling\n",
    "        ftrain_scaled = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftrain_scaled = fd.name\n",
    "        result = !$self.bin_dir/svm-scale -s $self.fscale $ftrain > $ftrain_scaled\n",
    "        \n",
    "        if self.debug:\n",
    "            print('cost:', cost)\n",
    "            print('train data file:', ftrain)\n",
    "            print('feature scaled train data file:', ftrain_scaled)\n",
    "        \n",
    "        # train rank svm and generate model file, if the model file exists, rewrite it\n",
    "        result = !$self.bin_dir/$self.bin_train -c $cost $ftrain_scaled $self.fmodel\n",
    "        if self.debug:\n",
    "            print('Training finished.')\n",
    "            for i in range(len(result)): print(result[i])\n",
    "                \n",
    "        # load model parameters\n",
    "        w = []\n",
    "        header = 5\n",
    "        with open(self.fmodel, 'r') as f:\n",
    "            for j in range(header): _ = f.readline()\n",
    "            for line in f: w.append(float(line.strip()))\n",
    "        self.w = np.array(w)\n",
    "\n",
    "        # remove train data file\n",
    "        if self.debug == False:\n",
    "            os.unlink(ftrain)\n",
    "            os.unlink(ftrain_scaled)        \n",
    "\n",
    "    \n",
    "    def predict(self, test_df, probability=False):\n",
    "        # predict ranking scores for the given feature matrix\n",
    "        if self.fmodel is None or not os.path.exists(self.fmodel):\n",
    "            print('Model should be trained before prediction')\n",
    "            return\n",
    "        \n",
    "        # write test data to file\n",
    "        ftest = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftest = fd.name\n",
    "            datastr = gen_data_str(test_df)\n",
    "            fd.write(datastr)\n",
    "                \n",
    "        # feature scaling\n",
    "        ftest_scaled = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            ftest_scaled = fd.name\n",
    "        result = !$self.bin_dir/svm-scale -r $self.fscale $ftest > $ftest_scaled\n",
    "            \n",
    "        # generate prediction file\n",
    "        fpredict = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            fpredict = fd.name\n",
    "            \n",
    "        if self.debug:\n",
    "            print('test data file:', ftest)\n",
    "            print('feature scaled test data file:', ftest_scaled)\n",
    "            print('predict result file:', fpredict)\n",
    "            \n",
    "        # predict using trained model and write prediction to file\n",
    "        result = !$self.bin_dir/$self.bin_predict $ftest_scaled $self.fmodel $fpredict\n",
    "        if self.debug:\n",
    "            print('Predict result: %-30s  %s' % (result[0], result[1]))\n",
    "        \n",
    "        # generate prediction DataFrame from prediction file\n",
    "        poi_rank_df = pd.read_csv(fpredict, header=None)\n",
    "        poi_rank_df.rename(columns={0:'rank'}, inplace=True)\n",
    "        poi_rank_df['poiID'] = test_df['poiID'].astype(np.int)\n",
    "        poi_rank_df.set_index('poiID', inplace=True)\n",
    "        if probability == True: poi_rank_df['probability'] = softmax(poi_rank_df['rank'])\n",
    "        \n",
    "        # remove test file and prediction file\n",
    "        if self.debug == False:\n",
    "            os.unlink(ftest)\n",
    "            os.unlink(ftest_scaled)\n",
    "            os.unlink(fpredict)\n",
    "\n",
    "        return poi_rank_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_poi_logtransmat(trajid_list, poi_set, poi_info, dat_obj, debug=False):\n",
    "    transmat_cat      = dat_obj.gen_transmat_cat(trajid_list, poi_info)\n",
    "    transmat_pop      = dat_obj.gen_transmat_pop(trajid_list, poi_info)\n",
    "    transmat_visit    = dat_obj.gen_transmat_visit(trajid_list, poi_info)\n",
    "    transmat_duration = dat_obj.gen_transmat_duration(trajid_list, poi_info)\n",
    "    transmat_neighbor = dat_obj.gen_transmat_neighbor(trajid_list, poi_info)\n",
    "\n",
    "    # Kronecker product\n",
    "    transmat_ix = list(itertools.product(transmat_cat.index, transmat_pop.index, transmat_visit.index, \\\n",
    "                                         transmat_duration.index, transmat_neighbor.index))\n",
    "    transmat_value = transmat_cat.values\n",
    "    for transmat in [transmat_pop, transmat_visit, transmat_duration, transmat_neighbor]:\n",
    "        transmat_value = kron(transmat_value, transmat.values)\n",
    "    transmat_feature = pd.DataFrame(data=transmat_value, index=transmat_ix, columns=transmat_ix)\n",
    "    \n",
    "    poi_train = sorted(poi_set)\n",
    "    feature_names = ['poiCat', 'popularity', 'nVisit', 'avgDuration', 'clusterID']\n",
    "    poi_features = pd.DataFrame(data=np.zeros((len(poi_train), len(feature_names))), \\\n",
    "                                columns=feature_names, index=poi_train)\n",
    "    poi_features.index.name = 'poiID'\n",
    "    poi_features['poiCat'] = poi_info.loc[poi_train, 'poiCat']\n",
    "    poi_features['popularity'] = np.digitize(poi_info.loc[poi_train, 'popularity'], dat_obj.LOGBINS_POP)\n",
    "    poi_features['nVisit'] = np.digitize(poi_info.loc[poi_train, 'nVisit'], dat_obj.LOGBINS_VISIT)\n",
    "    poi_features['avgDuration'] = np.digitize(poi_info.loc[poi_train, 'avgDuration'], dat_obj.LOGBINS_DURATION)\n",
    "    poi_features['clusterID'] = dat_obj.POI_CLUSTERS.loc[poi_train, 'clusterID']\n",
    "    \n",
    "    # shrink the result of Kronecker product and deal with POIs with the same features\n",
    "    poi_logtransmat = pd.DataFrame(data=np.zeros((len(poi_train), len(poi_train)), dtype=np.float), \\\n",
    "                                   columns=poi_train, index=poi_train)\n",
    "    for p1 in poi_logtransmat.index:\n",
    "        rix = tuple(poi_features.loc[p1])\n",
    "        for p2 in poi_logtransmat.columns:\n",
    "            cix = tuple(poi_features.loc[p2])\n",
    "            value_ = transmat_feature.loc[(rix,), (cix,)]\n",
    "            poi_logtransmat.loc[p1, p2] = value_.values[0, 0]\n",
    "    \n",
    "    # group POIs with the same features\n",
    "    features_dup = dict()\n",
    "    for poi in poi_features.index:\n",
    "        key = tuple(poi_features.loc[poi])\n",
    "        if key in features_dup:\n",
    "            features_dup[key].append(poi)\n",
    "        else:\n",
    "            features_dup[key] = [poi]\n",
    "    if debug == True:\n",
    "        for key in sorted(features_dup.keys()):\n",
    "            print(key, '->', features_dup[key])\n",
    "            \n",
    "    # deal with POIs with the same features\n",
    "    for feature in sorted(features_dup.keys()):\n",
    "        n = len(features_dup[feature])\n",
    "        if n > 1:\n",
    "            group = features_dup[feature]\n",
    "            v1 = poi_logtransmat.loc[group[0], group[0]]  # transition value of self-loop of POI group\n",
    "            \n",
    "            # divide incoming transition value (i.e. unnormalised transition probability) uniformly among group members\n",
    "            for poi in group:\n",
    "                poi_logtransmat[poi] /= n\n",
    "                \n",
    "            # outgoing transition value has already been duplicated (value copied above)\n",
    "            \n",
    "            # duplicate & divide transition value of self-loop of POI group uniformly among all outgoing transitions,\n",
    "            # from a POI to all other POIs in the same group (excluding POI self-loop)\n",
    "            v2 = v1 / (n - 1)\n",
    "            for pair in itertools.permutations(group, 2):\n",
    "                poi_logtransmat.loc[pair[0], pair[1]] = v2\n",
    "                            \n",
    "    # normalise each row\n",
    "    for p1 in poi_logtransmat.index:\n",
    "        poi_logtransmat.loc[p1, p1] = 0\n",
    "        rowsum = poi_logtransmat.loc[p1].sum()\n",
    "        assert(rowsum > 0)\n",
    "        logrowsum = np.log10(rowsum)\n",
    "        for p2 in poi_logtransmat.columns:\n",
    "            if p1 == p2:\n",
    "                poi_logtransmat.loc[p1, p2] = LOG_ZERO  # deal with log(0) explicitly\n",
    "            else:\n",
    "                poi_logtransmat.loc[p1, p2] = np.log10(poi_logtransmat.loc[p1, p2]) - logrowsum\n",
    "    \n",
    "    return poi_logtransmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_ILP(V, E, ps, L, withNodeWeight=False, alpha=0.5):\n",
    "    assert(isinstance(V, pd.DataFrame))\n",
    "    assert(isinstance(E, pd.DataFrame))\n",
    "    assert(ps in V.index)\n",
    "    assert(2 <= L <= V.index.shape[0])\n",
    "    if withNodeWeight == True:\n",
    "        assert(0 < alpha < 1)\n",
    "    beta = 1 - alpha\n",
    "    \n",
    "    p0 = str(ps); M = V.index.shape[0]\n",
    "    \n",
    "    # REF: pythonhosted.org/PuLP/index.html\n",
    "    pois = [str(p) for p in V.index] # create a string list for each POI\n",
    "    pb = pulp.LpProblem('MostLikelyTraj', pulp.LpMaximize) # create problem\n",
    "    # visit_i_j = 1 means POI i and j are visited in sequence\n",
    "    visit_vars = pulp.LpVariable.dicts('visit', (pois, pois), 0, 1, pulp.LpInteger) \n",
    "    # isend_l = 1 means POI l is the END POI of trajectory\n",
    "    isend_vars = pulp.LpVariable.dicts('isend', pois, 0, 1, pulp.LpInteger) \n",
    "    # a dictionary contains all dummy variables\n",
    "    dummy_vars = pulp.LpVariable.dicts('u', [x for x in pois if x != p0], 2, M, pulp.LpInteger)\n",
    "    \n",
    "    # add objective\n",
    "    objlist = []\n",
    "    if withNodeWeight == True:\n",
    "        objlist.append(alpha * V.loc[int(p0), 'weight'])\n",
    "    for pi in [x for x in pois]:     # from\n",
    "        for pj in [y for y in pois if y != p0]: # to\n",
    "            if withNodeWeight == True:\n",
    "                objlist.append(visit_vars[pi][pj] * (alpha * V.loc[int(pj), 'weight'] + beta * E.loc[int(pi), int(pj)]))\n",
    "            else:\n",
    "                objlist.append(visit_vars[pi][pj] * E.loc[int(pi), int(pj)])\n",
    "    pb += pulp.lpSum(objlist), 'Objective'\n",
    "    \n",
    "    # add constraints, each constraint should be in ONE line\n",
    "    pb += pulp.lpSum([visit_vars[pi][pi] for pi in pois]) == 0, 'NoSelfLoops'\n",
    "    pb += pulp.lpSum([visit_vars[p0][pj] for pj in pois]) == 1, 'StartAt_p0'\n",
    "    pb += pulp.lpSum([visit_vars[pi][p0] for pi in pois]) == 0, 'NoIncoming_p0'\n",
    "    pb += pulp.lpSum([visit_vars[pi][pj] for pi in pois for pj in pois]) == L-1, 'Length'\n",
    "    pb += pulp.lpSum([isend_vars[pi] for pi in pois]) == 1, 'OneEnd'\n",
    "    pb += isend_vars[p0] == 0, 'StartNotEnd'\n",
    "    \n",
    "    for pk in [x for x in pois if x != p0]:\n",
    "        pb += pulp.lpSum([visit_vars[pi][pk] for pi in pois]) == isend_vars[pk] + \\\n",
    "              pulp.lpSum([visit_vars[pk][pj] for pj in pois if pj != p0]), 'ConnectedAt_' + pk\n",
    "        pb += pulp.lpSum([visit_vars[pi][pk] for pi in pois]) <= 1, 'Enter_' + pk + '_AtMostOnce'\n",
    "        pb += pulp.lpSum([visit_vars[pk][pj] for pj in pois if pj != p0]) + isend_vars[pk] <= 1, \\\n",
    "              'Leave_' + pk + '_AtMostOnce'\n",
    "    for pi in [x for x in pois if x != p0]:\n",
    "        for pj in [y for y in pois if y != p0]:\n",
    "            pb += dummy_vars[pi] - dummy_vars[pj] + 1 <= (M - 1) * (1 - visit_vars[pi][pj]), \\\n",
    "                    'SubTourElimination_' + pi + '_' + pj\n",
    "    #pb.writeLP(\"traj_tmp.lp\")\n",
    "    \n",
    "    # solve problem: solver should be available in PATH\n",
    "    if USE_GUROBI == True:\n",
    "        gurobi_options = [('TimeLimit', '7200'), ('Threads', str(N_JOBS)), ('NodefileStart', '0.2'), ('Cuts', '2')]\n",
    "        pb.solve(pulp.GUROBI_CMD(path='gurobi_cl', options=gurobi_options)) # GUROBI\n",
    "    else:\n",
    "        pb.solve(pulp.COIN_CMD(path='cbc', options=['-threads', str(N_JOBS), '-strategy', '1', '-maxIt', '2000000']))#CBC\n",
    "    visit_mat = pd.DataFrame(data=np.zeros((len(pois), len(pois)), dtype=np.float), index=pois, columns=pois)\n",
    "    isend_vec = pd.Series(data=np.zeros(len(pois), dtype=np.float), index=pois)\n",
    "    for pi in pois:\n",
    "        isend_vec.loc[pi] = isend_vars[pi].varValue\n",
    "        for pj in pois: visit_mat.loc[pi, pj] = visit_vars[pi][pj].varValue\n",
    "    #visit_mat.to_csv('visit.csv')\n",
    "\n",
    "    # build the recommended trajectory\n",
    "    recseq = [p0]\n",
    "    while True:\n",
    "        pi = recseq[-1]\n",
    "        pj = visit_mat.loc[pi].idxmax()\n",
    "        value = visit_mat.loc[pi, pj]\n",
    "        #print(value, int(round(value)))\n",
    "        #print(recseq)\n",
    "        assert(int(round(value)) == 1)\n",
    "        recseq.append(pj)\n",
    "        if len(recseq) == L: \n",
    "            assert(int(round(isend_vec[pj])) == 1)\n",
    "            #print('===:', recseq, ':====')\n",
    "            return np.asarray([int(x) for x in recseq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- try_C: 0.010, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.633\n",
      "\n",
      "--------------- try_C: 0.010, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.592\n",
      "\n",
      "--------------- try_C: 0.010, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.619\n",
      "\n",
      "--------------- try_C: 0.010, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.611\n",
      "\n",
      "--------------- try_C: 0.010, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.577\n",
      "\n",
      "--------------- try_C: 0.010, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.585\n",
      "\n",
      "--------------- try_C: 0.010, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.568\n",
      "\n",
      "--------------- try_C: 0.030, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.569\n",
      "\n",
      "--------------- try_C: 0.030, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.688\n",
      "\n",
      "--------------- try_C: 0.030, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.615\n",
      "\n",
      "--------------- try_C: 0.030, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.661\n",
      "\n",
      "--------------- try_C: 0.030, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.641\n",
      "\n",
      "--------------- try_C: 0.030, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.588\n",
      "\n",
      "--------------- try_C: 0.030, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.534\n",
      "\n",
      "--------------- try_C: 0.100, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.523\n",
      "\n",
      "--------------- try_C: 0.100, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.681\n",
      "\n",
      "--------------- try_C: 0.100, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.605\n",
      "\n",
      "--------------- try_C: 0.100, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.631\n",
      "\n",
      "--------------- try_C: 0.100, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.609\n",
      "\n",
      "--------------- try_C: 0.100, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.645\n",
      "\n",
      "--------------- try_C: 0.100, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.605\n",
      "\n",
      "--------------- try_C: 0.300, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.668\n",
      "\n",
      "--------------- try_C: 0.300, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.624\n",
      "\n",
      "--------------- try_C: 0.300, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.483\n",
      "\n",
      "--------------- try_C: 0.300, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.695\n",
      "\n",
      "--------------- try_C: 0.300, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.624\n",
      "\n",
      "--------------- try_C: 0.300, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.639\n",
      "\n",
      "--------------- try_C: 0.300, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.585\n",
      "\n",
      "--------------- try_C: 1.000, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.614\n",
      "\n",
      "--------------- try_C: 1.000, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.603\n",
      "\n",
      "--------------- try_C: 1.000, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.728\n",
      "\n",
      "--------------- try_C: 1.000, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.668\n",
      "\n",
      "--------------- try_C: 1.000, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.672\n",
      "\n",
      "--------------- try_C: 1.000, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.627\n",
      "\n",
      "--------------- try_C: 1.000, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.755\n",
      "\n",
      "--------------- try_C: 3.000, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.571\n",
      "\n",
      "--------------- try_C: 3.000, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.640\n",
      "\n",
      "--------------- try_C: 3.000, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.671\n",
      "\n",
      "--------------- try_C: 3.000, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.712\n",
      "\n",
      "--------------- try_C: 3.000, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.655\n",
      "\n",
      "--------------- try_C: 3.000, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.531\n",
      "\n",
      "--------------- try_C: 3.000, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.608\n",
      "\n",
      "--------------- try_C: 10.000, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.713\n",
      "\n",
      "--------------- try_C: 10.000, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.633\n",
      "\n",
      "--------------- try_C: 10.000, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.634\n",
      "\n",
      "--------------- try_C: 10.000, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.619\n",
      "\n",
      "--------------- try_C: 10.000, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.508\n",
      "\n",
      "--------------- try_C: 10.000, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.632\n",
      "\n",
      "--------------- try_C: 10.000, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.551\n",
      "\n",
      "--------------- try_C: 30.000, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.533\n",
      "\n",
      "--------------- try_C: 30.000, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.678\n",
      "\n",
      "--------------- try_C: 30.000, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.675\n",
      "\n",
      "--------------- try_C: 30.000, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.644\n",
      "\n",
      "--------------- try_C: 30.000, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.643\n",
      "\n",
      "--------------- try_C: 30.000, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.621\n",
      "\n",
      "--------------- try_C: 30.000, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.535\n",
      "\n",
      "--------------- try_C: 100.000, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.576\n",
      "\n",
      "--------------- try_C: 100.000, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.669\n",
      "\n",
      "--------------- try_C: 100.000, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.621\n",
      "\n",
      "--------------- try_C: 100.000, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.635\n",
      "\n",
      "--------------- try_C: 100.000, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.619\n",
      "\n",
      "--------------- try_C: 100.000, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.650\n",
      "\n",
      "--------------- try_C: 100.000, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.664\n",
      "\n",
      "--------------- try_C: 300.000, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.574\n",
      "\n",
      "--------------- try_C: 300.000, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.525\n",
      "\n",
      "--------------- try_C: 300.000, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.667\n",
      "\n",
      "--------------- try_C: 300.000, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.591\n",
      "\n",
      "--------------- try_C: 300.000, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.729\n",
      "\n",
      "--------------- try_C: 300.000, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.626\n",
      "\n",
      "--------------- try_C: 300.000, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.601\n",
      "\n",
      "--------------- try_C: 1000.000, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.737\n",
      "\n",
      "--------------- try_C: 1000.000, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.699\n",
      "\n",
      "--------------- try_C: 1000.000, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.598\n",
      "\n",
      "--------------- try_C: 1000.000, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.528\n",
      "\n",
      "--------------- try_C: 1000.000, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.585\n",
      "\n",
      "--------------- try_C: 1000.000, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.564\n",
      "\n",
      "--------------- try_C: 1000.000, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.541\n",
      "\n",
      "--------------- try_C: 3000.000, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.645\n",
      "\n",
      "--------------- try_C: 3000.000, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.707\n",
      "\n",
      "--------------- try_C: 3000.000, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.599\n",
      "\n",
      "--------------- try_C: 3000.000, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.631\n",
      "\n",
      "--------------- try_C: 3000.000, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.699\n",
      "\n",
      "--------------- try_C: 3000.000, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.554\n",
      "\n",
      "--------------- try_C: 3000.000, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.656\n",
      "\n",
      "--------------- 1/47, Query: (1, 2), Best_C: 1.000, Best_alpha: 0.990 ---------------\n",
      "\n",
      "\n",
      "--------------- try_C: 0.010, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.626\n",
      "\n",
      "--------------- try_C: 0.010, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.707\n",
      "\n",
      "--------------- try_C: 0.010, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.561\n",
      "\n",
      "--------------- try_C: 0.010, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.576\n",
      "\n",
      "--------------- try_C: 0.010, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.672\n",
      "\n",
      "--------------- try_C: 0.010, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.625\n",
      "\n",
      "--------------- try_C: 0.010, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.605\n",
      "\n",
      "--------------- try_C: 0.030, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.625\n",
      "\n",
      "--------------- try_C: 0.030, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.615\n",
      "\n",
      "--------------- try_C: 0.030, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.572\n",
      "\n",
      "--------------- try_C: 0.030, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.680\n",
      "\n",
      "--------------- try_C: 0.030, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.752\n",
      "\n",
      "--------------- try_C: 0.030, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.501\n",
      "\n",
      "--------------- try_C: 0.030, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.669\n",
      "\n",
      "--------------- try_C: 0.100, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.608\n",
      "\n",
      "--------------- try_C: 0.100, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.704\n",
      "\n",
      "--------------- try_C: 0.100, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.590\n",
      "\n",
      "--------------- try_C: 0.100, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.726\n",
      "\n",
      "--------------- try_C: 0.100, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.722\n",
      "\n",
      "--------------- try_C: 0.100, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.621\n",
      "\n",
      "--------------- try_C: 0.100, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.594\n",
      "\n",
      "--------------- try_C: 0.300, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.656\n",
      "\n",
      "--------------- try_C: 0.300, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.593\n",
      "\n",
      "--------------- try_C: 0.300, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.601\n",
      "\n",
      "--------------- try_C: 0.300, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.752\n",
      "\n",
      "--------------- try_C: 0.300, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.608\n",
      "\n",
      "--------------- try_C: 0.300, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.678\n",
      "\n",
      "--------------- try_C: 0.300, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.605\n",
      "\n",
      "--------------- try_C: 1.000, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.564\n",
      "\n",
      "--------------- try_C: 1.000, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.571\n",
      "\n",
      "--------------- try_C: 1.000, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.668\n",
      "\n",
      "--------------- try_C: 1.000, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.611\n",
      "\n",
      "--------------- try_C: 1.000, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.627\n",
      "\n",
      "--------------- try_C: 1.000, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.618\n",
      "\n",
      "--------------- try_C: 1.000, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.637\n",
      "\n",
      "--------------- try_C: 3.000, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.812\n",
      "\n",
      "--------------- try_C: 3.000, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.506\n",
      "\n",
      "--------------- try_C: 3.000, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.541\n",
      "\n",
      "--------------- try_C: 3.000, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.690\n",
      "\n",
      "--------------- try_C: 3.000, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.683\n",
      "\n",
      "--------------- try_C: 3.000, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.684\n",
      "\n",
      "--------------- try_C: 3.000, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.717\n",
      "\n",
      "--------------- try_C: 10.000, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.666\n",
      "\n",
      "--------------- try_C: 10.000, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.628\n",
      "\n",
      "--------------- try_C: 10.000, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.608\n",
      "\n",
      "--------------- try_C: 10.000, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.596\n",
      "\n",
      "--------------- try_C: 10.000, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.686\n",
      "\n",
      "--------------- try_C: 10.000, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.672\n",
      "\n",
      "--------------- try_C: 10.000, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.592\n",
      "\n",
      "--------------- try_C: 30.000, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.602\n",
      "\n",
      "--------------- try_C: 30.000, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.644\n",
      "\n",
      "--------------- try_C: 30.000, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.668\n",
      "\n",
      "--------------- try_C: 30.000, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.607\n",
      "\n",
      "--------------- try_C: 30.000, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.661\n",
      "\n",
      "--------------- try_C: 30.000, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.631\n",
      "\n",
      "--------------- try_C: 30.000, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.635\n",
      "\n",
      "--------------- try_C: 100.000, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.611\n",
      "\n",
      "--------------- try_C: 100.000, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.615\n",
      "\n",
      "--------------- try_C: 100.000, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.669\n",
      "\n",
      "--------------- try_C: 100.000, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.657\n",
      "\n",
      "--------------- try_C: 100.000, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.729\n",
      "\n",
      "--------------- try_C: 100.000, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.638\n",
      "\n",
      "--------------- try_C: 100.000, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.624\n",
      "\n",
      "--------------- try_C: 300.000, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.627\n",
      "\n",
      "--------------- try_C: 300.000, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.526\n",
      "\n",
      "--------------- try_C: 300.000, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.683\n",
      "\n",
      "--------------- try_C: 300.000, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.778\n",
      "\n",
      "--------------- try_C: 300.000, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.648\n",
      "\n",
      "--------------- try_C: 300.000, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.557\n",
      "\n",
      "--------------- try_C: 300.000, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.682\n",
      "\n",
      "--------------- try_C: 1000.000, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.595\n",
      "\n",
      "--------------- try_C: 1000.000, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.658\n",
      "\n",
      "--------------- try_C: 1000.000, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.602\n",
      "\n",
      "--------------- try_C: 1000.000, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.722\n",
      "\n",
      "--------------- try_C: 1000.000, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.555\n",
      "\n",
      "--------------- try_C: 1000.000, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.575\n",
      "\n",
      "--------------- try_C: 1000.000, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.635\n",
      "\n",
      "--------------- try_C: 3000.000, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.523\n",
      "\n",
      "--------------- try_C: 3000.000, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.651\n",
      "\n",
      "--------------- try_C: 3000.000, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.706\n",
      "\n",
      "--------------- try_C: 3000.000, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.674\n",
      "\n",
      "--------------- try_C: 3000.000, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.556\n",
      "\n",
      "--------------- try_C: 3000.000, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.571\n",
      "\n",
      "--------------- try_C: 3000.000, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.685\n",
      "\n",
      "--------------- 2/47, Query: (1, 3), Best_C: 3.000, Best_alpha: 0.010 ---------------\n",
      "\n",
      "\n",
      "--------------- try_C: 0.010, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.594\n",
      "\n",
      "--------------- try_C: 0.010, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.633\n",
      "\n",
      "--------------- try_C: 0.010, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.744\n",
      "\n",
      "--------------- try_C: 0.010, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.731\n",
      "\n",
      "--------------- try_C: 0.010, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.553\n",
      "\n",
      "--------------- try_C: 0.010, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.623\n",
      "\n",
      "--------------- try_C: 0.010, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.627\n",
      "\n",
      "--------------- try_C: 0.030, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.592\n",
      "\n",
      "--------------- try_C: 0.030, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.573\n",
      "\n",
      "--------------- try_C: 0.030, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.651\n",
      "\n",
      "--------------- try_C: 0.030, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.624\n",
      "\n",
      "--------------- try_C: 0.030, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.588\n",
      "\n",
      "--------------- try_C: 0.030, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.615\n",
      "\n",
      "--------------- try_C: 0.030, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.714\n",
      "\n",
      "--------------- try_C: 0.100, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.658\n",
      "\n",
      "--------------- try_C: 0.100, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.739\n",
      "\n",
      "--------------- try_C: 0.100, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.661\n",
      "\n",
      "--------------- try_C: 0.100, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.701\n",
      "\n",
      "--------------- try_C: 0.100, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.575\n",
      "\n",
      "--------------- try_C: 0.100, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.530\n",
      "\n",
      "--------------- try_C: 0.100, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.627\n",
      "\n",
      "--------------- try_C: 0.300, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.690\n",
      "\n",
      "--------------- try_C: 0.300, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.578\n",
      "\n",
      "--------------- try_C: 0.300, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.652\n",
      "\n",
      "--------------- try_C: 0.300, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.745\n",
      "\n",
      "--------------- try_C: 0.300, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.620\n",
      "\n",
      "--------------- try_C: 0.300, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.739\n",
      "\n",
      "--------------- try_C: 0.300, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.692\n",
      "\n",
      "--------------- try_C: 1.000, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.597\n",
      "\n",
      "--------------- try_C: 1.000, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.605\n",
      "\n",
      "--------------- try_C: 1.000, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.516\n",
      "\n",
      "--------------- try_C: 1.000, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.645\n",
      "\n",
      "--------------- try_C: 1.000, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.563\n",
      "\n",
      "--------------- try_C: 1.000, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.688\n",
      "\n",
      "--------------- try_C: 1.000, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.555\n",
      "\n",
      "--------------- try_C: 3.000, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.625\n",
      "\n",
      "--------------- try_C: 3.000, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.563\n",
      "\n",
      "--------------- try_C: 3.000, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.552\n",
      "\n",
      "--------------- try_C: 3.000, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.596\n",
      "\n",
      "--------------- try_C: 3.000, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.646\n",
      "\n",
      "--------------- try_C: 3.000, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.568\n",
      "\n",
      "--------------- try_C: 3.000, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.558\n",
      "\n",
      "--------------- try_C: 10.000, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.604\n",
      "\n",
      "--------------- try_C: 10.000, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.571\n",
      "\n",
      "--------------- try_C: 10.000, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.604\n",
      "\n",
      "--------------- try_C: 10.000, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.675\n",
      "\n",
      "--------------- try_C: 10.000, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.607\n",
      "\n",
      "--------------- try_C: 10.000, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.680\n",
      "\n",
      "--------------- try_C: 10.000, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.598\n",
      "\n",
      "--------------- try_C: 30.000, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.572\n",
      "\n",
      "--------------- try_C: 30.000, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.650\n",
      "\n",
      "--------------- try_C: 30.000, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.595\n",
      "\n",
      "--------------- try_C: 30.000, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.678\n",
      "\n",
      "--------------- try_C: 30.000, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.638\n",
      "\n",
      "--------------- try_C: 30.000, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.602\n",
      "\n",
      "--------------- try_C: 30.000, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.574\n",
      "\n",
      "--------------- try_C: 100.000, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.726\n",
      "\n",
      "--------------- try_C: 100.000, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.571\n",
      "\n",
      "--------------- try_C: 100.000, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.630\n",
      "\n",
      "--------------- try_C: 100.000, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.576\n",
      "\n",
      "--------------- try_C: 100.000, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.613\n",
      "\n",
      "--------------- try_C: 100.000, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.587\n",
      "\n",
      "--------------- try_C: 100.000, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.664\n",
      "\n",
      "--------------- try_C: 300.000, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.608\n",
      "\n",
      "--------------- try_C: 300.000, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.636\n",
      "\n",
      "--------------- try_C: 300.000, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.535\n",
      "\n",
      "--------------- try_C: 300.000, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.657\n",
      "\n",
      "--------------- try_C: 300.000, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.622\n",
      "\n",
      "--------------- try_C: 300.000, try_alpha: 0.900 ---------------\n",
      "\n",
      "mean_Tau: 0.554\n",
      "\n",
      "--------------- try_C: 300.000, try_alpha: 0.990 ---------------\n",
      "\n",
      "mean_Tau: 0.624\n",
      "\n",
      "--------------- try_C: 1000.000, try_alpha: 0.010 ---------------\n",
      "\n",
      "mean_Tau: 0.624\n",
      "\n",
      "--------------- try_C: 1000.000, try_alpha: 0.100 ---------------\n",
      "\n",
      "mean_Tau: 0.532\n",
      "\n",
      "--------------- try_C: 1000.000, try_alpha: 0.300 ---------------\n",
      "\n",
      "mean_Tau: 0.569\n",
      "\n",
      "--------------- try_C: 1000.000, try_alpha: 0.500 ---------------\n",
      "\n",
      "mean_Tau: 0.529\n",
      "\n",
      "--------------- try_C: 1000.000, try_alpha: 0.700 ---------------\n",
      "\n",
      "mean_Tau: 0.689\n",
      "\n",
      "--------------- try_C: 1000.000, try_alpha: 0.900 ---------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e90ee5e2f68e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m                     \u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'probability'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwithNodeWeight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                     \u001b[0mF1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpF1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdat_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys_cv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0mF1_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpF1_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpF1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mTau_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-9877eac079a5>\u001b[0m in \u001b[0;36mfind_ILP\u001b[0;34m(V, E, ps, L, withNodeWeight, alpha)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpulp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGUROBI_CMD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gurobi_cl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgurobi_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# GUROBI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpulp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOIN_CMD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cbc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'-threads'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_JOBS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-strategy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-maxIt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2000000'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#CBC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mvisit_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpois\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpois\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpois\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpois\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0misend_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpois\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpois\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dawei/apps/miniconda3/lib/python3.5/site-packages/pulp/pulp.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, solver, **kwargs)\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \u001b[0;31m#time it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1642\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolutionTime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1643\u001b[0;31m         \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactualSolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1644\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolutionTime\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestoreObjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwasNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummyVar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dawei/apps/miniconda3/lib/python3.5/site-packages/pulp/solvers.py\u001b[0m in \u001b[0;36mactualSolve\u001b[0;34m(self, lp, **kwargs)\u001b[0m\n\u001b[1;32m   1301\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mactualSolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;34m\"\"\"Solve a well formulated lp problem\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve_CBC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mavailable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dawei/apps/miniconda3/lib/python3.5/site-packages/pulp/solvers.py\u001b[0m in \u001b[0;36msolve_CBC\u001b[0;34m(self, lp, use_mps)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         cbc = subprocess.Popen((self.path + cmds).split(), stdout = pipe,\n\u001b[1;32m   1361\u001b[0m                              stderr = pipe)\n\u001b[0;32m-> 1362\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1363\u001b[0m             raise PulpSolverError(\"Pulp: Error while trying to execute \" +  \\\n\u001b[1;32m   1364\u001b[0m                                     self.path)\n",
      "\u001b[0;32m/home/dawei/apps/miniconda3/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout, endtime)\u001b[0m\n\u001b[1;32m   1656\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1659\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dawei/apps/miniconda3/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1606\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1609\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "recdict = dict()\n",
    "cnt = 1\n",
    "keys = sorted(dat_obj.TRAJID_GROUP_DICT.keys())\n",
    "inference_fun = find_ILP\n",
    "\n",
    "# outer loop to evaluate the test performance by cross validation\n",
    "for i in range(len(keys)):\n",
    "    ps, L = keys[i]\n",
    "\n",
    "    best_C = 1\n",
    "    best_alpha = 0.5\n",
    "    best_Tau = 0\n",
    "    keys_cv = keys[:i] + keys[i+1:]\n",
    "\n",
    "    # use all training+validation set to compute POI features,\n",
    "    # make sure features do NOT change for training and validation\n",
    "    trajid_set_i = set(dat_obj.trajid_set_all) - dat_obj.TRAJID_GROUP_DICT[keys[i]]\n",
    "    poi_info_i = dat_obj.calc_poi_info(list(trajid_set_i))\n",
    "    poi_set_i = {p for tid in trajid_set_i for p in dat_obj.traj_dict[tid] if len(dat_obj.traj_dict[tid]) >= 2}\n",
    "    if ps not in poi_set_i: \n",
    "        sys.stderr.write('start POI of query %s does not exist in training set.\\n' % str(keys[i]))\n",
    "        continue\n",
    "\n",
    "    # tune regularisation constant C\n",
    "    for rank_C in C_SET:\n",
    "        for alpha in ALPHA_SET:\n",
    "            print('\\n--------------- try_C: %.3f, try_alpha: %.3f ---------------\\n' % (rank_C, alpha))\n",
    "            sys.stdout.flush()\n",
    "            F1_list = []; pF1_list = []; Tau_list = []        \n",
    "\n",
    "            # inner loop to evaluate the performance of a model with a specified C by Monte-Carlo cross validation\n",
    "            for j in range(MC_NITER):\n",
    "                poi_list = []\n",
    "                while True: # make sure the start POI in test set are also in training set\n",
    "                    rand_ix = np.arange(len(keys_cv)); np.random.shuffle(rand_ix)\n",
    "                    test_ix = rand_ix[:int(MC_PORTION*len(rand_ix))]\n",
    "                    assert(len(test_ix) > 0)\n",
    "                    trajid_set_train = set(dat_obj.trajid_set_all) - dat_obj.TRAJID_GROUP_DICT[keys[i]]\n",
    "                    for j in test_ix: \n",
    "                        trajid_set_train = trajid_set_train - dat_obj.TRAJID_GROUP_DICT[keys_cv[j]]\n",
    "                    poi_set = {poi for tid in trajid_set_train for poi in dat_obj.traj_dict[tid]}\n",
    "                    good_partition = True\n",
    "                    for j in test_ix: \n",
    "                        if keys_cv[j][0] not in poi_set: good_partition = False; break\n",
    "                    if good_partition == True: \n",
    "                        poi_list = sorted(poi_set)\n",
    "                        break\n",
    "\n",
    "                # train\n",
    "                train_df = gen_train_df(list(trajid_set_train), poi_info_i.loc[poi_list].copy(), dat_obj, n_jobs=N_JOBS)\n",
    "                ranksvm = RankSVM(ranksvm_dir, useLinear=True)\n",
    "                ranksvm.train(train_df, cost=rank_C)\n",
    "                poi_logtransmat = gen_poi_logtransmat(trajid_set_train,poi_list,poi_info_i.loc[poi_list].copy(),dat_obj)\n",
    "                edges = poi_logtransmat                \n",
    "\n",
    "                # test\n",
    "                for j in test_ix:  # test\n",
    "                    ps_cv, L_cv = keys_cv[j]\n",
    "                    test_df = gen_test_df(ps_cv, L_cv, poi_info_i.loc[poi_list].copy(), dat_obj)\n",
    "                    rank_df = ranksvm.predict(test_df, probability=True)\n",
    "                    nodes = rank_df.copy()\n",
    "                    nodes['weight'] = np.log10(nodes['probability'])\n",
    "\n",
    "                    y_hat = inference_fun(nodes, edges.copy(), ps_cv, L_cv, withNodeWeight=True, alpha=alpha)\n",
    "                    F1, pF1, tau = evaluate(dat_obj, keys_cv[j], [y_hat])\n",
    "                    F1_list.append(F1); pF1_list.append(pF1); Tau_list.append(tau)\n",
    "\n",
    "            mean_Tau = np.mean(Tau_list)\n",
    "            print('mean_Tau: %.3f' % mean_Tau)\n",
    "            if mean_Tau > best_Tau:\n",
    "                best_Tau = mean_Tau\n",
    "                best_C = rank_C\n",
    "                best_alpha = alpha\n",
    "    print('\\n--------------- %d/%d, Query: (%d, %d), Best_C: %.3f, Best_alpha: %.3f ---------------\\n' % \\\n",
    "          (cnt, len(keys), ps, L, best_C, best_alpha))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # train model using all examples in training set and measure performance on test set\n",
    "    train_df = gen_train_df(list(trajid_set_i), poi_info_i.copy(), dat_obj, n_jobs=N_JOBS)\n",
    "    ranksvm = RankSVM(ranksvm_dir, useLinear=True)\n",
    "    ranksvm.train(train_df, cost=best_C)\n",
    "    test_df = gen_test_df(ps, L, poi_info_i, dat_obj)\n",
    "    rank_df = ranksvm.predict(test_df, probability=True)\n",
    "    nodes = rank_df.copy()\n",
    "    nodes['weight'] = np.log10(nodes['probability'])\n",
    "    poi_logtransmat = gen_poi_logtransmat(trajid_set_i, set(poi_info_i.index), poi_info_i, dat_obj)\n",
    "    edges = poi_logtransmat \n",
    "\n",
    "    y_hat = inference_fun(nodes, edges, ps, L, withNodeWeight=True, alpha=best_alpha)\n",
    "    recdict[(ps, L)] = {'PRED': [y_hat], 'C': best_C, 'alpha': best_alpha}\n",
    "\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = os.path.join(data_dir, 'rankmarkovpath-' + dat_suffix[dat_ix] + '.pkl')\n",
    "pickle.dump(recdict, open(fname, 'bw'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
