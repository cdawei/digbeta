{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playlist generation by multilable learning (P-Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, time\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer, label_ranking_loss\n",
    "from scipy.sparse import lil_matrix, issparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('src')\n",
    "from PClassificationMLC import PClassificationMLC\n",
    "from BinaryRelevance import BinaryRelevance\n",
    "from evaluate import evaluatePrecision, evalPred, avgPrecisionK, f1_score_nowarn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/aotm-2011'\n",
    "fxtrain = os.path.join(data_dir, 'X_train_audio.pkl')\n",
    "fytrain = os.path.join(data_dir, 'Y_train_audio.pkl')\n",
    "fxdev   = os.path.join(data_dir, 'X_dev_audio.pkl')\n",
    "fydev   = os.path.join(data_dir, 'Y_dev_audio.pkl')\n",
    "fxtest  = os.path.join(data_dir, 'X_test_audio.pkl')\n",
    "fytest  = os.path.join(data_dir, 'Y_test_audio.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pkl.load(open(fxtrain, 'rb'))\n",
    "Y_train = pkl.load(open(fytrain, 'rb'))\n",
    "X_dev   = pkl.load(open(fxdev,   'rb'))\n",
    "Y_dev   = pkl.load(open(fydev,   'rb'))\n",
    "X_test  = pkl.load(open(fxtest,  'rb'))\n",
    "Y_test  = pkl.load(open(fytest,  'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train: %15s %15s' % (X_train.shape, Y_train.shape))\n",
    "print('Dev  : %15s %15s' % (X_dev.shape,   Y_dev.shape))\n",
    "print('Test : %15s %15s' % (X_test.shape,  Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns with all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ks = np.dot(Y_train, np.ones(Y_train.shape[1]))\n",
    "nullcols = []\n",
    "K = Y_train.shape[1]\n",
    "for k in np.arange(K):\n",
    "    kpos = np.sum(Y_train[:,k])\n",
    "    if kpos == 0: \n",
    "        nullcols.append(k)\n",
    "    if (k+1) % 100 == 0:\n",
    "        sys.stdout.write('\\r%d / %d' % (k+1, K))\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nullcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = PClassificationMLC(C=100, p=2, weighting=True)\n",
    "clf.fit_SGD(X_train, Y_train, batch_size=1000, n_epochs=20, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = PClassificationMLC(C=100, p=2, weighting=True)\n",
    "clf1.fit_SGD(X_train, Y_train, w=w0, batch_size=200, n_epochs=10, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_SGD(clf, eval_func, X_test, Y_test, threshold=None, batch_size=100):\n",
    "    assert X_test.shape[0] == Y_test.shape[0]\n",
    "    \n",
    "    N = X_test.shape[0]\n",
    "    metrics_all = []\n",
    "    n_batches = int((N-1) / batch_size) + 1\n",
    "    indices = np.arange(N)\n",
    "    \n",
    "    for nb in range(n_batches):\n",
    "        sys.stdout.write('\\r %d / %d' % (nb+1, n_batches))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        ix_start = nb * batch_size\n",
    "        ix_end = min((nb+1) * batch_size, N)\n",
    "        ix = indices[ix_start:ix_end]\n",
    "        \n",
    "        X = X_test[ix]\n",
    "        Y_true = Y_test[ix]\n",
    "        if issparse(Y_true):\n",
    "            Y_true = Y_true.toarray()\n",
    "        Y_pred = clf.decision_function(X)\n",
    "        if issparse(Y_pred):\n",
    "            Y_pred = Y_pred.toarray()\n",
    "        if threshold is not None:\n",
    "            Y_pred = Y_pred >= threshold\n",
    "            \n",
    "        metrics = eval_func(Y_true, Y_pred)\n",
    "        metrics_all = np.concatenate((metrics_all, metrics), axis=-1)\n",
    "        \n",
    "    return metrics_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_F1(Y_true, Y_pred):\n",
    "    \"\"\"\n",
    "    Compute F1 scores for multilabel prediction, one score for each example.\n",
    "    precision = true_positive / n_true\n",
    "    recall = true_positive / n_positive\n",
    "    f1 = (2 * precision * recall) / (precision + recall) = 2 * true_positive / (n_true + n_positive)\n",
    "    \"\"\"\n",
    "    assert Y_true.shape == Y_pred.shape\n",
    "    N, K = Y_true.shape\n",
    "    OneK = np.ones(K)\n",
    "    \n",
    "    n_true = np.dot(Y_true, OneK)\n",
    "    n_positive = np.dot(Y_pred, OneK)\n",
    "    true_positive = np.dot(np.multiply(Y_true, Y_pred), OneK)\n",
    "    \n",
    "    numerator = 2 * true_positive\n",
    "    denominator = n_true + n_positive\n",
    "    nonzero_ix = np.nonzero(denominator)[0]\n",
    "    \n",
    "    f1 = np.zeros(N)\n",
    "    f1[nonzero_ix] = np.divide(numerator[nonzero_ix], denominator[nonzero_ix])\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_precisionK(Y_true, Y_pred):\n",
    "    \"\"\"\n",
    "    Compute Precision@K, one score for each example.\n",
    "    - thresholding predictions using the K-th largest predicted score, K is #positives in ground truth\n",
    "    - Precision@K: #true_positives / #positives_in_ground_truth\n",
    "      where by the definition of Precision@K, #positives_in_ground_truth = #positive_in_prediction\n",
    "    \"\"\"\n",
    "    assert Y_true.shape == Y_pred.shape\n",
    "    N, K = Y_true.shape\n",
    "    OneK = np.ones(K)\n",
    "    KPosAll = np.dot(Y_true, OneK).astype(np.int)\n",
    "    assert np.all(KPosAll > 0)\n",
    "    \n",
    "    rows = np.arange(N)\n",
    "    sortedIx = np.argsort(-Y_pred, axis=1)\n",
    "    cols = sortedIx[rows, KPosAll-1]  # index of thresholds (the K-th largest scores, NOTE index starts at 0)\n",
    "    thresholds = Y_pred[rows, cols]   # the K-th largest scores\n",
    "    Y_pred_bin = Y_pred >= thresholds[:, None]  # convert scores to binary predictions\n",
    "    \n",
    "    true_positives = np.multiply(Y_true, Y_pred_bin)\n",
    "    return np.dot(true_positives, OneK) / KPosAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test `calc_precisionK`\n",
    "n = 50\n",
    "k = 100000\n",
    "for i in range(100):\n",
    "    sys.stdout.write('\\r%d / %d' % (i+1, 100))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    y0 = np.random.rand(n, k) >= 0.9996\n",
    "    y0 = y0.astype(np.int)\n",
    "    print('\\navg K:', np.mean(np.sum(y0, axis=1)))\n",
    "    y1 = np.random.randn(n, k)\n",
    "    pk1 = avgPrecisionK(y0, y1)\n",
    "    pk2 = np.mean(calc_precisionK(y0, y1))\n",
    "    assert np.equal(pk1, pk2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THs = [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for th in THs:\n",
    "    metrics = evaluate(clf=clf, eval_func=calcF1, X_test=X_test, Y_test=Y_test, threshold=th, batch_size=500)\n",
    "    print(' Threshold: %.2g, F1: %g' % (th, np.mean(metrics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import inspect\n",
    "#inspect.signature(evaluate).parameters.keys()\n",
    "#'threshold' in inspect.signature(evaluate).parameters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT needed, we can train in parallel\n",
    "class GridSearch():\n",
    "    \"\"\"Validation by grid search, supported params: C, p\"\"\"\n",
    "    \n",
    "    def __init__(self, estimator, param_grid, scorer):\n",
    "        self.estimator = estimator\n",
    "        if self.check_grid(param_grid):\n",
    "            self.param_grid = param_grid\n",
    "        self.scorer = scorer\n",
    "        \n",
    "    def check_grid(self, grid):\n",
    "        \"\"\"Currently only support a few specific parameters\"\"\"\n",
    "        if len(grid) > 1: \n",
    "            print('Please put all parameter configurations in the first dictionary!')\n",
    "            return False\n",
    "        \n",
    "        supported = ['C', 'p']\n",
    "        #for d in grid:\n",
    "        #for key in d.keys():\n",
    "        for key in grid[0].keys():\n",
    "            if key not in supported:\n",
    "                print('Parameter %s NOT supported!' % key)\n",
    "                return False\n",
    "        return True\n",
    "            \n",
    "    def fit(self, X_train, Y_train, X_dev, Y_dev):\n",
    "        results = dict()\n",
    "        params = self.param_grid[0].keys()\n",
    "        if len(params) > 1:\n",
    "            for C in params['C']:\n",
    "                for p in params['p']:\n",
    "                    clf = self.estimator(X_train=X_train, Y_train=Y_train, C=C, p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ranges = range(-6, 7)\n",
    "#ranges = range(-6, 5)\n",
    "#parameters = [{'C': sorted([10**(e) for e in ranges] + [3 * 10**(e) for e in ranges]),\n",
    "               #'r': [0.5, 1, 2, 4]}]\n",
    "#scorer = {'Prec': make_scorer(avgPrecisionK)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clf1 = GridSearchCV(PClassificationMLC(), parameters, scoring=scorer, cv=5, n_jobs=1, refit='Prec')\n",
    "#clf1.fit(X1_train, Y1_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
