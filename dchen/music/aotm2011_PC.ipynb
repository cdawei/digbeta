{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple example of generating playlist by multilable learning (toppush)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, time\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer, label_ranking_loss\n",
    "from scipy.sparse import lil_matrix, issparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('src')\n",
    "from PClassificationMLC import PClassificationMLC\n",
    "from evaluate import evaluatePrecision, evalPred, avgPrecisionK, f1_score_nowarn\n",
    "from BinaryRelevance import BinaryRelevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "faotm = os.path.join(data_dir, 'aotm-2011/aotm-2011-subset.pkl')\n",
    "#fmap  = os.path.join(data_dir, 'aotm-2011/songID2TrackID.pkl')\n",
    "ffeature = os.path.join(data_dir, 'msd/songID2Features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx      = os.path.join(data_dir, 'aotm-2011/X_audio.pkl')\n",
    "fy      = os.path.join(data_dir, 'aotm-2011/Y_audio.pkl')\n",
    "fxtrain = os.path.join(data_dir, 'aotm-2011/X_train_audio.pkl')\n",
    "fytrain = os.path.join(data_dir, 'aotm-2011/Y_train_audio.pkl')\n",
    "fxtest  = os.path.join(data_dir, 'aotm-2011/X_test_audio.pkl')\n",
    "fytest  = os.path.join(data_dir, 'aotm-2011/Y_test_audio.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load playlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists = pkl.load(open(faotm, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#Playlists: %d' % len(playlists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('#Songs: %d' % len({songID for p in playlists for songID in p['filtered_lists'][0]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lengths = [len(p['filtered_lists'][0]) for p in playlists]\n",
    "lengths = [len(sl) for sl in playlists]\n",
    "plt.hist(lengths, bins=20)\n",
    "print('Average playlist length: %.1f' % np.mean(lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `song_id` --> `track_id` mapping: a song may correspond to multiple tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#song2TrackID = pkl.load(open(fmap, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{ k : song2TrackID[k] for k in list(song2TrackID.keys())[:10] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `song_id` --> `feature array` mapping: map a song to the audio features of one of its corresponding tracks in MSD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2Features = pkl.load(open(ffeature, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of songs, which is the set of labels in this formulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#song_set = sorted(song2Features.keys())  # use MSD songs as label space\n",
    "song_set = sorted({sid for pl in playlists for sid in pl})   # use the intersection of MSD and AotM as label space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(song_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_indices = {songID: ix for ix, songID in enumerate(song_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(label_indices.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_training_set(playlists=playlists, label_indices=label_indices, features=song2Features):\n",
    "    \"\"\"\n",
    "        Create the labelled dataset for a given song index\n",
    "        \n",
    "        Input:\n",
    "            - playlists: which playlists to create features for\n",
    "            - label_indices: a dictionary that maps a songID to the index of the corresponding label\n",
    "            - features: a dictionary that maps a songID to its feature vector\n",
    "            \n",
    "        Output:\n",
    "            - (Feature, Label) pair (X, Y), with # num playlists rows\n",
    "              X comprises the features for each seed song\n",
    "              Y comprises the indicators of whether the given song is present in the respective playlist\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(playlists)\n",
    "    K = len(label_indices)\n",
    "\n",
    "    X = [ ]\n",
    "    Y = lil_matrix((N, K), dtype=np.int8)\n",
    "    \n",
    "    cnt = 0\n",
    "    for i in range(len(playlists)):\n",
    "        cnt += 1\n",
    "        if cnt % 100 == 0:\n",
    "            sys.stdout.write('\\r%d / %d' % (cnt, len(playlists)))\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        playlist = playlists[i]\n",
    "        seed     = playlist[0]\n",
    "\n",
    "        X.append(features[seed])\n",
    "        #indices = [label_indices[s] for s in playlist]\n",
    "        indices = [label_indices[s] for s in playlist if s in label_indices]\n",
    "        Y[i, indices] = 1\n",
    "\n",
    "    return np.array(X), Y.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {1: 0, 2: 1, 3: 2}\n",
    "[test_dict[s] for s in [1, 2, 5] if s in test_dict]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a logistic regression model for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.all([os.path.exists(fname) for fname in [fxtrain, fytrain, fxtest, fytest]]):\n",
    "    X_train = pkl.load(open(fxtrain, 'rb'))\n",
    "    Y_train = pkl.load(open(fytrain, 'rb'))\n",
    "    X_test  = pkl.load(open(fxtest,  'rb'))\n",
    "    Y_test  = pkl.load(open(fytest,  'rb'))\n",
    "else:\n",
    "    X, Y = gen_training_set(playlists=playlists, label_indices=label_indices, features=song2Features)\n",
    "    # by fixing random seed, the same playlists will be in the test set each time\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=31)\n",
    "    pkl.dump(X,       open(fx,      'wb'))\n",
    "    pkl.dump(Y,       open(fy,      'wb'))\n",
    "    pkl.dump(X_train, open(fxtrain, 'wb'))\n",
    "    pkl.dump(Y_train, open(fytrain, 'wb'))\n",
    "    pkl.dump(X_test,  open(fxtest,  'wb'))\n",
    "    pkl.dump(Y_test,  open(fytest,  'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mean = np.mean(X_train, axis=0).reshape((1, -1))\n",
    "X_train_std = np.std(X_train, axis=0).reshape((1, -1)) + 10 ** (-6)\n",
    "X_train -= X_train_mean\n",
    "X_train /= X_train_std\n",
    "X_test  -= X_train_mean\n",
    "X_test  /= X_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgF1(Y_true, Y_pred):\n",
    "    F1 = f1_score_nowarn(Y_true, Y_pred >= 0, average='samples')\n",
    "    print('F1: %g, #examples: %g' % (F1, Y_true.shape[0]))\n",
    "    return F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = PClassificationMLC(C=100, p=2, weighting=True)\n",
    "clf.fit_SGD(X_train, Y_train, batch_size=200, n_epochs=20, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumpclf(clf, batch_size, n_epochs, learning_rate):\n",
    "    C = clf.C\n",
    "    p = clf.p\n",
    "    weighting = clf.weighting\n",
    "    fname = 'model-%d-%g-%s-%d-%d-%g.pkl' % (C, p, weighting, batch_size, n_epochs, learning_rate)\n",
    "    w = np.concatenate((clf.b, clf.W.ravel()), axis=-1)\n",
    "    pkl.dump(w, open(fname, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumpclf(clf, batch_size=200, n_epochs=10, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'model-%d-%g-%s-%d-%d-%g.pkl' % (100, 2, True, 200, 10, 0.05)\n",
    "W0 = pkl.load(open(fname, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = PClassificationMLC(C=100, p=2, weighting=True)\n",
    "clf1.fit_SGD(X_train, Y_train, w=w0, batch_size=200, n_epochs=10, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(avgF1(Y_train, clf.decision_function(X_train)))\n",
    "#print(avgF1(Y_test, clf.decision_function(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect.signature(evaluate).parameters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'threshold' in inspect.signature(evaluate).parameters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(clf, eval_func, X_test, Y_test, threshold=None, batch_size=100):\n",
    "    assert X_test.shape[0] == Y_test.shape[0]\n",
    "    \n",
    "    N = X_test.shape[0]\n",
    "    metrics_all = []\n",
    "    n_batches = int((N-1) / batch_size) + 1\n",
    "    indices = np.arange(N)\n",
    "    \n",
    "    for nb in range(n_batches):\n",
    "        sys.stdout.write('\\r %d / %d' % (nb+1, n_batches))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        ix_start = nb * batch_size\n",
    "        ix_end = min((nb+1) * batch_size, N)\n",
    "        ix = indices[ix_start:ix_end]\n",
    "        \n",
    "        X = X_test[ix]\n",
    "        Y_true = Y_test[ix]\n",
    "        if issparse(Y_true):\n",
    "            Y_true = Y_true.toarray()\n",
    "        Y_pred = clf.decision_function(X)\n",
    "        if issparse(Y_pred):\n",
    "            Y_pred = Y_pred.toarray()\n",
    "        if threshold is not None:\n",
    "            Y_pred = Y_pred >= threshold\n",
    "            \n",
    "        metrics = eval_func(Y_true, Y_pred)\n",
    "        metrics_all = np.concatenate((metrics_all, metrics), axis=-1)\n",
    "        \n",
    "    return metrics_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcF1(Y_true, Y_pred):\n",
    "    \"\"\"\n",
    "    Compute F1 scores for multilabel prediction, one score for each example.\n",
    "    precision = true_positive / n_true\n",
    "    recall = true_positive / n_positive\n",
    "    f1 = (2 * precision * recall) / (precision + recall) = 2 * true_positive / (n_true + n_positive)\n",
    "    \"\"\"\n",
    "    assert Y_true.shape == Y_pred.shape\n",
    "    N, K = Y_true.shape\n",
    "    OneK = np.ones(K)\n",
    "    \n",
    "    n_true = np.dot(Y_true, OneK)\n",
    "    n_positive = np.dot(Y_pred, OneK)    \n",
    "    true_positive = np.dot(np.multiply(Y_true, Y_pred), OneK)\n",
    "    \n",
    "    numerator = 2 * true_positive\n",
    "    denominator = n_true + n_positive\n",
    "    nonzero_ix = np.nonzero(denominator)[0]\n",
    "    \n",
    "    f1 = np.zeros(N)\n",
    "    f1[nonzero_ix] = np.divide(numerator[nonzero_ix], denominator[nonzero_ix])\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate(clf=clf, eval_func=calcF1, X_test=X_test, Y_test=Y_test, threshold=0.1, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THs = [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for th in THs:\n",
    "    metrics = evaluate(clf=clf, eval_func=calcF1, X_test=X_test, Y_test=Y_test, threshold=th, batch_size=500)\n",
    "    print(' Threshold: %.2g, F1: %g' % (th, np.mean(metrics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precisionatK(Y_true, Y_pred):\n",
    "    assert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ranges = range(-6, 7)\n",
    "#ranges = range(-6, 5)\n",
    "#parameters = [{'C': sorted([10**(e) for e in ranges] + [3 * 10**(e) for e in ranges]),\n",
    "               #'r': [0.5, 1, 2, 4]}]\n",
    "#scorer = {'Prec': make_scorer(avgPrecisionK)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf1 = GridSearchCV(TopPushMLC(), parameters, scoring=scorer, cv=5, n_jobs=1, refit='Prec')\n",
    "clf1.fit(X1_train, Y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "br1 = GridSearchCV(BinaryRelevance(), param_grid=[{'C': parameters[0]['C']}], scoring=scorer, \\\n",
    "                   cv=5, n_jobs=4, refit='Prec')\n",
    "br1.fit(X1_train, Y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TP1:')\n",
    "evaluatePrecision(Y1_test, clf1.decision_function(X1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BR1:')\n",
    "evaluatePrecision(Y1_test, br1.decision_function(X1_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type2 songs: popularities are somewhat uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type2_songs = np.asarray(song_set)[indices[200:400]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type2_song_features = {sid: song2Features[sid] for sid in type2_songs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type2_song_label_indices = {sid: ix for ix, sid in enumerate(type2_songs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_subset2 = [pl for pl in playlists if pl[0] in type2_song_label_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2, Y2 = gen_training_set(playlists=playlist_subset2, label_indices=type2_song_label_indices, \\\n",
    "                          features=type2_song_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitering out playlists with only one song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y2 = Y2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2 = Y2.sum(axis=1) > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2, Y2 = X2[ind2], Y2[ind2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.sum(Y2, axis=1)).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Popularity histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.sum(Y2, axis=0)).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train, X2_test, Y2_train, Y2_test = train_test_split(X2, Y2, test_size=0.33, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train_mean = np.mean(X2_train, axis=0).reshape((1, -1))\n",
    "X2_train_std = np.std(X2_train, axis=0).reshape((1, -1)) + 10 ** (-6)\n",
    "X2_train -= X2_train_mean\n",
    "X2_train /= X2_train_std\n",
    "X2_test  -= X2_train_mean\n",
    "X2_test  /= X2_train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf2 = GridSearchCV(TopPushMLC(), parameters, scoring=scorer, cv=5, n_jobs=1, refit='Prec')\n",
    "clf2.fit(X2_train, Y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "br2 = GridSearchCV(BinaryRelevance(), param_grid=[{'C': parameters[0]['C']}], scoring=scorer, \\\n",
    "                   cv=5, n_jobs=4, refit='Prec')\n",
    "br2.fit(X2_train, Y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TP2:')\n",
    "evaluatePrecision(Y2_test, clf2.decision_function(X2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BR2:')\n",
    "evaluatePrecision(Y2_test, br2.decision_function(X2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(predictor, X_train, Y_train, X_test, Y_test, trainPerf=False):\n",
    "    \"\"\"\n",
    "        Compute and save performance results\n",
    "    \"\"\"\n",
    "    batch_size = 500\n",
    "    njobs = 16\n",
    "    \n",
    "    p3_test = []\n",
    "    p5_test = []\n",
    "    pk_test = []\n",
    "    p10_test = []\n",
    "    #rankloss_test = []\n",
    "    \n",
    "    N_test = X_test.shape[0]\n",
    "    N_batch_test = int((N_test-1) / batch_size) + 1\n",
    "    for i in range(N_batch_test):\n",
    "        sys.stdout.write('\\r%d / %d' % (i+1, N_batch_test)); sys.stdout.flush()\n",
    "        ix0 = i * batch_size\n",
    "        ix1 = min((i+1) * batch_size, N_test)\n",
    "        preds = predictor.decision_function(X_test[ix0:ix1])\n",
    "        evaldict = evaluatePrecision(Y_test[ix0:ix1].toarray(), preds, verbose=-1, n_jobs=njobs)\n",
    "        size = ix1 - ix0\n",
    "        p3_test.append(evaldict['Precision@3'][0] * size)\n",
    "        p5_test.append(evaldict['Precision@5'][0] * size)\n",
    "        pk_test.append(evaldict['Precision@K'][0] * size)\n",
    "        p10_test.append(evaldict['Precision@10'][0] * size)\n",
    "        #rankloss_test.append(evalPred1(Y_test[i].toarray()[0], pred, metricType='Ranking'))\n",
    "    print()\n",
    "    print('Test set:')\n",
    "    print('Precision@3:', (np.sum(p3_test) / N_test))\n",
    "    print('Precision@5:', (np.sum(p5_test) / N_test))\n",
    "    print('Precision@k:', (np.sum(pk_test) / N_test))\n",
    "    print('Precision@10:', (np.sum(p10_test) / N_test))\n",
    "    print()\n",
    "    \n",
    "    if trainPerf is True:\n",
    "        p3_train = []\n",
    "        p5_train = []\n",
    "        pk_train = []\n",
    "        p10_train = []\n",
    "        #rankloss_train = []\n",
    "\n",
    "        N_train = X_train.shape[0]\n",
    "        N_batch_train = int((N_train-1) / batch_size) + 1\n",
    "        for i in range(N_batch_train):\n",
    "            sys.stdout.write('\\r%d / %d' % (i+1, N_batch_train)); sys.stdout.flush()\n",
    "            ix0 = i * batch_size\n",
    "            ix1 = min((i+1) * batch_size, N_train)\n",
    "            preds = predictor.decision_function(X_train[ix0:ix1])\n",
    "            evaldict = evaluatePrecision(Y_train[ix0:ix1].toarray(), preds, verbose=-1, n_jobs=njobs)\n",
    "            size = ix1 - ix0\n",
    "            p3_train.append(evaldict['Precision@3'][0] * size)\n",
    "            p5_train.append(evaldict['Precision@5'][0] * size)\n",
    "            pk_train.append(evaldict['Precision@K'][0] * size)\n",
    "            p10_train.append(evaldict['Precision@10'][0] * size)\n",
    "            #rankloss_train.append(evalPred1(Y_train[i].toarray()[0], pred, metricType='Ranking'))\n",
    "        print()\n",
    "        print('Training set:')\n",
    "        print('Precision@3:', (np.sum(p3_train) / N_train))\n",
    "        print('Precision@5:', (np.sum(p5_train) / N_train))\n",
    "        print('Precision@k:', (np.sum(pk_train) / N_train))\n",
    "        print('Precision@10:', (np.sum(p10_train) / N_train))\n",
    "    \n",
    "    #print()\n",
    "    #print('Training set:')\n",
    "    #print('RankingLoss: %.1f, %.1f' % (np.mean(rankloss_train), np.std(rankloss_train) / N_train))\n",
    "    #print()\n",
    "    #print('Test set:')\n",
    "    #print('RankingLoss: %.1f, %.1f' % (np.mean(rankloss_test), np.std(rankloss_test) / N_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_info(X_train, Y_train, X_test, Y_test):\n",
    "    N_train, D = X_train.shape\n",
    "    K = Y_train.shape[1]\n",
    "    N_test = X_test.shape[0]\n",
    "    print('%-45s %s' % ('Number of training examples:', '{:,}'.format(N_train)))\n",
    "    print('%-45s %s' % ('Number of test examples:', '{:,}'.format(N_test)))\n",
    "    print('%-45s %s' % ('Number of features:', '{:,}'.format(D)))\n",
    "    print('%-45s %s' % ('Number of labels:', '{:,}'.format(K)))\n",
    "    avgK_train = np.mean(np.sum(Y_train, axis=1))\n",
    "    avgK_test  = np.mean(np.sum(Y_test, axis=1))\n",
    "    print('%-45s %.3f (%.3f%%)' % ('Average number of positive labels (train):', avgK_train, 100*avgK_train / K))\n",
    "    print('%-45s %.3f (%.3f%%)' % ('Average number of positive labels (test):', avgK_test, 100*avgK_test / K))\n",
    "    #print('%-45s %.4f%%' % ('Average label occurrence (train):', np.mean(np.sum(Y_train, axis=0)) / N_train))\n",
    "    #print('%-45s %.4f%%' % ('Average label occurrence (test):', np.mean(np.sum(Y_test, axis=0)) / N_test))\n",
    "    print('%-45s %.3f%%' % ('Sparsity (percent) (train):', 100 * np.sum(Y_train) / np.prod(Y_train.shape)))\n",
    "    print('%-45s %.3f%%' % ('Sparsity (percent) (test):', 100 * np.sum(Y_test) / np.prod(Y_test.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dataset_info(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ = TopPushMLC(C=10000, r=2)\n",
    "clf_.fit_SGD(X_train, Y_train, batch_size=500, n_epochs=10, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(clf_, X_train, Y_train, X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
