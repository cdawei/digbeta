% 1. Recommend songs for playlists
% Problems description
% - playlist augmentation
% - new song recommendation
% - related work: the same, the different
%
% 2. Playlist augmentation and bipartite ranking
% Method:
% - augment one playlist can be achieved by bipartite ranking (assume order of tracks is irrelevant)
% - known: bipartite ranking == binary classification (under assumptions)
% - Theorem 1
% 
% 3. Playlist augmentation and multi-label classification
% Method:
% - augment multiple playlists -> multiple bipartite ranking -> multi-label classification
% - Theorem 2
% - one user has multiple playlists -> multi-task regularisation
%
% 4. New song recommendation: an extension of playlist augmentation
% Method:
% - the same as previous work (cold start)
% - the different (?)
%
% 5. Experiment
% - multi-label classification
% - playlist augmentation
% - new song recommendation
% 1. Recommend songs for playlists
% Problems description
% - playlist augmentation
% - new song recommendation
% - related work: the same, the different
%

\section{Problem}

% brief description of both recommendation tasks
We are motivated by the problem of automatically augmenting music playlist with a collection of songs $\SCal$,
in particular, given a partial playlist with the first $K$ songs\footnote{$K$ can be different for different partial playlists.},
we would like to recommend a subset of $\SCal$ by learning from user created playlist dataset.
This task is also known as Automatic Playlist Continuation~\cite{schedl2017,recsysch2018}.


\section{Related work}
% related work
% describe each task with related work: the same, the different
% describe that we don't use the order of songs, conflicting findings in literature
% mainly two pieces of work:
% - music recommendation
% - playlist generation
{\it Some related work.}

In this paper, we describe two variants of the playlist recommendation problem,
one is augmenting a playlist by recommending a subset of songs from a collection of music $\SCal$,
given the first $K$ seed songs, where $K$ can be any positive integer from 1 to the total number of songs in playlist minus 1.
% in contrast to settings where all songs except the last one are observed\cite{}, or giving a fixed number of seed songs\cite{}.
Another variant is restricting that all songs to recommend are not observed during learning,
\ie in the setting of recommending newly released songs to augment a given playlist, which is an instance of the cold-start problem.
We call the first variant \emph{playlist augmentation} and the second \emph{new song recommendation}.


\section{Machine learning tasks for playlist recommendation}

We formulate the task of \emph{playlist augmentation} as a multi-label classification problem,
that is, for each song that is not in the given playlist, 
we predict whether it will be added to the given playlist.
This formulation is illustrated in Figure~\ref{fig:pla},
where rows represent songs (no specific order) and columns represent playlists (no specific order).
Further, columns with white colour represent playlists in training set, 
and columns with grey colour represent playlists that should be augmented (\ie test set).
If entry $(i, j)$ is \texttt{1} (or \texttt{0}), 
it means the $i$-th song is (or not) found in the $j$-th playlist, 
and a question mark \texttt{?} means that we do not know whether the $i$-th song is found in the $j$-th playlist.
As a remark, columns represent playlists in test set contain only \texttt{1} and \texttt{?} entries.

\input{fig_pla}


\paragraph{New song recommendation}
We formulate the task of \emph{new song recommendation} as a multi-label classification problem, 
where we predict, for each song in test set,
whether it will be included in a given playlist.
This formulation is illustrated in Figure~\ref{fig:mlr},
where rows represent songs (from top to bottom, sorted by the release date in ascending order)
and columns represent playlists (no specific order).
Further, rows with white colour represent songs in training set, and rows with grey colour represent songs in test set.
If entry $(i, j)$ is \texttt{1} (or \texttt{0}), it means the $i$-th song is (or not) found in the $j$-th playlist,
otherwise, we do not know whether the $i$-th song is found in the $j$-th playlist (\ie entry $(i, j)$ is a question mark \texttt{?}).
As a remark, we do not care about the order of songs in a playlist.

\input{fig_mlr}



\section{Method I: Playlist augmentation as multi-label classification}
{\it Two methods are described here, but only one of them is necessary for this paper.}


% so we can just do multiple bipartite ranking for these two problems?
% we can actually do better than this, given \ref{ertekin2011equivalence} showed that P-Classification == P-Norm Push, 
% we have (equivalence in binary setting) and (extend the equivalence to multi-label setting)
% so 
% - an independent  bipartite ranking seems to be a better baseline?
% - bottom push on MLC dataset?
% - Theorem 1
\input{binary}

% 3. Playlist augmentation and multi-label classification
% Method:
% - augment multiple playlists -> multiple bipartite ranking -> multi-label classification
% - Theorem 2
% - one user has multiple playlists -> multi-task regularisation
%
\input{multilabel}

% 4. New song recommendation: an extension of playlist augmentation
% Method:
% - the same as previous work (cold start)
% - the different (?)
%


\section{Method II: Playlist augmentation as bipartite ranking}
% For users with only one playlist, describe the Top push loss for bipartite ranking, and its dual
% For users with multiple playlists, use multitask regularisation + bipartite ranking w/ Top Push loss for each playlist, and the dual

Given a partial playlist with $K$ seed songs, a natural approach to recommend a subset of music collection $\SCal$ is 
ranking songs in $\SCal$ that are not seed songs, in particular, songs that are more relevant to the playlist should be
ranked higher that those that are not, which is a bipartite ranking problem.

%This can be formulated as a bipartite ranking problem where songs in the given playlist have positive labels,
%while songs that are not part of the playlist have negative labels.

% equation of bipartite ranking for playlist augmentation


One approach to \emph{focus on the most plausible songs}, for a given playlist,
is to learn a recommender system by minimising the rank of the top ranked song which is not in the playlist,
in other words, the higher it ranked over songs in playlist, the harder we penalise the learning system.
This is known as the Top Push loss in bipartite ranking~\cite{li2014top}, which is formally defined as,
\begin{equation}
\label{eq:toppush}
\LCal_\textsc{tp}(f; \DCal) 
= \frac{1}{M^+} \sum_{m: y^m = 1} \llb f(\x^m) \le \max_{n: y^n = 0} f(\x^n) \rrb,
\end{equation}
where $M = |\SCal|$ is the number of songs in $\SCal$, and $M^+$ is the number of songs in playlist.

To optimise the objective~\ref{eq:toppush}, 
we firstly have to replace the indicator function (0-1 loss) with one of its convex surrogate loss,
such as the exponential loss $\ell(f, y) = e^{-fy}$, logistic loss $\ell(f, y) = \log(1 + e^{-fy})$, 
or squared hinge loss $\ell(f, y) = \max\{0, (1 - fy)\}^2$.
Further, we have to deal with the challenge of \emph{max} operator in (\ref{eq:toppush}), which can sometimes be mitigated by solving the dual problem.
If the ranking function $f$ has a linear form, \ie $f(\x) = \w^\top \x$, it has been shown that the dual formulation of (\ref{eq:toppush}) 
with L2 regularisation is~\cite{li2014top}:
\begin{equation*}
\begin{aligned}
\min_{\alphabm, \betabm} \ & \frac{1}{2 C M^+} \| \alphabm^\top \X^+ - \betabm^\top \X^- \|^2 + \ell^*(\alphabm), \\
s.t. \ & \one^\top \alphabm = \one^\top \betabm, \\
       & \alphabm \in \R_+^{M^+}, \, \betabm \in \R_+^{M-M^+},
\end{aligned}
\end{equation*}
where $\ell^*$ is the convex conjugate of surrogate loss $\ell$.


\paragraph{Multitask regularisation}
% a user, in general, has multiple playlists, a reasonable assumption is that playlists of one user are more similar than playlists of different users,
% so we use a multitask regularisation to ensure the weights of playlists for the same users should be similar
Another observation is that a user generally has more than one playlists,
a reasonable assumption is playlists of the same user share similar characteristics. 
Suppose we learn a parameter vector for each playlist, then one approach to formalise this similarity is using an extra regulariser
for each user $u$ that has $N$ playlists,
\begin{equation}
\label{eq:mtreg}
\begin{aligned}
&\frac{1}{2} \cdot \frac{1}{N (N - 1) / 2} \sum_{i, j \in \{1,\dots,N\}, \, i < j} \| \w_i - \w_j \|^2 \\
&= \frac{1}{N (N - 1)} \left( (N - 1) \sum_{i=1}^{N} \w_i^\top \w_i - 2 \sum_{i, j \in \{1,\dots,N\}, \, i < j} \w_i^\top \w_j \right) \\
&= \frac{1}{N} \sum_{i=1}^{N} \w_i^\top \w_i - \frac{2}{N (N - 1)} \sum_{i, j \in \{1,\dots,N\}, \, i < j} \w_i^\top \w_j %\\
%&= \frac{1}{N} \sum_{i=1}^{N} \w_i^\top \w_i - \frac{1}{N (N - 1)} \sum_{i=1}^{N} \sum_{j \in \{1,\dots,N\}, j \ne i} \w_i^\top \w_j \\
%&= \left( \frac{1}{N} + \frac{1}{N (N - 1)} \right) \sum_{i=1}^{N} \w_i^\top \w_i 
%   - \frac{1}{N (N - 1)} \sum_{i=1}^{N} \sum_{j=1}^{N} \w_i^\top \w_j \\
%&= \frac{1}{N - 1} \sum_{i=1}^{N} \w_i^\top \w_i - \frac{1}{N (N - 1)} \sum_{i=1}^{N} \sum_{j=1}^{N} \w_i^\top \w_j.
\end{aligned}
\end{equation}
We call (\ref{eq:mtreg}) \emph{multitask regulariser} as we regularise the parameters of multiple bipartite ranking tasks. 

To optimise for the most plausible songs, we minimise the Top Push loss for each playlist of user $u$, 
we take into account the multitask regulariser in addition to the L2 regularisation, 
which results in an objective:
\begin{equation}
\label{eq:mtobj}
\begin{aligned}
&\frac{C_1}{2 N} \sum_{i=1}^{N} \| \w_i \|^2
+ \frac{C_2}{2 N (N - 1) / 2} \sum_{i, j \in \{1,\dots,N\}, \, i < j} \| \w_i - \w_j \|^2 
+ \frac{1}{N} \sum_{i = 1}^{N} \frac{1}{M_i^+} \sum_{m: y_i^m = 1} \llb f(\x^m) \le \max_{n: y_i^n = 0} f(\x^n) \rrb \\
&= \frac{C_1 + 2C_2}{2 N} \sum_{i=1}^{N} \w_i^\top \w_i 
- \frac{2C_2}{N (N - 1)} \sum_{i, j \in \{1,\dots,N\}, \, i < j} \w_i^\top \w_j
+ \frac{1}{N} \sum_{i = 1}^{N} \frac{1}{M_i^+} \sum_{m: y_i^m = 1} \llb f(\x^m) \le \max_{n: y_i^n = 0} f(\x^n) \rrb,
\end{aligned}
\end{equation}
where $C_1$ and $C_2$ are regularisation parameters.

If we assume a linear ranking function $f(\x) = \w^\top \x$, 
and use the exponential surrogate loss $\ell(f, y) = e^{-fy}$,
it can be shown that the dual of (\ref{eq:mtobj}) is
\begin{equation}
\label{eq:mtdual}
\begin{aligned}
\min_{\Thetabm} \ \ & \frac{1}{2} \sum_{d=1}^{D} \sum_{d'=1}^{D} \C \circ \left( \X^\top \Thetabm (\C^{-1})^2 \Thetabm^\top \X \right) 
    + \sum_{i = 1}^{N} \sum_{m: y_i^m = 1} \theta_i^m \left( 1 - \log(-\theta_i^m) - \log(N M_i^+) \right) \\
s.t. \ & \Thetabm^\top \one_M = \zero_{N} \\
       & \theta_k^n \ge 0, \, k \in \{1,\dots,N\}, \, n \in \{1,\dots,M\} \ \mathrm{and} \ y_k^n = 0.
\end{aligned}
\end{equation}
where $\circ$ denotes the element-wise multiplication,
$\Thetabm \in \R^{M \times N}$ are the dual variables, $D$ is the number of features of a song, 
$\X \in \R^{M \times D}$ is the design matrix where each row is the features of a song,
$M_i^+$ is the number of songs in the $i$-th playlist of user $u$,
and $\C \in \R^{N \times N}$ is a symmetric matrix such that
\begin{equation*}
C_{ij} = \begin{cases}
\frac{C_1 + 2C_2}{N}, & i = j \\
\frac{-2C_2}{N (N - 1)},  & \mathrm{otherwise}.
\end{cases}
\end{equation*}

Suppose $\Thetabm^*$ is the optimal solution of the dual problem (\ref{eq:mtdual}), 
then the optimal solution of the primal problem (\ref{eq:mtobj}) can be computed as
\begin{equation*}
\W^* = -\C^{-1} \Thetabm^{*\top} \X.
\end{equation*}


%\section{Experiment}
% experiments on two playlist dataset
\input{experiment}
