\subsection{Learning to recommend the most probable songs}
\label{ssec:bploss}

In this section, we describe a ranking based approach that learns to rank songs in playlist higher 
than those not in it, which hopefully ranks the most probable songs higher than those unlikely when making a recommendation.
This is known as the Bottom-Push loss~\cite{rudin2009p} in bipartite ranking literature.
Formally, given playlist $i$, we would like
\begin{equation*}
\begin{aligned}
\min_{m: y_m^i = 1} f(i, m) \ge f(i, n), \ \forall n \in \{1,\dots,M\} \ \text{and} \ y_n^i = 0,
\end{aligned}
\end{equation*}
where $y_m^i = 1$ denotes that song $m$ is in playlist $i$,
and $y_n^i = 0$ represents song $n$ does not appear in playlist $i$.

To achieve the goal, we can minimise the number of songs that not appeared in a given playlist
but have a higher score than the lowest ranked song in playlist, \ie the (normalised) empirical risk is
\begin{equation}
\label{eq:bprisk}
\RCal_\textsc{rank} = \frac{1}{N} \sum_{i=1}^N \frac{1}{M_-^i} \sum_{n: y_n^i = 0} \llb \min_{m: y_m^i = 1} f(i, m) \le f(i, n) \rrb,
\end{equation}
where $M_-^i$ denotes the number of songs that are not in playlist $i$,
and $\llb \cdot \rrb$ is the indicator function that represents the 0-1 loss.

To minimise $\RCal_\textsc{rank}$, we replace 0-1 loss with one of its convex surrogate,
\eg the exponential loss $\ell(f, y) = e^{-fy}$, the logistic loss $\ell(f, y) = \log(1 + e^{-fy})$,
or the squared hinge loss $\ell(f, y) = [\max(0, \, 1 - fy)]^2$ etc.

Suppose we use the exponential loss, the multitask learning objective~(\ref{eq:obj}) becomes
\begin{equation}
\label{eq:expobj}
\min_{\uptheta} \ R(\uptheta) + \frac{1}{N} \sum_{i=1}^N \frac{1}{M_-^i} \sum_{n: y_n^i = 0} \exp \left(f(i, n) - \min_{m: y_m^i = 1} f(i, m) \right),
\end{equation}
where $R(\uptheta) = \lambda_1 \sum_{j=1}^U \|\bu_j\|_2^2 + \lambda_2 \sum_{i=1}^N \|\w_i\|_1 + \lambda_3 \|\widebar\bu\|_1$ 
is the multitask regularisation term.



\subsection{Formulate a constrained optimisation problem}

Directly solving problem (\ref{eq:expobj}) is challenging due to the \emph{min} operator.
However, it is straightforward to show that (\ref{eq:expobj}) is equivalent to the following constrained optimisation problem:
\begin{equation}
\label{eq:expobj_cons}
\begin{aligned}
\min_{\uptheta} \ \, & R(\uptheta) + \frac{1}{N} \sum_{i=1}^N \frac{e^{-\xi_i}}{M_-^i} \sum_{n: y_n^i = 0} e^{f(i, n)} \\
s.t. \ \, & \xi_i \le f(i, m), \ \forall m \in \{1,\dots,M\} \ \text{and} \ y_m^i = 1.
\end{aligned}
\end{equation}

One may observe that the objective of problem~(\ref{eq:expobj_cons}) is convex but not differentiable due to the L1 regularisation terms in $R(\uptheta)$,
nonetheless, we can make use of its sub-gradient.
Another observation is the number of constraints in problem~(\ref{eq:expobj_cons}) is 
$$
\sum_{i=1}^N \sum_{m=1}^M \llb y_m^i = 1 \rrb,
$$
in other words, the number of constraints equals the total number of songs played in all playlists.
Asymptotically, it is of order $O(\widebar{L} N)$ where $\widebar{L}$ is the average number of songs in playlists.
Although $\widebar{L}$ is dataset dependent, but typically it is less than $100$, 
while the total number of playlists $N$ can be very large in production systems (\eg Spotify hosts more than $2$ billion playlists~\cite{recsysch2018}),
which imposes further challenge to optimise problem~(\ref{eq:expobj_cons}).



\subsection{From ranking to classification}

It has been known that binary classification and bipartite ranking are
closely related~\cite{ertekin2011equivalence,menon2016bipartite}.
In particular, \citet{ertekin2011equivalence} have shown that the P-Norm Push bipartite ranking loss~\cite{rudin2009p}
is equivalent with the P-Classification loss~\cite{ertekin2011equivalence} when using the exponential surrogate.
Further, the P-Norm Push loss is an approximation of the Infinite-Push loss~\cite{agarwal2011infinite},
or equivalently, the Top-Push loss~\cite{li2014top}, which focuses on the highest ranked negative example instead of
of the lowest ranked positive example as in Bottom-Push loss described in Section~\ref{ssec:bploss}.

Inspired by these connections, we seek a classification loss that is equivalent to a bipartite ranking loss (under a few assumptions), 
which further approximates the Bottom-Push loss we use in~(\ref{eq:bprisk}).
This will make it possible to formulate an unconstrained optimisation problem that approximates the empirical loss $\RCal_\textsc{rank}$.

Here, we show how the Bottom-Push loss can be approximated,
and propose a classification loss that is equivalent to the approximation (under a few assumptions).

First, note that the \emph{max} operator can be approximated by the smooth log-sum-exp function~\cite[p. 72]{boyd2004convex},
$$
\max_i z_i \approx \frac{1}{p} \log \sum_i e^{p z_i},
$$
where $p > 0$ is a parameter that trades off the approximating precision, then
$$
\min_i z_i = -\max_i (-z_i) \approx -\frac{1}{p} \log \sum_i e^{-p z_i}.
$$

The empirical risk $\RCal_\textsc{rank}$ can be approximated (with the exponential surrogate) as follows:
\begin{equation*}
\begin{aligned}
\RCal_\textsc{rank} 
\approx \widetilde\RCal_\textsc{rank}
= \frac{1}{N} \sum_{i=1}^N \frac{1}{M_-^i} \left( \sum_{m: y_m^i = 1} e^{-p f(i, m)} \right)^\frac{1}{p} \sum_{n: y_n^i = 0} e^{f(i, n)}.
\end{aligned}
\end{equation*}

Let $\RCal_\textsc{clf}$ be the following classification risk:
\begin{equation*}
\RCal_\textsc{clf} 
= \frac{1}{N} \sum_{i=1}^N \left( 
  \frac{1}{p M_+^i} \sum_{m: y_m^i = 1} e^{-p f(i, m)} 
  + \frac{1}{M_-^i} \sum_{n: y_n^i = 0} e^{f(i, n)} \right),
\end{equation*}
we have a theorem
\begin{theorem}
\label{th:rank2clf}
Let $\uptheta^* \in \argmin_{\uptheta} \RCal_\textsc{clf}$ (assuming minimisers exist), 
then $\uptheta^* \in \argmin_{\uptheta} \widetilde\RCal_\textsc{rank}$.
\end{theorem}

\begin{proof}
We following the proof technique in~\cite{ertekin2011equivalence}
by first introducing a constant feature $1$ for each song,
without loss of generality, let $x_m^0$ be the constant feature, \ie $x_m^0 = 1$.
Then we can show that 
$\frac{\partial \, \RCal_\textsc{clf}} {\partial \, \uptheta} = 0$ implies
$\frac{\partial \, \widetilde\RCal_\textsc{rank}} {\partial \, \uptheta} = 0$,
which means minimisers of $\RCal_\textsc{clf}$ also minimise $\widetilde\RCal_\textsc{rank}$.

The complete proof is described in Appendix.
\end{proof}

By Theorem~\ref{th:rank2clf}, we have the following problem
which approximate the objective of problem~(\ref{eq:expobj}),
\begin{equation}
\label{eq:expobj_clf}
\min_{\uptheta} \, R(\uptheta) + \RCal_\textsc{clf}.
\end{equation}
This unconstrained optimisation problem can be optimised more efficiently.



\subsection{Discussion}

In this section, we want to discuss the advantages and limitations of the 
two approaches that solve problem~(\ref{eq:expobj}), 
as well as a few practical strategies for the proposed approaches.

The constrained optimisation problem~(\ref{eq:expobj_cons}) can be solved using interior-point method,
although the large number of constraints requires a large amount of computation resources.
Cutting-plane~\cite{avriel2003nonlinear} techniques can be employed to deal with this challenge, 
but we found it does not work extremely well in practice.
However, this approach allows us to use different type surrogate loss beside the exponential loss,
such as the hinge loss or squared hinge loss, which may provide a few other desired properties. % detailing this
% solvers: IPOPT, CVX etc.

It is obvious that solving the unconstrained optimisation problem~(\ref{eq:expobj_clf})
is generally easier than solving problem (\ref{eq:expobj_cons}) which has a large number of constraints.
The Orthant-Wise Limited-memory Quasi-Newton (OWL-QN)~\cite{andrew2007scalable} L-BFGS variant can be employed to
efficiently deal with the L1 term in (\ref{eq:expobj_clf}) without huge memory burdens.
% solver: pylbfgs
