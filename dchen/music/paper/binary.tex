\section{P-Classification loss for binary classification and P-Norm Push loss for bipartite ranking}
\label{sec:binary}

Given a dataset with binary labels $\SCal = \SCal_+ \cup \SCal_-$, where $\SCal_+$ is a set of positive examples, 
\ie $\SCal_+ = \{(\x_+, +1)\}$, and $\SCal_-$ is a set of negative examples, \ie $\SCal_- = \{(\x_-, -1)\}$.

The P-Classification loss~\cite{ertekin2011equivalence}, for the exponential surrogate, is
\begin{equation*}
\LCal_\textsc{pc}(f; \SCal) 
= \sum_{\x_+ \in \SCal_+} e^{-f(\x_+)} + \frac{1}{p} \sum_{\x_- \in \SCal_-} e^{p f(\x_-)},
\end{equation*}
where $p > 0$ is a parameter.

The P-Norm Push loss~\cite{rudin2009p}, for the exponential surrogate, is
\begin{equation*}
\LCal_\textsc{pn}^\ell (f; \SCal)
= \sum_{\x_- \in \SCal_-} \left( \sum_{\x_+ \in \SCal_+} \ell \left( f(\x_+) - f(\x_-) \right) \right)^p.
\end{equation*}

The Top Push loss~\cite{li2014top} is\footnote{We omit the normalisation factor, and $\llb \cdot \rrb$ denotes the indicator function.}
\begin{equation*}
\LCal_\textsc{tp}(f; \SCal)
= \sum_{\x_+ \in \SCal_+} \llb f(\x_+) \le \max_{\x_- \in \SCal_-} f(\x_-) \rrb.
\end{equation*}

The Infinite Push loss~\cite{rudin2009p} is
\begin{equation*}
\LCal_\infty(f; \SCal)
= \max_{\x_- \in \SCal_-} \sum_{\x_+ \in \SCal_+} \llb f(\x_+) \le f(\x_-) \rrb.
\end{equation*}

It is known that:
\begin{itemize}
\item The Top Push loss is equivalent to the Infinite Push loss~\cite{li2014top}.
\item The Infinite Push loss is equivalent to the P-Norm Push loss, with the same surrogate $\ell$,
      when $p = \infty$~\cite{agarwal2011infinite,rakotomamonjy2012sparse}.
\end{itemize}
This is due to the fact that $p$-norm approximation of the $\max$ function, and when $p \to \infty$,
the approximation becomes exact:
\begin{equation*}
\begin{aligned}
\lim_{p \to \infty} \left( \LCal_\textsc{pn}^\ell (f; \SCal) \right)^\frac{1}{p}
&= \lim_{p \to \infty} \left( \sum_{\x_- \in \SCal_-} \left( \sum_{\x_+ \in \SCal_+} 
   \ell \left( f(\x_+) - f(\x_-) \right) \right)^p \right)^\frac{1}{p} \\
&= \max_{\x_- \in \SCal_-} \left( \sum_{\x_+ \in \SCal_+} \ell \left( f(\x_+) - f(\x_-) \right) \right) \\
&= \LCal_\infty^\ell (f; \SCal) \\
&= \sum_{\x_+ \in \SCal_+} \ell \left( f(\x_+) - \max_{\x_- \in \SCal_-} f(\x_-) \right) \\
&= \LCal_\textsc{tp}^\ell (f; \SCal).
\end{aligned}
\end{equation*}

Symmetrically, for the Bottom Push loss~\cite{rudin2009p}:
\begin{equation*}
\LCal_\textsc{bp}(f; \SCal)
= \frac{1}{|\SCal_-|} \sum_{\x_- \in \SCal_-} \llb f(\x_-) \ge \min_{\x_+ \in \SCal_+} f(\x_+) \rrb,
\end{equation*}
we have\footnote{Note that $\min_{i} z_i = -\max_i (-z_i)$.}
\begin{equation*}
\begin{aligned}
\lim_{p \to \infty} \left( \sum_{\x_+ \in \SCal_+} \left( \sum_{\x_- \in \SCal_-} 
   \ell \left( f(\x_+) - f(\x_-) \right) \right)^p \right)^\frac{1}{p}
&= \max_{\x_+ \in \SCal_+} \left( \sum_{\x_- \in \SCal_-} \ell \left( f(\x_+) - f(\x_-) \right) \right) \\
&= \max_{\x_+ \in \SCal_+} \left( \sum_{\x_- \in \SCal_-} \ell \left(-f(\x_-) - (-f(\x_+)) \right) \right) \\
&= \sum_{\x_- \in \SCal_-} \ell \left( -f(\x_-) - \max_{\x_+ \in \SCal_+} (-f(\x_+)) \right) \\
&= \sum_{\x_- \in \SCal_-} \ell \left( -f(\x_-) + \min_{\x_+ \in \SCal_+} f(\x_+) \right) \\
&= \sum_{\x_- \in \SCal_-} \ell \left(\min_{\x_+ \in \SCal_+} f(\x_+) - f(\x_-) \right) \\
&= \LCal_\textsc{bp}^\ell (f; \SCal).
\end{aligned}
\end{equation*}

Further, (\cite{ertekin2011equivalence}) showed that the P-Classification loss and the P-Norm Push loss share the same minimisers
(if minimisers exist) when $\ell$ is the exponential surrogate.
It turns out that this equivalent relationship is a special case of a general equivalence relationship 
between binary classification and bipartite ranking loss, which we will describe in detail in the next section.



\subsection{Binary classification loss vs. bipartite ranking loss}

Let function $f(\cdot, \cdot)$ be
$$
f(\x) := g(\x; \w) + b,
$$
where $\x$ is an input, $\w$ is a weight vector, $b$ is a bias parameter, 
and function $g(\x; \w)$ is differentiable (w.r.t. $\w$) and bounded.

Suppose $\alpha, \beta, m, C \in \R_+$ are \emph{finite} positive numbers, we define 
$\RCal_\textsc{bc}$ be the following classification loss:
\begin{equation*}
%\label{eq:bc}
%\resizebox{\linewidth}{!}{$
%\begin{aligned}
\LCal_\textsc{bc}(f; \SCal)
= \displaystyle 
  \frac{1}{\alpha} \sum_{\x_+ \in \SCal_+} e^{-\alpha f(\x_+)} +
  \frac{C}{\beta}  \sum_{\x_- \in \SCal_-} e^{ \beta  f(\x_-)},
%\end{aligned}
%$}
\end{equation*}
and let
$\LCal_\textsc{br}$ be a bipartite ranking loss defined as:
\begin{equation*}
%\label{eq:br}
%\resizebox{\linewidth}{!}{$
\begin{aligned}
\LCal_\textsc{br}(f; \SCal)
&= \left[ \displaystyle 
   \sum_{\x_- \in \SCal_-} \left( \sum_{\x_+ \in \SCal_+} e^{-\alpha (f(\x_+) - f(\x_-))} \right)^\frac{\beta}{\alpha}
   \right]^\frac{m}{\beta} \\
&= \left[ \sum_{\x_+ \in \SCal_+} e^{-\alpha f(\x_+)} \right]^\frac{m}{\alpha}
   \left[ \sum_{\x_- \in \SCal_-} e^{ \beta  f(\x_-)} \right]^\frac{m}{\beta}.
\end{aligned}
%$}
\end{equation*}


\begin{theorem}
\label{th:bc=br}
$\LCal_\textsc{bc}$ and $\LCal_\textsc{br}$ share the same set of minimisers $\WCal_\textsc{b}$
(assuming minimisers exist), in particular, for $\w_\textsc{b} \in \WCal_\textsc{b}$,
\begin{equation*}
\begin{aligned}
                \w_\textsc{b} & \in \argmin_\w     \, \LCal_\textsc{br}(f; \SCal), \\
(\w_\textsc{b}, b_\textsc{b}) & \in \argmin_{\w,b} \, \LCal_\textsc{bc}(f; \SCal),
\end{aligned}
\end{equation*}
where
$$
%\resizebox{\linewidth}{!}{$
\displaystyle
b_\textsc{b} 
= \frac{1}{\alpha + \beta} \left( 
  \ln \sum_{\x_+ \in \SCal_+} e^{-\alpha g(\x_+; \w_\textsc{b})} -
  \ln \sum_{\x_- \in \SCal_-} e^{  \beta g(\x_-; \w_\textsc{b})} - \ln(C) \right).
%$}
$$
\end{theorem}

This theorem can be proved using the same technique as that for Theorem 1 and 2 in~\cite{ertekin2011equivalence},
see Appendix for the full proof.

We can have a few interesting special cases of Theorem~\ref{th:bc=br},
including Theorem 1, 2 and 3 in~\cite{ertekin2011equivalence} by 
assigning appropriate values for parameters $\alpha$, $\beta$, $m$ and $C$ in Theorem~\ref{th:bc=br}.
We describe them in the next section.




\subsection{Special cases}

\TODO

{\it Cost-sensitive AdaBoost == RankBoost, Theorem 3 of~\cite{ertekin2011equivalence}.
IR Push == P-Classification,
IR Push = Top Push + exponential surrogate + log-sum-exp, P-Norm Push vs. Top Push vs. IR Push (approximation inside/outside)
Bottom Push + exponential surrogate + log-sum-exp == a classification loss similar to P-Classification.
}

\begin{table}[!h]
\centering
\caption{Summary of parameters for the equivalence of (risk of rank loss, risk of classification loss) pairs}
\label{tab:config}
%\resizebox{\linewidth}{!}{
\begin{tabular}{r*{6}{c}}
\toprule
{\bf Loss pairs}                                         & $\alpha$ & $\beta$ & $\gamma$ & $\delta$ & $P$ & $Q$ \\ \hline
$\RCal_\textsc{pn} \equiva \RCal_\textsc{pc}$            & $1$      & $p$     & $p$      & $1$      & $\frac{1}{p}$ & $\frac{1}{p}$ \\
$\widetilde\RCal_\textsc{tp} \equiva \RCal_\textsc{pc}$  & $1$      & $r=p$   & $1$      & $\frac{1}{r}=\frac{1}{p}$ & $1$ & $1$ \\
$\widetilde\RCal_\textsc{bp} \equiva \RCal_\textsc{rc}$  & $r=q$    & $1$     & $\frac{1}{r}=\frac{1}{q}$ & $1$ & $1$ & $1$ \\
$\widetilde\RCal_\textsc{csa} \equiva \RCal_\textsc{rb}$ & $1$      & $1$     & $1$      & $1$      & $1$ & $C$ \\
\bottomrule
\end{tabular}
%}
\end{table}
