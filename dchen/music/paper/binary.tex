\subsection{P-Classification loss for binary classification and P-Norm Push loss for bipartite ranking}
\label{sec:binary}

Given a dataset with binary labels $\SCal = \SCal_+ \cup \SCal_-$, where $\SCal_+$ is a set of positive examples, 
\ie $\SCal_+ = \{(\x_+, +1)\}$, and $\SCal_-$ is a set of negative examples, \ie $\SCal_- = \{(\x_-, -1)\}$.

The P-Classification loss~\cite{ertekin2011equivalence}, for the exponential surrogate, is
\begin{equation*}
\LCal_\textsc{pc}(f; \SCal) 
= \sum_{\x_+ \in \SCal_+} e^{-f(\x_+)} + \frac{1}{p} \sum_{\x_- \in \SCal_-} e^{p f(\x_-)},
\end{equation*}
where $p > 0$ is a parameter.

The P-Norm Push loss~\cite{rudin2009p}, for the exponential surrogate, is
\begin{equation*}
\LCal_\textsc{pn}^\ell (f; \SCal)
= \sum_{\x_- \in \SCal_-} \left( \sum_{\x_+ \in \SCal_+} \ell \left( f(\x_+) - f(\x_-) \right) \right)^p.
\end{equation*}

The Top Push loss~\cite{li2014top} is\footnote{We omit the normalisation factor, and $\llb \cdot \rrb$ denotes the indicator function.}
\begin{equation*}
\LCal_\textsc{tp}(f; \SCal)
= \sum_{\x_+ \in \SCal_+} \llb f(\x_+) \le \max_{\x_- \in \SCal_-} f(\x_-) \rrb.
\end{equation*}

The Infinite Push loss~\cite{rudin2009p} is
\begin{equation*}
\LCal_\infty(f; \SCal)
= \max_{\x_- \in \SCal_-} \sum_{\x_+ \in \SCal_+} \llb f(\x_+) \le f(\x_-) \rrb.
\end{equation*}

It is known that:
\begin{itemize}
\item The Top Push loss is equivalent to the Infinite Push loss~\cite{li2014top}.
\item The Infinite Push loss is equivalent to the P-Norm Push loss, with the same surrogate $\ell$,
      when $p = \infty$~\cite{agarwal2011infinite,rakotomamonjy2012sparse}.
\end{itemize}
This is due to the fact that $p$-norm approximation of the $\max$ function, and when $p \to \infty$,
the approximation becomes exact:
\begin{equation}
\label{eq:pn2tp}
\begin{aligned}
&  \lim_{p \to \infty} \left( \LCal_\textsc{pn}^\ell (f; \SCal) \right)^\frac{1}{p} \\
&= \lim_{p \to \infty} \left( \sum_{\x_- \in \SCal_-} \left( \sum_{\x_+ \in \SCal_+} 
   \ell \left( f(\x_+) - f(\x_-) \right) \right)^p \right)^\frac{1}{p} \\
&= \max_{\x_- \in \SCal_-} \left( \sum_{\x_+ \in \SCal_+} \ell \left( f(\x_+) - f(\x_-) \right) \right) \\
&= \LCal_\infty^\ell (f; \SCal) \\
&= \sum_{\x_+ \in \SCal_+} \ell \left( f(\x_+) - \max_{\x_- \in \SCal_-} f(\x_-) \right) \\
&= \LCal_\textsc{tp}^\ell (f; \SCal).
\end{aligned}
\end{equation}

Symmetrically, for the Bottom Push loss~\cite{?} is equivalent to a variant of P-Norm Push that concentrating on the bottom~\cite{rudin2009p}:
\begin{equation*}
\LCal_\textsc{bp}(f; \SCal)
= \sum_{\x_- \in \SCal_-} \llb f(\x_-) \ge \min_{\x_+ \in \SCal_+} f(\x_+) \rrb,
\end{equation*}
we have\footnote{Note that $\min_{i} z_i = -\max_i (-z_i)$.}
\begin{equation}
\label{eq:pnp2bp}
\begin{aligned}
&  \lim_{p \to \infty} \left( \sum_{\x_+ \in \SCal_+} \left( \sum_{\x_- \in \SCal_-} 
   \ell \left( f(\x_+) - f(\x_-) \right) \right)^p \right)^\frac{1}{p} \\
&= \max_{\x_+ \in \SCal_+} \left( \sum_{\x_- \in \SCal_-} \ell \left( f(\x_+) - f(\x_-) \right) \right) \\
&= \max_{\x_+ \in \SCal_+} \left( \sum_{\x_- \in \SCal_-} \ell \left(-f(\x_-) - (-f(\x_+)) \right) \right) \\
&= \sum_{\x_- \in \SCal_-} \ell \left( -f(\x_-) - \max_{\x_+ \in \SCal_+} (-f(\x_+)) \right) \\
&= \sum_{\x_- \in \SCal_-} \ell \left( -f(\x_-) + \min_{\x_+ \in \SCal_+} f(\x_+) \right) \\
&= \sum_{\x_- \in \SCal_-} \ell \left(\min_{\x_+ \in \SCal_+} f(\x_+) - f(\x_-) \right) \\
&= \LCal_\textsc{bp}^\ell (f; \SCal).
\end{aligned}
\end{equation}

Further, (\cite{ertekin2011equivalence}) showed that the P-Classification loss and the P-Norm Push loss share the same minimisers
(if minimisers exist) when $\ell$ is the exponential surrogate.
It turns out that this equivalent relationship is a special case of a general equivalence relationship 
between binary classification and bipartite ranking loss, which we will describe in detail in the next section.



\subsection{Binary classification loss vs. bipartite ranking loss}

Let function $f(\cdot, \cdot)$ be
$$
f(\x) := g(\x; \w) + b,
$$
where $\x$ is an input, $\w$ is a weight vector, $b$ is a bias parameter, 
and function $g(\x; \w)$ is differentiable (w.r.t. $\w$) and bounded.

Suppose $\alpha, \beta, C \in \R_+$ are \emph{finite} positive numbers, we define 
$\RCal_\textsc{bc}$ be the following classification loss:
\begin{equation*}
%\label{eq:bc}
%\resizebox{\linewidth}{!}{$
%\begin{aligned}
\LCal_\textsc{bc}(f; \SCal)
= \displaystyle 
    \sum_{\x_+ \in \SCal_+} e^{-\alpha f(\x_+)} +
  C \sum_{\x_- \in \SCal_-} e^{ \beta  f(\x_-)},
%\end{aligned}
%$}
\end{equation*}
and let
$\LCal_\textsc{br}$ be a bipartite ranking loss defined as:
\begin{equation*}
%\label{eq:br}
%\resizebox{\linewidth}{!}{$
\begin{aligned}
\LCal_\textsc{br}(f; \SCal)
&= \displaystyle 
   \sum_{\x_- \in \SCal_-} \left( \sum_{\x_+ \in \SCal_+} e^{-\alpha (f(\x_+) - f(\x_-))} \right)^\frac{\beta}{\alpha} \\
&= \left( \sum_{\x_+ \in \SCal_+} e^{-\alpha f(\x_+)} \right)^\frac{\beta}{\alpha}
   \sum_{\x_- \in \SCal_-} e^{ \beta f(\x_-)}
\end{aligned}
%$}
\end{equation*}


\begin{theorem}
\label{th:bc=br}
$\LCal_\textsc{bc}$ and $\LCal_\textsc{br}$ share the same set of minimisers $\WCal_\textsc{b}$
(assuming minimisers exist), in particular, for $\w_\textsc{b} \in \WCal_\textsc{b}$,
\begin{equation*}
\begin{aligned}
                \w_\textsc{b} & \in \argmin_\w     \, \LCal_\textsc{br}(f; \SCal), \\
(\w_\textsc{b}, b_\textsc{b}) & \in \argmin_{\w,b} \, \LCal_\textsc{bc}(f; \SCal),
\end{aligned}
\end{equation*}
where
$$
%\resizebox{\linewidth}{!}{$
\displaystyle
b_\textsc{b} 
= \frac{1}{\alpha + \beta} \left( 
  \ln \sum_{\x_+ \in \SCal_+} e^{-\alpha g(\x_+; \w_\textsc{b})} -
  \ln \sum_{\x_- \in \SCal_-} e^{  \beta g(\x_-; \w_\textsc{b})} - \ln(C) \right).
%$}
$$
\end{theorem}

This theorem can be proved using the same technique as that for Theorem 1 and 2 in~\cite{ertekin2011equivalence},
see Appendix for the full proof.

We can have a few interesting special cases of Theorem~\ref{th:bc=br},
including Theorem 1, 2 and 3 in~\cite{ertekin2011equivalence} by 
assigning appropriate values for parameters $\alpha$, $\beta$ and $C$ in Theorem~\ref{th:bc=br}.
We describe them in the next section.




\subsection{Special cases}

It is straightforward to show the fact that P-Classification and P-Norm Push share the same set of minimisers 
(Theorem 1 and 2 in~\cite{ertekin2011equivalence}) is an instance of Theorem~\ref{th:bc=br}, by letting $\alpha = C = 1$, and $\beta = p$.
Similarly, the fact that Cost-Sensitive AdaBoost and RankBoost share the same set of minimisers (Theorem 3 in~\cite{ertekin2011equivalence}) 
can be obtained by letting $\alpha = \beta = 1$.

Further, we know that P-Norm Push loss is an approximation of Infinite Push loss, and by Equation~\ref{eq:pn2tp}, 
it is natural to conjecture that P-Norm Push is an approximation of Top Push.
It turns out this is indeed the case.

By using the exponential surrogate and approximate the $\max$ function with the log-sum-exp function, \ie
\begin{equation*}
\begin{aligned}
\widetilde\LCal_\textsc{tp}^\ell(f; \SCal)
&= \sum_{\x_+ \in \SCal_+} \exp \left( - \left( f(\x_+) - \frac{1}{p} \log \sum_{\x_- \in \SCal_-} e^{p f(\x_-)} \right) \right) \\
&= \sum_{\x_+ \in \SCal_+} e^{-f(\x_+)} \left( \sum_{\x_- \in \SCal_-} e^{p f(\x_-)} \right)^\frac{1}{p} \\
&= \left( \left( \sum_{\x_+ \in \SCal_+} e^{-f(\x_+)} \right)^p \cdot \sum_{\x_- \in \SCal_-} e^{p f(\x_-)} \right)^\frac{1}{p} \\
&= \left( \LCal_\textsc{pn}^\ell (f; \SCal) \right)^\frac{1}{p},
\end{aligned}
\end{equation*}
where $\ell$ is the exponential surrogate.
Further, by Theorem~\ref{th:bc=br}, and let $\alpha = C = 1$, and $\beta = p$,
we know that both $\widetilde\LCal_\textsc{tp}^\ell(f; \SCal)$ and P-Norm Push share the same set of minimisers with
P-Classification loss, for the exponential surrogate.

We can achieve similar results for the Bottom Push loss, 
which can be shown to share the same set of minimisers with another classification loss that resembles the P-Classification loss.
\begin{equation*}
\begin{aligned}
\widetilde\LCal_\textsc{bp}^\ell(f; \SCal)
&= \sum_{\x_- \in \SCal_-} \exp \left( \frac{1}{p} \log \sum_{\x_+ \in \SCal_+} e^{-p f(\x_+)} + f(\x_-) \right) \\
&= \sum_{\x_- \in \SCal_-} e^{f(\x_-)} \left( \sum_{\x_+ \in \SCal_+} e^{-p f(\x_+)} \right)^\frac{1}{p},
\end{aligned}
\end{equation*}
where $\ell$ is the exponential surrogate.
Further, by Theorem~\ref{th:bc=br}, and let $\alpha = p$, and $\beta = C = 1$,
we know that both $\widetilde\LCal_\textsc{bp}^\ell(f; \SCal)$ and 
$\frac{1}{p} \sum_{\x_+ \in \SCal_+} e^{-p f(\x_+)} + \sum_{\x_- \in \SCal_-} e^{f(\x_-)}$
share the same set of minimisers for the exponential surrogate.
