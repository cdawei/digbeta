\section{P-Classification loss for binary classification and P-Norm Push loss for bipartite ranking}
\label{sec:binary}

Given a dataset with binary labels $\SCal = \SCal_+ \cup \SCal_-$, where $\SCal_+$ is a set of positive examples, 
\ie $\SCal_+ = \{(\x_+, +1)\}$, and $\SCal_-$ is a set of negative examples, \ie $\SCal_- = \{(\x_-, -1)\}$.

The P-Classification loss~\cite{ertekin2011equivalence}, for the exponential surrogate, is
\begin{equation*}
\LCal_\textsc{pc}(f; \SCal) 
= \sum_{\x_+ \in \SCal_+} e^{-f(\x_+)} + \frac{1}{p} \sum_{\x_- \in \SCal_-} e^{p f(\x_-)},
\end{equation*}
where $p > 0$ is a parameter.

The P-Norm Push loss~\cite{rudin2009p}, for the exponential surrogate, is
\begin{equation*}
\LCal_\textsc{pn}^\ell (f; \SCal)
= \sum_{\x_- \in \SCal_-} \left( \sum_{\x_+ \in \SCal_+} \ell \left( f(\x_+) - f(\x_-) \right) \right)^p.
\end{equation*}

The Top Push loss~\cite{li2014top} is\footnote{We omit the normalisation factor, and $\llb \cdot \rrb$ denotes the indicator function.}
\begin{equation*}
\LCal_\textsc{tp}(f; \SCal)
= \sum_{\x_+ \in \SCal_+} \llb f(\x_+) \le \max_{\x_- \in \SCal_-} f(\x_-) \rrb.
\end{equation*}

The Infinite Push loss~\cite{rudin2009p} is
\begin{equation*}
\LCal_\infty(f; \SCal)
= \max_{\x_- \in \SCal_-} \sum_{\x_+ \in \SCal_+} \llb f(\x_+) \le f(\x_-) \rrb.
\end{equation*}

It is known that:
\begin{itemize}
\item The Top Push loss is equivalent to the Infinite Push loss~\cite{li2014top}.
\item The Infinite Push loss is equivalent to the P-Norm Push loss, with the same surrogate $\ell$,
      when $p = \infty$~\cite{agarwal2011infinite,rakotomamonjy2012sparse}.
\end{itemize}
This is due to the fact that $p$-norm approximation of the $\max$ function, and when $p \to \infty$,
the approximation becomes exact:
\begin{equation*}
\begin{aligned}
\lim_{p \to \infty} \left( \LCal_\textsc{pn}^\ell (f; \SCal) \right)^\frac{1}{p}
&= \lim_{p \to \infty} \left( \sum_{\x_- \in \SCal_-} \left( \sum_{\x_+ \in \SCal_+} 
   \ell \left( f(\x_+) - f(\x_-) \right) \right)^p \right)^\frac{1}{p} \\
&= \max_{\x_- \in \SCal_-} \left( \sum_{\x_+ \in \SCal_+} \ell \left( f(\x_+) - f(\x_-) \right) \right) \\
&= \LCal_\infty^\ell (f; \SCal) \\
&= \sum_{\x_+ \in \SCal_+} \ell \left( f(\x_+) - \max_{\x_- \in \SCal_-} f(\x_-) \right) \\
&= \LCal_\textsc{tp}^\ell (f; \SCal).
\end{aligned}
\end{equation*}

Symmetrically, for the Bottom Push loss~\cite{rudin2009p}:
\begin{equation*}
\LCal_\textsc{bp}(f; \SCal)
= \frac{1}{|\SCal_-|} \sum_{\x_- \in \SCal_-} \llb f(\x_-) \ge \min_{\x_+ \in \SCal_+} f(\x_+) \rrb,
\end{equation*}
we have\footnote{Note that $\min_{i} z_i = -\max_i (-z_i)$.}
\begin{equation*}
\begin{aligned}
\lim_{p \to \infty} \left( \sum_{\x_+ \in \SCal_+} \left( \sum_{\x_- \in \SCal_-} 
   \ell \left( f(\x_+) - f(\x_-) \right) \right)^p \right)^\frac{1}{p}
&= \max_{\x_+ \in \SCal_+} \left( \sum_{\x_- \in \SCal_-} \ell \left( f(\x_+) - f(\x_-) \right) \right) \\
&= \max_{\x_+ \in \SCal_+} \left( \sum_{\x_- \in \SCal_-} \ell \left(-f(\x_-) - (-f(\x_+)) \right) \right) \\
&= \sum_{\x_- \in \SCal_-} \ell \left( -f(\x_-) - \max_{\x_+ \in \SCal_+} (-f(\x_+)) \right) \\
&= \sum_{\x_- \in \SCal_-} \ell \left( -f(\x_-) + \min_{\x_+ \in \SCal_+} f(\x_+) \right) \\
&= \sum_{\x_- \in \SCal_-} \ell \left(\min_{\x_+ \in \SCal_+} f(\x_+) - f(\x_-) \right) \\
&= \LCal_\textsc{bp}^\ell (f; \SCal).
\end{aligned}
\end{equation*}

Further, (\cite{ertekin2011equivalence}) showed that the P-Classification loss and the P-Norm Push loss share the same minimisers
(if minimisers exist) when $\ell$ is the exponential surrogate.
It turns out that this equivalent relationship is a special case of a general equivalence relationship 
between binary classification and bipartite ranking loss, which we will describe in detail in the next section.



\subsection{Binary classification loss vs. bipartite ranking loss}

Let function $f(\cdot, \cdot)$ be
$$
f(x; \w) := g(x; \w) + b,
$$
where $\x$ is an input, $\w$ is a weight vector, $b$ is a bias parameter, 
and function $g(x; \w)$ is differentiable (w.r.t. $\w$) and bounded.

Suppose $\alpha, \beta, c, P, Q \in \R_+$ are \emph{finite} positive numbers, we define 
$\RCal_\textsc{bc}$ be the following classification risk\footnote{
We note that there is a equivalent definition:
$\RCal_\textsc{bc}(\w) = \frac{1}{\alpha} \sum_{x_+ \in \SCal_+} \exp(-\alpha f(x_+; \w)) + 
\frac{C}{\beta} \sum_{x_- \in \SCal_-} \exp( \beta f(x_-; \w))$
where $C = Q/P$, since multiplying a positive constant to a loss function will not change its minimiser.}
:
\begin{equation*}
%\label{eq:bc}
%\resizebox{\linewidth}{!}{$
%\begin{aligned}
\RCal_\textsc{bc}(\w, b)
= \displaystyle 
  \frac{P}{\alpha} \sum_{x_+ \in \SCal_+} e^{-\alpha f(x_+; \w)} +
  \frac{Q}{\beta}  \sum_{x_- \in \SCal_-} e^{ \beta  f(x_-; \w)},
%\end{aligned}
%$}
\end{equation*}
and let
$\RCal_\textsc{br}$ be a bipartite ranking risk defined as:
\begin{equation*}
%\label{eq:br}
%\resizebox{\linewidth}{!}{$
\begin{aligned}
\RCal_\textsc{br}(\w)
&= \left[ \displaystyle 
   \sum_{x_+ \in \SCal_+} \left( \sum_{x_- \in \SCal_-} e^{-\beta (f(x_+; \w) - f(x_-; \w))} \right)^\frac{\alpha}{\beta} 
   \right]^c \\
&= \left[ \sum_{x_+ \in \SCal_+} e^{-\alpha f(x_+; \w)} \right]^\frac{c}{\alpha}
   \left[ \sum_{x_- \in \SCal_-} e^{ \beta  f(x_-; \w)} \right]^\frac{c}{\beta}.
\end{aligned}
%$}
\end{equation*}
Note that $\RCal_\textsc{br}$ is independent of $b$.


\begin{theorem}
\label{th:bc=br}
If $(\w_\textsc{bc}, b_\textsc{bc}) \in \argmin_{\w,b} \RCal_\textsc{bc}(\w, b)$,
then $\w_\textsc{bc} \in \argmin_\w \RCal_\textsc{br}(\w)$, and
$$
%\resizebox{\linewidth}{!}{$
\displaystyle
b_\textsc{bc} 
= \frac{1}{\alpha + \beta} \left( 
  P \ln \sum_{x_+ \in \SCal_+} e^{-\alpha g(x_+; \w_\textsc{bc})} -
  Q \ln \sum_{x_- \in \SCal_-} e^{  \beta g(x_-; \w_\textsc{bc})} \right).
%$}
$$
Further, if $\w_\textsc{br} \in \argmin_\w \RCal_\textsc{br}(\w)$ (assuming minimisers exist),
then $(\w_\textsc{br}, b_\textsc{br}) \in \argmin_{\w,b} \, \RCal_\textsc{bc}(\w, b)$, where
$$
%\resizebox{\linewidth}{!}{$
\displaystyle
b_\textsc{br} 
= \frac{1}{\alpha + \beta} \left( 
  P \ln \sum_{x_+ \in \SCal_+} e^{-\alpha g(x_+; \w_\textsc{br})} -
  Q \ln \sum_{x_- \in \SCal_-} e^{  \beta g(x_-; \w_\textsc{br})} \right).
%$}
$$
\end{theorem}

We can get Theorem~\ref{th:pc=pn} from Theorem~\ref{th:bc=br} by letting $\alpha=c=P=Q=1$ and $\beta=p$.
Moreover, Theorem~\ref{th:bc=br} summaries a few more well-known special cases.

\TODO

{\it Cost-sensitive AdaBoost == RankBoost, Theorem 3 of~\cite{ertekin2011equivalence}.
IR Push == P-Classification,
IR Push = Top Push + exponential surrogate + log-sum-exp, P-Norm Push vs. Top Push vs. IR Push (approximation inside/outside)
Bottom Push + exponential surrogate + log-sum-exp == a classification loss similar to P-Classification.
}

\begin{table}[!h]
\centering
\caption{Summary of parameters for the equivalence of (risk of rank loss, risk of classification loss) pairs}
\label{tab:config}
%\resizebox{\linewidth}{!}{
\begin{tabular}{r*{6}{c}}
\toprule
{\bf Loss pairs}                                         & $\alpha$ & $\beta$ & $\gamma$ & $\delta$ & $P$ & $Q$ \\ \hline
$\RCal_\textsc{pn} \equiva \RCal_\textsc{pc}$            & $1$      & $p$     & $p$      & $1$      & $\frac{1}{p}$ & $\frac{1}{p}$ \\
$\widetilde\RCal_\textsc{tp} \equiva \RCal_\textsc{pc}$  & $1$      & $r=p$   & $1$      & $\frac{1}{r}=\frac{1}{p}$ & $1$ & $1$ \\
$\widetilde\RCal_\textsc{bp} \equiva \RCal_\textsc{rc}$  & $r=q$    & $1$     & $\frac{1}{r}=\frac{1}{q}$ & $1$ & $1$ & $1$ \\
$\widetilde\RCal_\textsc{csa} \equiva \RCal_\textsc{rb}$ & $1$      & $1$     & $1$      & $1$      & $1$ & $C$ \\
\bottomrule
\end{tabular}
%}
\end{table}
