\section{The dual problem of multitask bipartite ranking for new song recommendation}

The optimisation problem is
\begin{equation}
\label{eq:mbropt}
\begin{aligned}
&\min_{\V, \W, \mubm} \ \frac{C}{2} \left( \frac{1}{U} \sum_{u=1}^U \bv_u^\top \bv_u 
     + \frac{1}{N} \sum_{i=1}^N \w_i^\top \w_i + \mubm^\top \mubm \right) \\
& \hspace{5em}
     + \frac{1}{N} \sum_{i=1}^N \frac{1}{M_i^+} \sum_{m: y_m^i = 1} \ell \left( (\bv_{u(i)} + \w_i + \mubm)^\top \x_m 
     - \max_{n: y_n^i = 0} (\bv_{u(i)} + \w_i + \mubm)^\top \x_n \right).
\end{aligned}
\end{equation}

Let 
\begin{equation*}
\begin{aligned}
f_0(\V, \W, \mubm, \xibm) &= \frac{C}{2} \left( \frac{1}{U} \sum_{u=1}^U \bv_u^\top \bv_u 
     + \frac{1}{N} \sum_{i=1}^N \w_i^\top \w_i + \mubm^\top \mubm \right) \\
& \hspace{3em}
     + \frac{1}{N} \sum_{i=1}^N \frac{1}{M_i^+} \sum_{m: y_m^i = 1} 
       \ell \left( (\bv_{u(i)} + \w_i + \mubm)^\top \x_m - \xi_i \right) \\
f_{i,n} (\V, \W, \mubm, \xibm) &= (\bv_{u(i)} + \w_i + \mubm)^\top \x_n - \xi_i, \
i \in \{1,\dots,N\}, \, n \in \{1,\dots,M\} \ \mathrm{and} \ y_n^i = 0.
\end{aligned}
\end{equation*}

Problem (\ref{eq:mbropt}) is equivalent to 
\begin{equation}
\label{eq:stdopt}
\begin{aligned}
\min_{\V, \W, \mubm, \xibm} \ & f_0(\V, \W, \mubm, \xibm) \\
s.t. \quad & f_{i,n}(\V, \W, \mubm, \xibm) \le 0, \
i \in \{1,\dots,N\}, \, n \in \{1,\dots,M\} \ \mathrm{and} \ y_n^i = 0.
\end{aligned}
\end{equation}

The \emph{Lagrangian} of (\ref{eq:stdopt}) is
\begin{equation*}
\begin{aligned}
L(\V, \W, \mubm, \xibm, \betabm) 
&= f_0(\V, \W, \mubm, \xibm) + \sum_{i=1}^N \sum_{n: y_n^i = 0} \beta_i^n f_{i,n} (\V, \W, \mubm, \xibm), \
\beta_i^n \ge 0.
\end{aligned}
\end{equation*}

Note that the conjugate of the conjugate of a convex function is itself, \ie $f(\z) = f^{**}(\z) = \sup_\y \left(\z^\top \y - f^*(\y) \right)$, we have
\begin{equation*}
\begin{aligned}
\ell \left( (\bv_{u(i)} + \w_i + \mubm)^\top \x_m - \xi_i \right)
&= \sup_{\alpha_i^m} \left[ \left( (\bv_{u(i)} + \w_i + \mubm)^\top \x_m - \xi_i \right) \alpha_i^m - \ell^*(\alpha_i^m) \right],
\end{aligned}
\end{equation*}
where $\ell^*$ is the convex conjugate of surrogate loss $\ell$.

Let
\begin{equation*}
\begin{aligned}
g(\V, \W, \mubm, \xibm, \alphabm, \betabm)
&= \frac{C}{2} \left( \frac{1}{U} \sum_{u=1}^U \bv_u^\top \bv_u 
     + \frac{1}{N} \sum_{i=1}^N \w_i^\top \w_i + \mubm^\top \mubm \right) \\
& \hspace{3em}
     + \sum_{i=1}^N \frac{1}{N M_i^+} \sum_{m: y_m^i = 1} \left( (\bv_{u(i)} + \w_i + \mubm)^\top \x_m - \xi_i \right) \alpha_i^m \\
& \hspace{3em}
     + \sum_{i=1}^N \sum_{n: y_n^i = 0} \beta_i^n \left( (\bv_{u(i)} + \w_i + \mubm)^\top \x_n - \xi_i \right), \\
r(\alphabm)
&= \sum_{i=1}^N \frac{1}{N M_i^+} \sum_{m: y_m^i = 1} \ell^*(\alpha_i^m),
\end{aligned}
\end{equation*}
then 
\begin{equation*}
\begin{aligned}
L(\V, \W, \mubm, \xibm, \alphabm, \betabm) 
= \sup_\alphabm \left[ g(\V, \W, \mubm, \xibm, \alphabm, \betabm) - r(\alphabm) \right].
\end{aligned}
\end{equation*}

Assuming strong duality, the \emph{Lagrangian dual function} of (\ref{eq:stdopt}) is
\begin{equation}
\label{eq:dualfunc}
\begin{aligned}
&\inf_{\V, \W, \mubm, \xibm} \ L(\V, \W, \mubm, \xibm, \alphabm, \betabm) \\
&= \inf_{\V, \W, \mubm, \xibm} \ \sup_\alphabm \left[ g(\V, \W, \mubm, \xibm, \alphabm, \betabm) - r(\alphabm) \right] \\
&= \sup_\alphabm \ \inf_{\V, \W, \mubm, \xibm} \left[ g(\V, \W, \mubm, \xibm, \alphabm, \betabm) - r(\alphabm) \right] \\
&= \sup_\alphabm \left[ \inf_{\V, \W, \mubm, \xibm} \ g(\V, \W, \mubm, \xibm, \alphabm, \betabm) - r(\alphabm) \right] \\
&= \max_\alphabm \left[ \min_{\V, \W, \mubm, \xibm} \ g(\V, \W, \mubm, \xibm, \alphabm, \betabm) - r(\alphabm) \right].
\end{aligned}
\end{equation}

To solve the inner (unconstrained) minimisation, let
\begin{equation*}
\begin{aligned}
\zero &= \frac{\partial g}{\partial \bv_u} 
       = \frac{C}{U} \bv_u 
         + \sum_{i \in P_u} \left( \frac{1}{N M_i^+} \sum_{m: y_m^i = 1} \alpha_i^m \x_m + \sum_{n: y_n^i = 0} \beta_i^n \x_n \right) \\
\zero &= \frac{\partial g}{\partial \w_i}
       = \frac{C}{N} \w_i + \frac{1}{N M_i^+} \sum_{m: y_m^i = 1} \alpha_i^m \x_m + \sum_{n: y_n^i = 0} \beta_i^n \x_n \\
\zero &= \frac{\partial g}{\partial \mubm} 
       = C \mubm + \sum_{i=1}^N \left( \frac{1}{N M_i^+} \sum_{m: y_m^i = 1} \alpha_i^m \x_m + \sum_{n: y_n^i = 0} \beta_i^n \x_n \right) \\
0     &= \frac{\partial g}{\partial \xi_i}
       = - \frac{1}{N M_i^+} \sum_{m: y_m^i = 1} \alpha_i^m - \sum_{n: y_n^i = 0} \beta_i^n \\
\end{aligned}
\end{equation*}
where $P_u$ denotes the indices of playlists from user $u$.

To simplify the notation, let $\Thetabm \in \R^{N \times M}$ such that
\begin{equation*}
\theta_i^m = 
\begin{cases}
    \frac{\alpha_i^m}{N M_i^+}, & y_m^i = 1 \\
    \beta_i^m, & y_m^i = 0, \ i \in \{1,\dots,N\}, \, m \in \{1,\dots,M\},
\end{cases}
\end{equation*}
then we have
\begin{equation}
\label{eq:dual2primal}
\begin{aligned}
\bv_u
&= -\frac{U}{C} \sum_{i \in P_u} \sum_{m=1}^M \theta_i^m \x_m
 = -\frac{U}{C} \sum_{i \in P_u} \left( \thetabm_i^\top \X \right)^\top \\
%
\w_i
&= -\frac{N}{C} \sum_{m=1}^M \theta_i^m \x_m
 = -\frac{N}{C} \left( \thetabm_i^\top \X \right)^\top \\
%
\mubm
&= -\frac{1}{C} \sum_{i=1}^N \sum_{m=1}^M \theta_i^m \x_m 
 = -\frac{1}{C} \sum_{i=1}^N \left( \thetabm_i^\top \X \right)^\top \\
%
0
&= \sum_{m=1}^M \theta_i^m 
 = \thetabm_i^\top \one_M
\end{aligned}
\end{equation}

Thus,
\begin{equation*}
\begin{aligned}
&\min_{\V, \W, \mubm, \xibm} g(\V, \W, \mubm, \xibm, \alphabm, \betabm) \\
&= \frac{C}{2} \left( \frac{1}{U} \sum_{u=1}^U \bv_u^\top \bv_u 
     + \frac{1}{N} \sum_{i=1}^N \w_i^\top \w_i + \mubm^\top \mubm \right) \\
& \hspace{3em}
     + \sum_{i=1}^N \left( 
       \sum_{m: y_m^i = 1} \frac{\alpha_i^m}{N M_i^+} (\bv_{u(i)} + \w_i + \mubm)^\top \x_m 
     + \sum_{n: y_n^i = 0} \beta_i^n (\bv_{u(i)} + \w_i + \mubm)^\top \x_n \right) \\
& \hspace{3em}
     - \sum_{i=1}^N \xi_i \left( \sum_{m: y_m^i = 1} \frac{\alpha_i^m}{N M_i^+} + \sum_{n: y_n^i = 0} \beta_i^n \right) \\
&= \frac{C}{2} \left( \frac{1}{U} \sum_{u=1}^U \bv_u^\top \bv_u 
     + \frac{1}{N} \sum_{i=1}^N \w_i^\top \w_i + \mubm^\top \mubm \right)
     + \sum_{i=1}^N \sum_{m=1}^M \theta_i^m (\bv_{u(i)} + \w_i + \mubm)^\top \x_m
\end{aligned}
\end{equation*}

Note that by (\ref{eq:dual2primal})
\begin{equation*}
\begin{aligned}
&\sum_{i=1}^N \sum_{m=1}^M \theta_i^m (\bv_{u(i)} + \w_i + \mubm)^\top \x_m \\
&= \sum_{i=1}^N (\bv_{u(i)} + \w_i + \mubm)^\top \left( \sum_{m=1}^M \theta_i^m \x_m \right) \\
&= \sum_{i=1}^N (\bv_{u(i)} + \w_i + \mubm)^\top \left( \thetabm_i^\top \X \right)^\top \\
&= \sum_{i=1}^N \bv_{u(i)}^\top \left( \thetabm_i^\top \X \right)^\top 
     + \sum_{i=1}^N \w_i^\top \left( \thetabm_i^\top \X \right)^\top 
     + \mubm^\top \left( \sum_{i=1}^N \thetabm_i^\top \X \right)^\top \\
&= \sum_{u=1}^U \sum_{i \in P_u} \bv_u^\top \left( \thetabm_i^\top \X \right)^\top
     - \frac{C}{N} \sum_{i=1}^N \w_i^\top \w_i 
     - C \mubm^\top \mubm \\
&= \sum_{u=1}^U \bv_u^\top \left( \sum_{i \in P_u} \thetabm_i^\top \X \right)^\top
     - \frac{C}{N} \sum_{i=1}^N \w_i^\top \w_i 
     - C \mubm^\top \mubm \\
&= -\frac{C}{U} \sum_{u=1}^U \bv_u^\top \bv_u
     - \frac{C}{N} \sum_{i=1}^N \w_i^\top \w_i 
     - C \mubm^\top \mubm \\
&= -C \left( \frac{1}{U} \sum_{u=1}^U \bv_u^\top \bv_u
     + \frac{1}{N} \sum_{i=1}^N \w_i^\top \w_i 
     + \mubm^\top \mubm \right)
\end{aligned}
\end{equation*}

As a result,
\begin{equation}
\label{eq:gmin}
\begin{aligned}
&\min_{\V, \W, \mubm, \xibm} g(\V, \W, \mubm, \xibm, \alphabm, \betabm) \\
&= \frac{C}{2} \left( \frac{1}{U} \sum_{u=1}^U \bv_u^\top \bv_u 
     + \frac{1}{N} \sum_{i=1}^N \w_i^\top \w_i + \mubm^\top \mubm \right)
     -C \left( \frac{1}{U} \sum_{u=1}^U \bv_u^\top \bv_u + \frac{1}{N} \sum_{i=1}^N \w_i^\top \w_i + \mubm^\top \mubm \right) \\
&= -\frac{C}{2} \left( \frac{1}{U} \sum_{u=1}^U \bv_u^\top \bv_u + \frac{1}{N} \sum_{i=1}^N \w_i^\top \w_i + \mubm^\top \mubm \right) \\
&= -\frac{1}{2 C} \left[
     U \sum_{u=1}^U \left( \sum_{i \in P_u} \thetabm_i^\top \X \right) \left( \sum_{i \in P_u} \thetabm_i^\top \X \right)^\top
   + N \sum_{i=1}^N \left( \thetabm_i^\top \X \right) \left( \thetabm_i^\top \X \right)^\top
   + \left( \sum_{i=1}^N \thetabm_i^\top \X \right) \left( \sum_{i=1}^N \thetabm_i^\top \X \right)^\top \right]
\end{aligned}
\end{equation}

Lastly, if we choose the exponential surrogate $\ell(f, y) = e^{-fy}$, then
\begin{equation*}
\ell^*(\alpha) = \sup_z \left(\alpha z - \ell(z) \right) = \max_z \left(\alpha z - e^{-z} \right),
\end{equation*}
let 
\begin{equation*}
0 = \frac{\partial (\alpha z - e^{-z})}{\partial z} = \alpha + e^{-z},
\end{equation*}
so we have
\begin{equation*}
z = -\log(-\alpha),
\end{equation*}
then
\begin{equation*}
\ell^*(\alpha) = \alpha ( 1 - \log(-\alpha) ), \ \alpha < 0.
\end{equation*}
Thus,
\begin{equation}
\label{eq:expconj}
\begin{aligned}
r(\alphabm)
&= \sum_{i=1}^N \frac{1}{N M_i^+} \sum_{m: y_m^i = 1} \ell^*(\alpha_i^m) \\
&= \sum_{i=1}^N \frac{1}{N M_i^+} \sum_{m: y_m^i = 1} \alpha_i^m (1 - \log(-\alpha_i^m)) \\
&= \sum_{i=1}^N \sum_{m: y_m^i = 1} \frac{\alpha_i^m}{N M_i^+} \left(1 - \log \left( -\frac{\alpha_i^m}{N M_i^+} \cdot N M_i^+ \right) \right) \\
&= \sum_{i=1}^N \sum_{m: y_m^i = 1} \theta_i^m \left(1 - \log(-\theta_i^m) - \log(N M_i^+) \right) \\
&= \sum_{i=1}^N \left( \y_i \circ \thetabm_i \right)^\top 
                \left( \y_i \circ \left( (1 - \log(N M_i^+)) \one_M - \log(-\thetabm_i) \right) \right) \\
&= \sum_{i=1}^N \left( \y_i \circ \thetabm_i \right)^\top 
                \left( (1 - \log(N M_i^+)) \y_i - \y_i \circ \log(-\thetabm_i) \right)
\end{aligned}
\end{equation}

By (\ref{eq:dualfunc}, \ref{eq:gmin}, \ref{eq:expconj}), the \emph{Lagrangian dual problem} of (\ref{eq:stdopt}) when using the exponential surrogate is
\begin{equation*}
\begin{aligned}
&\max_{\betabm} \ \inf_{\V, \W, \mubm, \xibm} L(\V, \W, \mubm, \xibm, \alphabm, \betabm) \\
&= \max_{\alphabm, \betabm} \left[ \min_{\V, \W, \mubm, \xibm} g(\V, \W, \mubm, \xibm, \alphabm, \betabm) - r(\alphabm) \right] \\
&= \min_{\Thetabm} \, \frac{1}{2 C} \left[
     U \sum_{u=1}^U \left( \sum_{i \in P_u} \thetabm_i^\top \X \right) \left( \sum_{i \in P_u} \thetabm_i^\top \X \right)^\top
   + N \sum_{i=1}^N \left( \thetabm_i^\top \X \right) \left( \thetabm_i^\top \X \right)^\top
   + \left( \sum_{i=1}^N \thetabm_i^\top \X \right) \left( \sum_{i=1}^N \thetabm_i^\top \X \right)^\top \right] \\
& \hspace{5em}
   + \sum_{i=1}^N \left( \y_i \circ \thetabm_i \right)^\top 
     \left( (1 - \log(N M_i^+)) \y_i - \y_i \circ \log(-\thetabm_i) \right).
\end{aligned}
\end{equation*}
subject to constraints
\begin{equation*}
\begin{aligned}
\Thetabm \one_M &= \zero_N, \\
\theta_i^m & < 0, \ \mathrm{if} \ y_m^i = 1, \\
\theta_i^n & \ge 0, \ \mathrm{if} \ y_n^i = 0, \ i \in \{1,\dots,N\}, \, m,n \in \{1,\dots,M\}.
\end{aligned}
\end{equation*}

The dual problem is a constrained convex minimisation problem, 
which can be optimised directly using a general convext optimisation method, \eg the interior point method.
However, if $M \gg D$ and $N \gg D$, recall that $\Thetabm \in \R^{N \times M}$, 
the number of dual variables ($N \times M$) may be too big to optimise in practice.

To deal with this situation, let $\lambdabm_i \in \R^D$ be
\begin{equation*}
\lambdabm_i = \X^\top \thetabm_i, \ i \in \{1,\dots,N\},
\end{equation*}
or equivalently $\Lambdabm = \Thetabm \X$, $\Lambdabm \in \R^{N \times D}$, then
\begin{equation*}
\thetabm_i = \Z \lambdabm_i,
\end{equation*}
where $\Z \in \R^{M \times D}$ is the left inverse of $\X^\top$, \ie $\Z \X^\top = \I_M$.

The dual problem can be rewritten as 
\begin{equation*}
\begin{aligned}
\min_{\Lambdabm} \ & \frac{1}{2 C} \left[
     U \sum_{u=1}^U \left( \sum_{i \in P_u} \lambdabm_i \right)^\top \left( \sum_{i \in P_u} \lambdabm_i \right)
   + N \sum_{i=1}^N \lambdabm_i^\top \lambdabm_i
   + \left( \sum_{i=1}^N \lambdabm_i \right)^\top \left( \sum_{i=1}^N \lambdabm_i \right) \right] \\
& \hspace{2em}
   + \sum_{i=1}^N \left( \y_i \circ \Z \lambdabm_i \right)^\top 
     \left( (1 - \log(N M_i^+)) \y_i - \y_i \circ \log(-\Z \lambdabm_i) \right) \\
s.t. \
& \left( \Z \lambdabm_i \right)^\top \one_M = 0, \\
& \z_m^\top \lambdabm_i < 0, \ \mathrm{if} \ y_m^i = 1, \\
& \z_n^\top \lambdabm_i \ge 0, \ \mathrm{if} \ y_n^i = 0, \ i \in \{1,\dots,N\}, \, m,n \in \{1,\dots,M\}.
\end{aligned}
\end{equation*}

Let $\Lambdabm^*$ be an optimimal solution of the above problem, then
\begin{equation*}
\begin{aligned}
\bv_u^*
&= -\frac{U}{C} \sum_{i \in P_u} \left( (\thetabm_i^*)^\top \X \right)^\top 
 = -\frac{U}{C} \sum_{i \in P_u} \lambdabm_i^* \\
%
\w_i^*
&= -\frac{N}{C} \left( (\thetabm_i^*)^\top \X \right)^\top 
 = -\frac{N}{C} \lambdabm_i^* \\
%
\mubm^*
&= -\frac{1}{C} \sum_{i=1}^N \left( (\thetabm_i^*)^\top \X \right)^\top 
 = -\frac{1}{C} \sum_{i=1}^N \lambdabm_i^*
\end{aligned}
\end{equation*}
