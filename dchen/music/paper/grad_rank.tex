\section{Derivative for bipartite ranking}

The optimisation objective for playlist augmentation is:
\begin{equation}
\label{eq:primal_pla}
\begin{aligned}
J_\textsc{pla} 
&= \frac{C}{2} \left( \frac{1}{U} \sum_{u=1}^U \bv_u^\top \bv_u 
     + \frac{1}{N} \sum_{i=1}^N \w_i^\top \w_i + \mubm^\top \mubm \right) \\
& \hspace{5em}
     + \frac{1}{N} \sum_{i=1}^N \frac{1}{M_+^i} \sum_{m: y_m^i = 1} \ell \left( (\bv_{u(i)} + \w_i + \mubm)^\top \phibm_{u(i),m}
     - \max_{n: y_n^i = 0} (\bv_{u(i)} + \w_i + \mubm)^\top \phibm_{u(i),n} \right).
\end{aligned}
\end{equation}

It can be approximated as
\begin{equation*}
\begin{aligned}
J_\textsc{pla} 
&\approx \frac{C}{2} \left( \frac{1}{U} \sum_{u=1}^U \bv_u^\top \bv_u + \frac{1}{N} \sum_{i=1}^N \w_i^\top \w_i + \mubm^\top \mubm \right) \\
& \hspace{3em}
  + \sum_{i=1}^N \frac{1}{N M_+^i} \sum_{m: y_m^i = 1} 
    e^{-(\bv_{u(i)} + \w_i + \mubm)^\top \phibm_{u(i),m}}
    \left( \sum_{n: y_n^i = 0} e^{p (\bv_{u(i)} + \w_i + \mubm)^\top \phibm_{u(i),n}} \right)^\frac{1}{p} \\
&= \frac{C}{2} \left( \frac{1}{U} \sum_{u=1}^U \bv_u^\top \bv_u + \frac{1}{N} \sum_{i=1}^N \w_i^\top \w_i + \mubm^\top \mubm \right) \\
& \hspace{3em}
  + \sum_{i=1}^N \frac{1}{N M_+^i} 
    \left( \sum_{m: y_m^i = 1} e^{-(\bv_{u(i)} + \w_i + \mubm)^\top \phibm_{u(i),m}} \right)
    \left( \sum_{n: y_n^i = 0} e^{p (\bv_{u(i)} + \w_i + \mubm)^\top \phibm_{u(i),n}} \right)^\frac{1}{p}.
\end{aligned}
\end{equation*}

The gradient w.r.t. $\bv_u, \w_i$ and $\mubm$ can be approximated as 
\begin{equation*}
\begin{aligned}
\frac{\partial J_\textsc{pla}}{\partial \bv_u}
&\approx \frac{C}{U} \bv_u
  + \sum_{i \in P_u} \frac{1}{N M_+^i} \left[
    \left(-\sum_{m: y_m^i = 1} \phibm_{u,m} e^{-(\bv_u + \w_i + \mubm)^\top \phibm_{u,m}} \right)
    \left( \sum_{n: y_n^i = 0} e^{p (\bv_u + \w_i + \mubm)^\top \phibm_{u,n}} \right)^\frac{1}{p} \right. \\
& \hspace{1.5em} + \left.
    \left( \sum_{m: y_m^i = 1} e^{-(\bv_u + \w_i + \mubm)^\top \phibm_{u,m}} \right)
    \left( \sum_{n: y_n^i = 0} e^{p (\bv_u + \w_i + \mubm)^\top \phibm_{u,n}} \right)^{\frac{1}{p} - 1}
    \left( \sum_{n: y_n^i = 0} \phibm_{u,n} e^{p (\bv_u + \w_i + \mubm)^\top \phibm_{u,n}} \right) \right] \\
&= \frac{C}{U} \bv_u
  + \sum_{i \in P_u} \frac{1}{N M_+^i} 
    \left( \sum_{m: y_m^i = 1} e^{-(\bv_u + \w_i + \mubm)^\top \phibm_{u,m}} \right)
    \left( \sum_{n: y_n^i = 0} e^{p (\bv_u + \w_i + \mubm)^\top \phibm_{u,n}} \right)^\frac{1}{p} \\
& \hspace{9.8em} \left[ 
    \frac{ \displaystyle \sum_{n: y_n^i = 0} \phibm_{u,n} e^{p (\bv_u + \w_i + \mubm)^\top \phibm_{u,n}} }
         { \displaystyle \sum_{n: y_n^i = 0} e^{p (\bv_u + \w_i + \mubm)^\top \phibm_{u,n}} }
    - \frac{ \displaystyle \sum_{m: y_m^i = 1} \phibm_{u,m} e^{-(\bv_u + \w_i + \mubm)^\top \phibm_{u,m}} }
           { \displaystyle \sum_{m: y_m^i = 1} e^{-(\bv_u + \w_i + \mubm)^\top \phibm_{u,m}} } \right] \\
%
\frac{\partial J_\textsc{pla}}{\partial \w_i}
&\approx \frac{C}{N} \w_i
  + \frac{1}{N M_+^i} 
    \left( \sum_{m: y_m^i = 1} e^{-(\bv_{u(i)} + \w_i + \mubm)^\top \phibm_{u(i),m}} \right)
    \left( \sum_{n: y_n^i = 0} e^{p (\bv_{u(i)} + \w_i + \mubm)^\top \phibm_{u(i),n}} \right)^\frac{1}{p} \\
& \hspace{8em} \left[ 
    \frac{ \displaystyle \sum_{n: y_n^i = 0} \phibm_{u(i),n} e^{p (\bv_{u(i)} + \w_i + \mubm)^\top \phibm_{u(i),n}} }
         { \displaystyle \sum_{n: y_n^i = 0} e^{p (\bv_{u(i)} + \w_i + \mubm)^\top \phibm_{u(i),n}} }
    - \frac{ \displaystyle \sum_{m: y_m^i = 1} \phibm_{u(i),m} e^{-(\bv_{u(i)} + \w_i + \mubm)^\top \phibm_{u(i),m}} }
           { \displaystyle \sum_{m: y_m^i = 1} e^{-(\bv_{u(i)} + \w_i + \mubm)^\top \phibm_{u(i),m}} } \right] \\
%
\frac{\partial J_\textsc{pla}}{\partial \mubm}
&\approx C \mubm
  + \sum_{i=1}^N \frac{1}{N M_+^i} 
    \left( \sum_{m: y_m^i = 1} e^{-(\bv_{u(i)} + \w_i + \mubm)^\top \phibm_{u(i),m}} \right)
    \left( \sum_{n: y_n^i = 0} e^{p (\bv_{u(i)} + \w_i + \mubm)^\top \phibm_{u(i),n}} \right)^\frac{1}{p} \\
& \hspace{8em} \left[ 
    \frac{ \displaystyle \sum_{n: y_n^i = 0} \phibm_{u(i),n} e^{p (\bv_{u(i)} + \w_i + \mubm)^\top \phibm_{u(i),n}} }
         { \displaystyle \sum_{n: y_n^i = 0} e^{p (\bv_{u(i)} + \w_i + \mubm)^\top \phibm_{u(i),n}} }
    - \frac{ \displaystyle \sum_{m: y_m^i = 1} \phibm_{u(i),m} e^{-(\bv_{u(i)} + \w_i + \mubm)^\top \phibm_{u(i),m}} }
           { \displaystyle \sum_{m: y_m^i = 1} e^{-(\bv_{u(i)} + \w_i + \mubm)^\top \phibm_{u(i),m}} } \right]
\end{aligned}
\end{equation*}


The Lagrangian dual problem when using the exponential surrogate is
\begin{equation}
\label{eq:dual_pla}
\begin{aligned}
\min_{\Thetabm} \ & \frac{1}{2 C} \left[
     U \sum_{u=1}^U \left( \sum_{i \in P_u} \thetabm_i \right)^\top \Phibm_u \Phibm_u^\top \left( \sum_{i \in P_u} \thetabm_i \right)^\top
   + N \sum_{i=1}^N \thetabm_i^\top \Phibm_{u(i)} \Phibm_{u(i)}^\top \thetabm_i \right. \\
& \hspace{2em} \left.
   + \left( \sum_{u=1}^U \sum_{i \in P_u} \thetabm_i^\top \Phibm_u \right)
     \left( \sum_{u=1}^U \sum_{i \in P_u} \thetabm_i^\top \Phibm_u \right)^\top \right]
   + \sum_{i=1}^N \left( \y_i \circ \thetabm_i \right)^\top 
     \left( (1 - \log(N M_+^i)) \y_i - \y_i \circ \log(-\thetabm_i) \right), \\
s.t. \ 
& \Thetabm \one_M = \zero_N, \\
&\theta_i^m < 0, \ \mathrm{if} \ y_m^i = 1, \\
&\theta_i^n \ge 0, \ \mathrm{if} \ y_n^i = 0, \ i \in \{1,\dots,N\}, \, m,n \in \{1,\dots,M\}.
\end{aligned}
\end{equation}

The gradient of the optimisation objective w.r.t. to dual variables $\thetabm_k \in \R^{M}$, $k \in \{1,\dots,N\}$,
\begin{equation*}
\begin{aligned}
\frac{\partial J_\textsc{pla}'}{\partial \thetabm_k}
&= \frac{1}{2 C} \left( U \cdot 2 \Phibm_{u(k)} \left(\sum_{i \in P_{u(k)}} \thetabm_i^\top \Phibm_{u(k)} \right)^\top 
   + N \cdot 2 \Phibm_{u(k)} \left( \thetabm_k^\top \Phibm_{u(k)} \right)^\top
   + 2 \Phibm_{u(k)} \left(\sum_{u=1}^U \sum_{i \in P_u} \thetabm_i^\top \Phibm_u \right)^\top \right) \\
& \hspace{2.5em}
   + (1 - \log(N M_k^+)) \y_k - \y_k \circ \log(-\thetabm_k) + \y_k \circ \thetabm_k \left( -\frac{-1}{-\y_k \circ \thetabm_k} \right) \\
&= \frac{\Phibm_{u(k)}}{C} \left( U \sum_{i \in P_{u(k)}} \thetabm_i^\top \Phibm_{u(k)}
   + N \thetabm_k^\top \Phibm_{u(k)} 
   + \sum_{u=1}^U \sum_{i \in P_u} \thetabm_i^\top \Phibm_u \right)^\top
   - \log(N M_k^+) \y_k - \y_k \circ \log(-\thetabm_k) \\
&= \frac{\Phibm_{u(k)}}{C} \left( \left( U \sum_{i \in P_{u(k)}} \thetabm_i + N \thetabm_k \right)^\top \Phibm_{u(k)} 
   + \sum_{u=1}^U \sum_{i \in P_u} \thetabm_i^\top \Phibm_u \right)^\top
   - \log(N M_k^+) \y_k - \y_k \circ \log(-\thetabm_k),
\end{aligned}
\end{equation*}
where $\Phibm_{u(k)} \in \R^{M \times (D + U - 1)}$ is the design matrix in which 
the $m$-th row is the concatenation of the feature vector of song $m$ 
and the number of listening events of all users except $u(k)$ (the user that owns playlist $k$) with regards to song $m$.

The Hessian is
\begin{equation*}
\begin{aligned}
\frac{\partial^2 J_\textsc{pla}'}{\partial \thetabm_k^2}
&= \frac{\Phibm_{u(k)}}{C} \left( U \Phibm_{u(k)} + N \Phibm_{u(k)} + \Phibm_{u(k)} \right)^\top - \Hb_k
 = \frac{1}{C} (U + N + 1) \Phibm_{u(k)} \Phibm_{u(k)}^\top - \Hb_k,
\end{aligned}
\end{equation*}
where $\Hb_k \in \R^{M \times M}$ is a diagonal matrix such that
\begin{equation*}
H_k^m = \frac{y_m^k}{\theta_k^m}, \ m = \{1,\dots,M\}.
\end{equation*}


The optimisation objective for new song recommendation is:
\begin{equation}
\label{eq:primal_nsr}
\begin{aligned}
J_\textsc{nsr}
&= \frac{C}{2} \left( \frac{1}{U} \sum_{u=1}^U \bv_u^\top \bv_u 
     + \frac{1}{N} \sum_{i=1}^N \w_i^\top \w_i + \mubm^\top \mubm \right) \\
& \hspace{5em}
     + \frac{1}{N} \sum_{i=1}^N \frac{1}{M_+^i} \sum_{m: y_m^i = 1} \ell \left( (\bv_{u(i)} + \w_i + \mubm)^\top \x_m 
     - \max_{n: y_n^i = 0} (\bv_{u(i)} + \w_i + \mubm)^\top \x_n \right).
\end{aligned}
\end{equation}

It can be approximated as
\begin{equation*}
\begin{aligned}
J_\textsc{nsr} 
&\approx \frac{C}{2} \left( \frac{1}{U} \sum_{u=1}^U \bv_u^\top \bv_u + \frac{1}{N} \sum_{i=1}^N \w_i^\top \w_i + \mubm^\top \mubm \right) \\
& \hspace{3em}
  + \sum_{i=1}^N \frac{1}{N M_+^i} 
    \left( \sum_{m: y_m^i = 1} e^{-(\bv_{u(i)} + \w_i + \mubm)^\top \x_m} \right)
    \left( \sum_{n: y_n^i = 0} e^{p (\bv_{u(i)} + \w_i + \mubm)^\top \x_n} \right)^\frac{1}{p}.
\end{aligned}
\end{equation*}

The gradient w.r.t. $\bv_u, \w_i$ and $\mubm$ can be approximated as 
\begin{equation*}
\begin{aligned}
\frac{\partial J_\textsc{nsr}}{\partial \bv_u}
&\approx \frac{C}{U} \bv_u
  + \sum_{i \in P_u} \frac{1}{N M_+^i} 
    \left( \sum_{m: y_m^i = 1} e^{-(\bv_u + \w_i + \mubm)^\top \x_m} \right)
    \left( \sum_{n: y_n^i = 0} e^{p (\bv_u + \w_i + \mubm)^\top \x_n} \right)^\frac{1}{p} \\
& \hspace{9.8em} \left[ 
    \frac{ \displaystyle \sum_{n: y_n^i = 0} \x_n e^{p (\bv_u + \w_i + \mubm)^\top \x_n} }
         { \displaystyle \sum_{n: y_n^i = 0} e^{p (\bv_u + \w_i + \mubm)^\top \x_n} }
    - \frac{ \displaystyle \sum_{m: y_m^i = 1} \x_m e^{-(\bv_u + \w_i + \mubm)^\top \x_m} }
           { \displaystyle \sum_{m: y_m^i = 1} e^{-(\bv_u + \w_i + \mubm)^\top \x_m} } \right] \\
%
\frac{\partial J_\textsc{nsr}}{\partial \w_i}
&\approx \frac{C}{N} \w_i
  + \frac{1}{N M_+^i} 
    \left( \sum_{m: y_m^i = 1} e^{-(\bv_{u(i)} + \w_i + \mubm)^\top \x_m} \right)
    \left( \sum_{n: y_n^i = 0} e^{p (\bv_{u(i)} + \w_i + \mubm)^\top \x_n} \right)^\frac{1}{p} \\
& \hspace{8em} \left[ 
    \frac{ \displaystyle \sum_{n: y_n^i = 0} \x_n e^{p (\bv_{u(i)} + \w_i + \mubm)^\top \x_n} }
         { \displaystyle \sum_{n: y_n^i = 0} e^{p (\bv_{u(i)} + \w_i + \mubm)^\top \x_n} }
    - \frac{ \displaystyle \sum_{m: y_m^i = 1} \x_m e^{-(\bv_{u(i)} + \w_i + \mubm)^\top \x_m} }
           { \displaystyle \sum_{m: y_m^i = 1} e^{-(\bv_{u(i)} + \w_i + \mubm)^\top \x_m} } \right] \\
%
\frac{\partial J_\textsc{nsr}}{\partial \mubm}
&\approx C \mubm
  + \sum_{i=1}^N \frac{1}{N M_+^i} 
    \left( \sum_{m: y_m^i = 1} e^{-(\bv_{u(i)} + \w_i + \mubm)^\top \x_m} \right)
    \left( \sum_{n: y_n^i = 0} e^{p (\bv_{u(i)} + \w_i + \mubm)^\top \x_n} \right)^\frac{1}{p} \\
& \hspace{8em} \left[ 
    \frac{ \displaystyle \sum_{n: y_n^i = 0} \x_n e^{p (\bv_{u(i)} + \w_i + \mubm)^\top \x_n} }
         { \displaystyle \sum_{n: y_n^i = 0} e^{p (\bv_{u(i)} + \w_i + \mubm)^\top \x_n} }
    - \frac{ \displaystyle \sum_{m: y_m^i = 1} \x_m e^{-(\bv_{u(i)} + \w_i + \mubm)^\top \x_m} }
           { \displaystyle \sum_{m: y_m^i = 1} e^{-(\bv_{u(i)} + \w_i + \mubm)^\top \x_m} } \right]
\end{aligned}
\end{equation*}


The Lagrangian dual problem when using the exponential surrogate is
\begin{equation}
\label{eq:dual_nsr}
\begin{aligned}
\min_{\Thetabm} \ & \frac{1}{2 C} \left[
     U \sum_{u=1}^U \left( \sum_{i \in P_u} \thetabm_i \right)^\top \X \X^\top \left( \sum_{i \in P_u} \thetabm_i \right)
   + N \sum_{i=1}^N \thetabm_i^\top \X \X^\top \thetabm_i
   + \left( \sum_{i=1}^N \thetabm_i \right)^\top \X \X^\top \left( \sum_{i=1}^N \thetabm_i \right) \right] \\
& \hspace{2em}
   + \sum_{i=1}^N \left( \y_i \circ \thetabm_i \right)^\top \left( (1 - \log(N M_+^i)) \y_i - \y_i \circ \log(-\thetabm_i) \right), \\
s.t. \ 
& \Thetabm \one_M = \zero_N, \\
& \theta_i^m < 0, \ \mathrm{if} \ y_m^i = 1, \\
& \theta_i^n \ge 0, \ \mathrm{if} \ y_n^i = 0, \ i \in \{1,\dots,N\}, \, m,n \in \{1,\dots,M\}.
\end{aligned}
\end{equation}
Note that the above objective can be computed as
\begin{equation*}
\begin{aligned}
J &= \frac{1}{2 C} \sum_{n=1}^N \sum_{n'=1}^N (U \A + N \I_N + \one_{N \times N}) 
     \circ \left( \Thetabm^\top \X \right) \left( \Thetabm^\top \X \right)^\top \\
& \hspace{3em}
     + \sum_{i=1}^N \thetabm_i^\top \left( (1 - \log(N M_+^i)) \y_i - \y_i \circ \log(-\thetabm_i) \right),
\end{aligned}
\end{equation*}
where $\I_N \in \R^{N \times N}$ is an identity matrix and $\A \in \R^{N \times N}$ is a symmetric matrix such that
\begin{equation*}
A_{ij} = 
\begin{cases}
1, & \text{playlist} \ i \ \text{and} \ j \ \text{from the same user}, \\
0, & \text{otherwise}.
\end{cases}
\end{equation*}

The gradient of the optimisation objective w.r.t. to dual variables $\thetabm_k \in \R^{M}$, $k \in \{1,\dots,N\}$,
\begin{equation*}
\begin{aligned}
\frac{\partial J_\textsc{nsr}'}{\partial \thetabm_k}
&= \frac{1}{2 C} \left( U \cdot 2 \X \left(\sum_{i \in P_{u(k)}} \thetabm_i^\top \X \right)^\top 
   + N \cdot 2 \X \left( \thetabm_k^\top \X \right)^\top
   + 2 \X \left(\sum_{i=1}^N \thetabm_i^\top \X \right)^\top \right) \\
& \hspace{2.5em}
   + (1 - \log(N M_k^+)) \y_k - \y_k \circ \log(-\thetabm_k) + \y_k \circ \thetabm_k \left( -\frac{-1}{-\y_k \circ \thetabm_k} \right) \\
&= \frac{\X}{C} \left( U \sum_{i \in P_{u(k)}} \thetabm_i^\top \X 
   + N \thetabm_k^\top \X 
   + \sum_{i=1}^N \thetabm_i^\top \X \right)^\top
   - \log(N M_k^+) \y_k - \y_k \circ \log(-\thetabm_k) \\
&= \frac{\X \X^\top}{C} \left( U \sum_{i \in P_{u(k)}} \thetabm_i + N \thetabm_k + \sum_{i=1}^N \thetabm_i \right)
   - \log(N M_k^+) \y_k - \y_k \circ \log(-\thetabm_k) \\
\end{aligned}
\end{equation*}

The Hessian is
\begin{equation*}
\begin{aligned}
\frac{\partial^2 J_\textsc{nsr}'}{\partial \thetabm_k^2}
&= \frac{\X}{C} \left( U \X + N \X + \X \right)^\top - \Hb_k
 = \frac{1}{C} (U + N + 1) \X \X^\top - \Hb_k,
\end{aligned}
\end{equation*}
where $\Hb_k \in \R^{M \times M}$ is a diagonal matrix such that
\begin{equation*}
H_k^m = \frac{y_m^k}{\theta_k^m}, \ m = \{1,\dots,M\}.
\end{equation*}
