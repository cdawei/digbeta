\section{The dual problem of bipartite ranking with multitask regularisation}

\begin{equation}
\label{eq:mtopt}
\begin{aligned}
\min_{\W} \ \frac{\lambda_1 + 2\lambda_2}{2 N} \sum_{i=1}^{N} \w_i^\top \w_i 
- \frac{2\lambda_2}{N (N - 1)} \sum_{i, j \in \{1,\dots,N\}, \, i < j} \w_i^\top \w_j
+ \frac{1}{N} \sum_{i = 1}^{N} \frac{1}{M_i^+} \sum_{m: y_i^m = 1} \ell \left( \w_i^\top \x^m - \max_{n: y_i^n = 0} \w_i^\top \x^n \right).
\end{aligned}
\end{equation}

Problem (\ref{eq:mtopt}) is hard to optimise in general due to the \emph{max} operator,
a widely used trick is to form its dual problem.
Let $C_1 = \frac{\lambda_1 + 2\lambda_2}{2 N}$, $C_2 = \frac{-2\lambda_2}{N (N - 1)}$ and
\begin{equation*}
\begin{aligned}
f_0(\W, \xibm) &=  C_1 \sum_{i=1}^{N} \w_i^\top \w_i + C_2 \sum_{i, j \in \{1,\dots,N\}, \, i < j} \w_i^\top \w_j
    + \sum_{i = 1}^{N} \frac{1}{N M_i^+} \sum_{m: y_i^m = 1} \ell \left( \w_i^\top \x^m - \max_{n: y_i^n = 0} \w_i^\top \x^n \right), \\
f_{i,n}(\w_i, \xi_i) &= \w_i^\top \x^n - \xi_i, \ i \in \{1,\dots,N\}, \, n \in \{1,\dots,M\} \ \mathrm{and} \ y_n^i = 0,
\end{aligned}
\end{equation*}
where $\W \in \R^{N \times D}$ is the matrix of weights.

Then problem (\ref{eq:mtopt}) is equivalent to 
\begin{equation}
\label{eq:mtstd}
\begin{aligned}
\min_{\W, \xibm} \ & f_0(\W, \xibm) \\
s.t. \ & f_{i,n}(\w_i, \xi_i) \le 0, \ i \in \{1,\dots,N\}, \, n \in \{1,\dots,M\} \ \mathrm{and} \ y_n^i = 0.
\end{aligned}
\end{equation}
Let $\nu_i^n \ge 0$, the \emph{Lagrangian} of (\ref{eq:mtstd}) is
\begin{equation*}
\begin{aligned}
L(\W, \xibm, \nubm) 
&= f_0(\W, \xibm) + \sum_{i=1}^{N} \sum_{n: y_i^n = 0} \nu_i^n \cdot f_{i,n}(\w_i, \xi_i) \\
&= C_1 \sum_{i=1}^{N} \w_i^\top \w_i + C_2 \sum_{i, j \in \{1,\dots,N\}, \, i < j} \w_i^\top \w_j
   + \sum_{i = 1}^{N} \frac{1}{N M_i^+} \sum_{m: y_i^m = 1} \ell \left( \w_i^\top \x^m - \max_{n: y_i^n = 0} \w_i^\top \x^n \right) \\
& \quad  + \sum_{i=1}^{N} \sum_{n: y_i^n = 0} \nu_i^n \left( \w_i^\top \x^n - \xi_i \right) \\
\end{aligned}
\end{equation*}
Note that the conjugate of a convex function is itself, \ie $f(\z) = f^{**}(\z) = \sup_\y \left( \z^\top \y - f^*(\y) \right)$, we have
\begin{equation*}
\begin{aligned}
\ell \left( \w_i^\top \x^m - \max_{n: y_i^n = 0} \w_i^\top \x^n \right) 
= \sup_{\alpha_i^m} \left( (\w_i^\top \x^m - \xi_i) \alpha_i^m - \ell^*(\alpha_i^m) \right),
\end{aligned}
\end{equation*}
where $\ell^*$ is the convex conjugate of surrogate loss $\ell$, then
\begin{equation*}
\begin{aligned}
L(\W, \xibm, \nubm) 
&= C_1 \sum_{i=1}^{N} \w_i^\top \w_i + C_2 \sum_{i, j \in \{1,\dots,N\}, \, i < j} \w_i^\top \w_j
   + \sum_{i = 1}^{N} \frac{1}{N M_i^+} \sum_{m: y_i^m = 1} 
     \sup_{\alpha_i^m} \left( (\w_i^\top \x^m - \xi_i) \alpha_i^m - \ell^*(\alpha_i^m) \right) \\
& \quad  + \sum_{i=1}^{N} \sum_{n: y_i^n = 0} \nu_i^n \left( \w_i^\top \x^n - \xi_i \right) \\
&= \sup_\alphabm \left[ g(\W, \xibm, \alphabm, \nubm) - r(\alphabm) \right]
\end{aligned}
\end{equation*}
where 
\begin{equation*}
\begin{aligned}
g(\W, \xibm, \alphabm, \nubm)
&= C_1 \sum_{i=1}^{N} \w_i^\top \w_i + C_2 \sum_{i < j \in \{1,\dots,N\}} \w_i^\top \w_j
   + \frac{1}{N} \sum_{i = 1}^{N} \frac{1}{M_i^+} \sum_{m: y_i^m = 1} \left( \w_i^\top \x^m - \xi_i \right) \alpha_i^m \\
& \quad + \sum_{i=1}^{N} \sum_{n: y_i^n = 0} \nu_i^n \left( \w_i^\top \x^n - \xi_i \right), \\
r(\alphabm) &= \sum_{i = 1}^{N} \frac{1}{N M_i^+} \sum_{m: y_i^m = 1} \ell^*(\alpha_i^m).
\end{aligned}
\end{equation*}

Assuming strong duality, the \emph{Lagrangian dual function} of (\ref{eq:mtstd}) is
\begin{equation*}
\begin{aligned}
\inf_{\W, \xibm} L(\W, \xibm, \alphabm, \nubm)
&= \inf_{\W, \xibm} \sup_\alphabm \left[ g(\W, \xibm, \alphabm, \nubm) - r(\alphabm) \right] \\
&= \sup_\alphabm \inf_{\W, \xibm} \left[ g(\W, \xibm, \alphabm, \nubm) - r(\alphabm) \right] \\
&= \max_\alphabm \min_{\W, \xibm} \left[ g(\W, \xibm, \alphabm, \nubm) - r(\alphabm) \right] \\
&= \max_\alphabm \left[ \min_{\W, \xibm} \, g(\W, \xibm, \alphabm, \nubm) - r(\alphabm) \right].
\end{aligned}
\end{equation*}
To solve the (unconstrained) inner minimisation, let
\begin{equation*}
\begin{aligned}
\zero_D &= \frac{\partial g}{\partial \w_k} 
   = 2C_1 \w_k + C_2 \sum_{j \in \{1,\dots,N\}, \, j \ne k} \w_j 
     + \frac{1}{N M_k^+} \sum_{m: y_k^m = 1} \alpha_k^m \x^m
     + \sum_{n: y_k^n = 0} \nu_k^n \x^n \\
0 &= \frac{\partial g}{\partial \xi_k} 
   = -\frac{1}{N M_k^+} \sum_{m: y_k^m = 1} \alpha_k^m - \sum_{n: y_k^n = 0} \nu_k^n,
   \ k \in \{1,\dots,N\}.
\end{aligned}
\end{equation*}
To simplify the notation, let $\Thetabm \in \R^{M \times N}$ such that
\begin{equation*}
\theta_k^m = \begin{cases}
\frac{\alpha_k^m}{N M_k^+}, & y_k^m = 1 \\
\nu_k^m, & y_k^m = 0, \ m \in \{1,\dots,M\}, \, k \in \{1,\dots,N\},
\end{cases}
\end{equation*}
then we have
\begin{equation*}
\begin{aligned}
\bc_k^\top \W &= -\thetabm_k^\top \X, \\
\one_M^\top \thetabm_k &= 0, \ k \in \{1,\dots,N\}
\end{aligned}
\end{equation*}
where $\bc_k \in \R^{N}$ where the $k$-th element is $2C_1$ and all other elements are $C_2$,
$\X \in \R^{M \times N}$ is the design matrix.
The equivalent matrix form is
\begin{equation}
\label{eq:wxi}
\begin{aligned}
\C^\top \W &= -\Thetabm^\top \X \\
\Thetabm^\top \one_M &= \zero_{N}
\end{aligned}
\end{equation}
where $\C \in \R^{N \times N}$ such that
\begin{equation*}
C_{ij} = \begin{cases}
2C_1, & i = j \\
C_2,  & \mathrm{otherwise}
\end{cases}
\end{equation*}
Note that $\C = \C^\top$, thus $\W = -\C^{-1} \Thetabm^\top \X$.

Observe that
\begin{equation*}
\begin{aligned}
&\min_{\W, \xibm} \, g(\W, \xibm, \alphabm, \nubm) \\
&= C_1 \sum_{i=1}^{N} \w_i^\top \w_i + C_2 \sum_{i, j \in \{1,\dots,N\}, \, i < j} \w_i^\top \w_j
   + \sum_{i = 1}^{N} \w_i^\top \left( \frac{1}{N M_i^+} \sum_{m: y_i^m = 1} \alpha_i^m \x^m + \sum_{n: y_i^n = 0} \nu_i^n \x^n \right) \\
& \quad - \sum_{i = 1}^{N} \xi_i \left( \frac{1}{N M_i^+} \sum_{m: y_i^m = 1} \alpha_i^m + \sum_{n: y_i^n = 0} \nu_i^n \right) \\
&= C_1 \sum_{i=1}^{N} \w_i^\top \w_i + C_2 \sum_{i, j \in \{1,\dots,N\}, \, i < j} \w_i^\top \w_j 
   - \sum_{i = 1}^{N} \w_i^\top \left( 2C_1 \w_i + C_2 \sum_{j \in \{1,\dots,N\}, \, j \ne i} \w_j \right) \\
%&= - C_1 \sum_{i=1}^{N} \w_i^\top \w_i - C_2 \sum_{i < j \in \{1,\dots,N\}} \w_i^\top \w_j \\
%&= - C_1 \sum_{i=1}^{N} \w_i^\top \w_i - \frac{C_2}{2} \sum_{i=1}^{N} \w_i^\top \sum_{j \ne i \in \{1,\dots,N\}} \w_j \\
%&= -\frac{1}{2} \sum_{i=1}^{N} \w_i^\top \left( 2C_1 \w_i + C_2 \sum_{j \ne i \in \{1,\dots,N\}} \w_j \right) \\
%&= -\frac{1}{2} \sum_{i=1}^{N} \w_i^\top \left( \bc_i^\top \W \right) \\
%&= \frac{1}{2} \sum_{i=1}^{N} \w_i^\top \left( \thetabm_i^\top \X \right) \\
%&= \frac{1}{2} \Thetabm^\top \X \circ \W
&= - C_1 \sum_{i=1}^{N} \w_i^\top \w_i - \frac{C_2}{2} \sum_{i=1}^{N} \sum_{j \in \{1,\dots,N\}, \, j \ne i} \w_i^\top \w_j \\
&= - \frac{1}{2} \left( 2C_1 \sum_{i=1}^{N} \w_i^\top \w_i + C_2 \sum_{i=1}^{N} \sum_{j \in \{1,\dots,N\}, \, j \ne i} \w_i^\top \w_j \right) \\
&= - \frac{1}{2} \sum_{d=1}^{D} \sum_{d'=1}^{D} \C \circ \W^\top \W \\
&= - \frac{1}{2} \sum_{d=1}^{D} \sum_{d'=1}^{D} \C \circ \left( \C^{-1} \Thetabm^\top \X \right)^\top  \left( \C^{-1} \Thetabm^\top \X \right) \\
&= - \frac{1}{2} \sum_{d=1}^{D} \sum_{d'=1}^{D} \C \circ \left( \X^\top \Thetabm (\C^{-1})^2 \Thetabm^\top \X \right), \\
\end{aligned}
\end{equation*}
where we used the fact that the inverse of a symmetric matrix (assuming invertible) is also a symmetric matrix.

Lastly, the \emph{Lagrangian dual problem} of (\ref{eq:mtstd}) is
\begin{equation}
\label{eq:mtdual}
\begin{aligned}
\max_{\alphabm, \nubm} \inf_{\W, \xibm} L(\W, \xibm, \alphabm, \nubm) 
&= \max_{\alphabm, \nubm} \left[ \min_{\W, \xibm} g(\W, \xibm, \alphabm, \nubm) - r(\alphabm) \right] \\
&= \min_{\alphabm, \nubm} \left[ \frac{1}{2} \sum_{d=1}^{D} \sum_{d'=1}^{D} \C \circ \left( \X^\top \Thetabm (\C^{-1})^2 \Thetabm^\top \X \right) 
   + r(\alphabm) \right], \\
\end{aligned}
\end{equation}
subject to constraints
\begin{equation*}
\Thetabm^\top \one_M = \zero_{N}.
\end{equation*}

If we use the exponential surrogate, \ie $\ell(fy) = e^{-fy}$, then
\begin{equation*}
\begin{aligned}
\ell^*(\alpha) = \sup_z (\alpha z - \ell(z) ) = \max_z (\alpha z - e^{-z}).
\end{aligned}
\end{equation*}
Let 
\begin{equation*}
0 = \frac{\partial (\alpha z - e^{-z})} {\partial z} = \alpha + e^{-z},
\end{equation*}
we have 
\begin{equation*}
z = -\log(-\alpha).
\end{equation*}
Thus, 
\begin{equation*}
\begin{aligned}
r(\alphabm) 
&= \sum_{i = 1}^{N} \frac{1}{N M_i^+} \sum_{m: y_i^m = 1} \ell^*(\alpha_i^m) \\
&= \sum_{i = 1}^{N} \frac{1}{N M_i^+} \sum_{m: y_i^m = 1} \left( -\alpha_i^m \log(-\alpha_i^m) + \alpha_i^m \right) \\
&= \sum_{i = 1}^{N} \sum_{m: y_i^m = 1} \frac{\alpha_i^m}{N M_i^+} \left(1 - \log \left( -\frac{\alpha_i^m}{N M_i^+} 
   \cdot N M_i^+ \right) \right) \\
&= \sum_{i = 1}^{N} \sum_{m: y_i^m = 1} \theta_i^m \left( 1 - \log(-\theta_i^m) - \log(N M_i^+) \right)
\end{aligned}
\end{equation*}
