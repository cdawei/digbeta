\subsection{Variant III}


Let function $f(\cdot, \cdot)$ be
$$
f(\x^n; \w_k) := g(\x^n; \w_k) + b, \ n \in \{1,\dots,N\}, \, k \in \{1,\dots,K\}
$$
where $\w_k$ is a parameter vector, $b$ is a bias parameter, 
and $g(\x^n; \w_k)$ is a differentiable function (w.r.t. $\w_k$).

Further, let $u(\x, \y), \, v(\x, \y)$ be functions of $(\x, \y)$ (and independent of $\W$), 
we require both $u(\cdot,\cdot)$ and $v(\cdot,\cdot)$ to be bounded.

Suppose $\alpha, \beta, \gamma, \delta, P, Q \in \R_+$ are \emph{finite} positive numbers, 
we can define $\RCal_\textsc{mc}$ be the following risk\footnote{
We note that there is a equivalent definition:
$$
\RCal_\textsc{mc}(\W, b) 
= \gamma \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m=1} \exp(-\alpha f(\x^m; \w_i)) + 
C \delta \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n=0} \exp(\beta  f(\x^n; \w_j)),
$$
where $C = Q/P$, since multiplying a positive constant to a loss function will not change its minimiser.}
:
\begin{equation}
\label{eq:mc}
\RCal_\textsc{mc}(\W, b) = P \gamma \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m = 1} \exp(-\alpha f(\x^m; \w_i)) \, + \,
                           Q \delta \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  f(\x^n; \w_j)),
\end{equation}
and let $\RCal_\textsc{mr}$ be a risk defined as
\begin{equation}
\label{eq:mr}
\RCal_\textsc{mr}(\W, b) 
= \left[ \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m = 1} \exp(-\alpha f(\x^m; \w_i)) \right]^\gamma  
  \left[ \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  f(\x^n; \w_j)) \right]^\delta.
\end{equation}

Let 
\begin{equation}
\label{eq:mr}
\widetilde\RCal_\textsc{mr}(\W) 
= \left[ \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m = 1} \exp(-\alpha g(\x^m; \w_i)) \right]^\gamma  
  \left[ \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  g(\x^n; \w_j)) \right]^\delta.
\end{equation}


\begin{lemma}
\label{lemma:basic_ml}
If there exists a minimiser for $\RCal_\textsc{mr}(\W, b)$, then
$$
\alpha \gamma = \beta \delta,
$$
and 
$$
\RCal_\textsc{mr}(\W, b) = \widetilde\RCal_\textsc{mr}(\W).
$$
\end{lemma}


\begin{proof}
Note that 
\begin{equation}
\label{eq:mr_derive}
\begin{aligned}
\RCal_\textsc{mr}(\W, b) 
&= \left[ \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m = 1} \exp(-\alpha (g(\x^m; \w_i) + b)) \right]^\gamma 
   \left[ \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  (g(\x^n; \w_j) + b)) \right]^\delta \\
&= \exp( (\beta \delta - \alpha \gamma) b) 
   \left[ \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m = 1} \exp(-\alpha g(\x^m; \w_i)) \right]^\gamma 
   \left[ \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  g(\x^n; \w_j)) \right]^\delta.
\end{aligned}
\end{equation}
Suppose $(\W_\textsc{mr}, b_\textsc{mr}) \in \argmin_{\W, b} \RCal_\textsc{mr}(\W, b)$, then
\begin{equation*}
\begin{aligned}
0 
&= \frac{\partial \, \RCal_\textsc{mr}(\W, b)} {\partial \, b} \Bigg|_{\W = \W_\textsc{mr}, \, b = b_\textsc{mr}} \\
&= \exp((\beta \delta - \alpha \gamma) b) \cdot (\beta \delta - \alpha \gamma) 
   \left[ \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m = 1} \exp(-\alpha g(\x^m; \w_i)) \right]^\gamma  
   \left[ \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  g(\x^n; \w_j)) \right]^\delta
   \Bigg|_{\W = \W_\textsc{mr}, \, b = b_\textsc{mr}},
\end{aligned}
\end{equation*}
so we have $\beta \delta - \alpha \gamma = 0$, or equivalently,
\begin{equation*}
\alpha \gamma = \beta \delta.
\end{equation*}

Then by Eq.~(\ref{eq:mr_derive}), we have
\begin{equation*}
\begin{aligned}
\RCal_\textsc{mr}(\W, b) 
= \left[ \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m = 1} \exp(-\alpha g(\x^m; \w_i)) \right]^\gamma  
  \left[ \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  g(\x^n; \w_j)) \right]^\delta
= \widetilde\RCal_\textsc{mr}(\W).
\end{aligned}
\end{equation*}
\end{proof}

Lemma~\ref{lemma:basic_ml} shows that $\alpha \gamma = \beta \delta$ 
is a necessary condition for $\RCal_\textsc{mr}(\W,b)$ to have a (finite) minimiser.


\begin{theorem}
\label{theorem:mc2mr}
If $(\W_\textsc{mc}, b_\textsc{mc}) \in \argmin_{\W,b} \RCal_\textsc{mc}(\W, b)$ (assuming minimisers exist)
and $\alpha \gamma = \beta \delta$, then $\W_\textsc{mc} \in \argmin_\W \widetilde\RCal_\textsc{mr}(\W)$.
\end{theorem}


\begin{proof}
Let $(\W_\textsc{mc}, b_\textsc{mc})$ be a minimiser of $\RCal_\textsc{mc}(\W, b)$ (assuming minimisers exist),
then we have
\begin{equation*}
\begin{aligned}
0 
&= \frac{\partial \, \RCal_\textsc{mc}(\W, b)} {\partial \, b} \Bigg|_{\W = \W_\textsc{mc}, \, b = b_\textsc{mc}} \\
&= P \gamma \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m = 1} \exp(-\alpha (g(\x^m; \w_i) + b)) \cdot (-\alpha) \, + \,
   Q \delta \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  (g(\x^n; \w_j) + b)) \cdot \beta 
   \Big|_{\W = \W_\textsc{mc}, \, b = b_\textsc{mc}},
\end{aligned}
\end{equation*}
since $\alpha \gamma = \beta \delta$, we have
\begin{equation}
\label{eq:skew_ml}
  \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m = 1} \exp(-\alpha g(\x^m; \w_i)) 
= \frac{Q}{P} \exp \left((\alpha + \beta) b_\textsc{mc} \right) 
  \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  g(\x^n; \w_j)) 
  \Big|_{\W = \W_\textsc{mc}},
\end{equation}
which acts similarly as the skew condition in~\cite{ertekin2011equivalence}.

Further, for all $k \in \{1,\dots,K\}$, we have
\begin{equation*}
\begin{aligned}
0 
&= \frac{\partial \, \RCal_\textsc{mc}(\W, b)} {\partial \, \w_k} \Bigg|_{\W = \W_\textsc{mc}, \, b = b_\textsc{mc}} \\
&= P \gamma \sum_{m=1}^N u(\x^m, \y^m) \cdot \llb y_k^m = 1 \rrb \cdot \exp(-\alpha (g(\x^m; \w_k) + b)) \cdot (-\alpha) \cdot 
     \frac{\partial \, g(\x^m; \w_k)}{\partial \, \w_k} \\
&  \hspace{3em} + \,
   Q \delta \sum_{n=1}^N v(\x^n, \y^n) \cdot \llb y_k^n = 0 \rrb \cdot \exp( \beta  (g(\x^n; \w_k) + b)) \cdot \beta \cdot 
     \frac{\partial \, g(\x^n; \w_k)}{\partial \, \w_k} \Bigg|_{\W = \W_\textsc{mc}, \, b = b_\textsc{mc}},
\end{aligned}
\end{equation*}
since $\alpha \gamma = \beta \delta$, we have
\begin{equation}
\label{eq:grad_mc_eq0}
\begin{aligned}
&  \sum_{m=1}^N u(\x^m, \y^m) \cdot \llb y_k^m = 1 \rrb \cdot \exp(-\alpha g(\x^m; \w_k)) \frac{\partial \, g(\x^m; \w_k)}{\partial \, \w_k} \\
&= \frac{Q}{P} \exp \left( (\alpha + \beta) b_\textsc{mc} \right)
   \sum_{n=1}^N v(\x^n, \y^n) \cdot \llb y_k^n = 0 \rrb \cdot \exp(\beta g(\x^n; \w_k))  
   \frac{\partial \, g(\x^n; \w_k)}{\partial \, \w_k} \Bigg|_{\W = \W_\textsc{mc}}, 
   \quad \forall k \in \{1,\dots,K\}.
\end{aligned}
\end{equation}

Note that
\begin{equation}
\label{eq:grad_mr}
\begin{aligned}
\frac{\partial \, \widetilde\RCal_\textsc{mr}(\W)} {\partial \, \w_k}
=& \left[ \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m = 1} \exp(-\alpha g(\x^m; \w_i)) \right]^{\gamma-1} \gamma 
   \sum_{m=1}^N u(\x^m, \y^m) \cdot \llb y_k^m = 1 \rrb \cdot \exp(-\alpha g(\x^m; \w_k)) \cdot (-\alpha) \cdot 
   \frac{\partial \, g(\x^m; \w_k)}{\partial \, \w_k} \\
 & \left[ \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  g(\x^n; \w_j)) \right]^\delta \\
 & \ + \,
   \left[ \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m = 1} \exp(-\alpha g(\x^m; \w_i)) \right]^\gamma \\
 & \hspace{1.5em}
   \left[ \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  g(\x^n; \w_j)) \right]^{\delta-1} \delta
   \sum_{n=1}^N v(\x^n, \y^n) \cdot \llb y_k^n = 0 \rrb \cdot \exp( \beta  g(\x^n; \w_k)) \cdot \beta \cdot 
   \frac{\partial \, g(\x^n; \w_k)}{\partial \, \w_k} \\
=& \left[ \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m = 1} \exp(-\alpha g(\x^m; \w_i)) \right]^{\gamma-1}
   \left[ \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  g(\x^n; \w_j)) \right]^{\delta-1} (-\alpha \gamma) \\
 & \left[ 
   \left( \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  g(\x^n; \w_j)) \right)
   \left( \sum_{m=1}^N u(\x^m, \y^m) \cdot \llb y_k^m = 1 \rrb \cdot \exp(-\alpha g(\x^m; \w_k)) 
   \frac{\partial \, g(\x^m; \w_k)}{\partial \, \w_k} \right)
   \right. \\
 & \left. \ - 
   \left( \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m = 1} \exp(-\alpha g(\x^m; \w_i)) \right)
   \left( \sum_{n=1}^N v(\x^n, \y^n) \cdot \llb y_k^n = 0 \rrb \cdot \exp( \beta  g(\x^n; \w_k)) 
   \frac{\partial \, g(\x^n; \w_k)}{\partial \, \w_k} \right)
   \right].
\end{aligned}
\end{equation}

By first using Eq.~(\ref{eq:skew_ml}) and then (\ref{eq:grad_mc_eq0}), we have
\begin{equation}
\label{eq:mr_equal}
\begin{aligned}
&  \left( \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m = 1} \exp(-\alpha g(\x^m; \w_i)) \right)
   \left( \sum_{n=1}^N v(\x^n, \y^n) \cdot \llb y_k^n = 0 \rrb \cdot \exp( \beta  g(\x^n; \w_k)) 
   \frac{\partial \, g(\x^n; \w_k)}{\partial \, \w_k} \right)
   \Bigg|_{\W = \W_\textsc{mc}} \\
&= \frac{Q}{P} \exp \left((\alpha + \beta) b_\textsc{mc} \right) 
   \left( \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n=0} \exp( \beta g(\x^n; \w_j)) \right) \\
&  \hspace{1.5em}
   \left( \sum_{n=1}^N v(\x^n, \y^n) \cdot \llb y_k^n = 0 \rrb \cdot \exp( \beta g(\x^n; \w_k)) 
   \frac{\partial \, g(\x^n; \w_k)}{\partial \, \w_k} \right)
   \Bigg|_{\W = \W_\textsc{mc}} \\
&= \left( \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n=0} \exp(\beta g(\x^n; \w_j)) \right)
   \left( \sum_{m=1}^N u(\x^m, \y^m) \cdot \llb y_k^m = 1 \rrb \cdot \exp(-\alpha g(\x^m; \w_k)) 
   \frac{\partial \, g(\x^m; \w_k)}{\partial \, \w_k} \right)
   \Bigg|_{\W = \W_\textsc{mc}}.
\end{aligned}
\end{equation}

Finally, by Eq.~(\ref{eq:grad_mr}) and (\ref{eq:mr_equal}), we have\footnote{
Note that $\displaystyle \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m = 1} \exp(-\alpha g(\x^m; \w_i))$ and 
$\displaystyle \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  g(\x^n; \w_j))$
have finite values when $\W = \W_\textsc{mc}$ and $b = b_\textsc{mc}$,
since $\gamma$, $\delta$ are finite and $u(\cdot,\cdot), v(\cdot, \cdot)$ are bounded.}
$$
\frac{\partial \, \widetilde\RCal_\textsc{mr}(\W)} {\partial \, \w_k} \Bigg|_{\W = \W_\textsc{mc}} = 0.
$$
This means $\W_\textsc{mc}$ is a minimiser of $\widetilde\RCal_\textsc{mr}(\W)$.
\end{proof}


\begin{theorem}
\label{theorem:mr2mc}
If $\W_\textsc{mr} \in \argmin_\W \widetilde\RCal_\textsc{mr}(\W)$ (assuming minimisers exist) and $\alpha \gamma = \beta \delta$,
then 
$$
(\W_\textsc{mr}, \bt_\textsc{m}) \in \argmin_{\W,b} \, \RCal_\textsc{mc}(\W, b),
$$ 
where
$$
\bt_\textsc{m} = \frac{1}{\alpha + \beta} \ln 
      \frac{P \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m = 1} \exp(-\alpha g(\x^m; \w_i))}
           {Q \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  g(\x^n; \w_j)) \hspace{1em}} \Bigg|_{\W = \W_\textsc{mr}}.
$$
\end{theorem}


\begin{proof}
Let $\W_\textsc{mr} \in \argmin_\W \widetilde\RCal_\textsc{mr}(\W)$ (assuming minimisers exist), we have
$$
\frac{\partial \, \widetilde\RCal_\textsc{mr}(\W)} {\partial \, \w_k} \Bigg|_{\W = \W_\textsc{mr}} = 0,
\quad \forall k \in \{1,\dots,K\}
$$
then by Eq.~(\ref{eq:grad_mr}), for all $k \in \{1,\dots,K\}$ we have
\begin{equation}
\label{eq:grad_mr_eq0}
\begin{aligned}
&  \left( \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  g(\x^n; \w_j)) \right)
   \left( \sum_{m=1}^N u(\x^m, \y^m) \cdot \llb y_k^m = 1 \rrb \cdot \exp(-\alpha g(\x^m; \w_k)) 
   \frac{\partial \, g(\x^m; \w_k)}{\partial \, \w_k} \right)
   \Bigg|_{\W = \W_\textsc{mr}} \\
&= \left( \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m = 1} \exp(-\alpha g(\x^m; \w_i)) \right)
   \left( \sum_{n=1}^N v(\x^n, \y^n) \cdot \llb y_k^n = 0 \rrb \cdot \exp( \beta  g(\x^n; \w_k)) 
   \frac{\partial \, g(\x^n; \w_k)}{\partial \, \w_k} \right)
   \Bigg|_{\W = \W_\textsc{mr}}.
\end{aligned}
\end{equation}

Further, note that 
\begin{equation}
\label{eq:bias_ml}
\exp \left( (\alpha + \beta) \bt_\textsc{m} \right) 
= \frac{P \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m = 1} \exp(-\alpha g(\x^m; \w_i))}
       {Q \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  g(\x^n; \w_j)) \hspace{1em}} \Bigg|_{\W = \W_\textsc{mr}}.
\end{equation}

We first show that, when $\W = \W_\textsc{mr}$, the following two equalities hold:
\begin{equation}
\label{eq:skew_ml_equal}
P \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m = 1} \exp(-\alpha g(\x^m; \w_i)) 
\, = \,
Q \cdot \exp \left( (\alpha + \beta) \bt_\textsc{m} \right)
  \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  g(\x^n; \w_j)),
\end{equation}
%
\begin{equation}
\label{eq:grad_mc_equal}
\begin{aligned}
&P \sum_{m=1}^N u(\x^m, \y^m) \cdot \llb y_k^m = 1 \rrb \cdot \exp(-\alpha g(\x^m; \w_k)) \frac{\partial \, g(\x^m; \w_k)}{\partial \, \w_k} \\
&= \, Q \cdot \exp \left( (\alpha + \beta) \bt_\textsc{m} \right)
   \sum_{n=1}^N v(\x^n, \y^n) \cdot \llb y_k^n = 0 \rrb \cdot \exp( \beta  g(\x^n; \w_k)) \frac{\partial \, g(\x^n; \w_k)}{\partial \, \w_k}, 
   \quad \forall k \in \{1,\dots,K\}.
\end{aligned}
\end{equation}

By Eq.~(\ref{eq:bias_ml}), the right hand side of (\ref{eq:skew_ml_equal}) is
\begin{equation*}
\begin{aligned}
&  Q \cdot \exp \left( (\alpha + \beta) \bt_\textsc{m} \right) \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  g(\x^n; \w_j)) \Big|_{\W = \W_\textsc{mr}} \\
&= Q  
   \frac{P \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m = 1} \exp(-\alpha g(\x^m; \w_i))}
        {Q \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  g(\x^n; \w_j)) \hspace{1em}} 
   \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  g(\x^n; \w_j)) \Big|_{\W = \W_\textsc{mr}} \\
&= P \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m = 1} \exp(-\alpha g(\x^m; \w_i)) \Big|_{\W = \W_\textsc{mr}}.
\end{aligned}
\end{equation*}
thus Eq.~(\ref{eq:skew_ml_equal}) holds.

Moreover, by using Eq.~(\ref{eq:bias_ml}) and then (\ref{eq:grad_mr_eq0}), the right hand side of (\ref{eq:grad_mc_equal}) is
\begin{equation*}
\begin{aligned}
&Q \cdot \exp \left( (\alpha + \beta) \bt_\textsc{m} \right)
   \sum_{n=1}^N v(\x^n, \y^n) \cdot \llb y_k^n = 0 \rrb \cdot \exp(\beta g(\x^n; \w_k)) 
   \frac{\partial \, g(\x^n; \w_k)}{\partial \, \w_k} \Bigg|_{\W = \W_\textsc{mr}} \\
&= Q 
   \frac{P \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m = 1} \exp(-\alpha g(\x^m; \w_i))}
        {Q \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  g(\x^n; \w_j)) \hspace{1em}} 
   \sum_{n=1}^N v(\x^n, \y^n) \cdot \llb y_k^n = 0 \rrb \cdot \exp( \beta  g(\x^n; \w_k)) 
   \frac{\partial \, g(\x^n; \w_k)}{\partial \, \w_k} \Bigg|_{\W = \W_\textsc{mr}} \\
&= P \left( \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  g(\x^n; \w_j)) \right)^{-1} 
     \left( \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m = 1} \exp(-\alpha g(\x^m; \w_i)) \right) \\
&    \hspace{2.1em}
     \left( \sum_{n=1}^N v(\x^n, \y^n) \cdot \llb y_k^n = 0 \rrb \cdot \exp( \beta  g(\x^n; \w_k)) 
     \frac{\partial \, g(\x^n; \w_k)}{\partial \, \w_k} \right) 
     \Bigg|_{\W = \W_\textsc{mr}} \\
&= P \left( \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  g(\x^n; \w_j)) \right)^{-1} 
     \left( \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  g(\x^n; \w_j)) \right) \\
&    \hspace{2.1em}
     \left( \sum_{m=1}^N \llb y_k^m=1 \rrb \cdot u(\x^m, \y^m) \cdot \exp(-\alpha g(\x^m; \w_k)) 
     \frac{\partial \, g(\x^m; \w_k)}{\partial \, \w_k} \right)
     \Bigg|_{\W = \W_\textsc{mr}} \\
&= P \sum_{m=1}^N \llb y_k^m=1 \rrb \cdot u(\x^m, \y^m) \cdot \exp(-\alpha g(\x^m; \w_k)) 
     \frac{\partial \, g(\x^m; \w_k)}{\partial \, \w_k} 
     \Bigg|_{\W = \W_\textsc{mr}}, \quad \forall k \in \{1,\dots,K\},
\end{aligned}
\end{equation*}
thus Eq.~(\ref{eq:grad_mc_equal}) holds.

Now by Eq.~(\ref{eq:skew_ml_equal}) and note that $\alpha \gamma = \beta \delta$, we have
\begin{equation}
\label{eq:grad_mc_bias}
\begin{aligned}
&  \frac{\partial \, \RCal_\textsc{mc}(\W, b)} {\partial \, b} 
   \Bigg|_{\W = \W_\textsc{mr}, \, b = \bt_\textsc{m}} \\
&= P \gamma \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m = 1} \exp(-\alpha (g(\x^m; \w_i) + b)) \cdot (-\alpha) \\
&  \hspace{2em} + \,
   Q \delta \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  (g(\x^n; \w_j) + b)) \cdot \beta 
   \Big|_{\W = \W_\textsc{mr}, \, b = \bt_\textsc{m}} \\
&= -\alpha \gamma \cdot \exp \left( -\alpha \bt_\textsc{m} \right) \left[
   P \sum_{m=1}^N u(\x^m, \y^m) \sum_{i: y_i^m = 1} \exp(-\alpha g(\x^m; \w_i)) \right. \\
&  \hspace{10em} - \left.
   Q \cdot \exp \left( (\alpha + \beta) \bt_\textsc{m} \right)
     \sum_{n=1}^N v(\x^n, \y^n) \sum_{j: y_j^n = 0} \exp( \beta  g(\x^n; \w_j)) \right] 
   \Bigg|_{\W = \W_\textsc{mr}} \\
&= -\alpha \gamma \cdot \exp \left( -\alpha \bt_\textsc{m} \right) \cdot 0 \\
&= 0.
\end{aligned}
\end{equation}

Further, by Eq.~(\ref{eq:grad_mc_equal}) and note that $\alpha \gamma = \beta \delta$, for all $k \in \{1,\dots,K\}$ we have
\begin{equation}
\label{eq:grad_mc_w}
\begin{aligned}
&  \frac{\partial \, \RCal_\textsc{mc}(\W, b)} {\partial \, \w_k} 
   \Bigg|_{\W = \W_\textsc{mr}, \, b = \bt_\textsc{m}} \\
&= P \gamma \sum_{m=1}^N u(\x^m, \y^m) \cdot \llb y_k^m = 1 \rrb \cdot \exp(-\alpha (g(\x^m; \w_k) + b)) \cdot (-\alpha) \cdot 
     \frac{\partial \, g(\x^m; \w_k)}{\partial \, \w_k} \\
&  \hspace{2em} + \,
   Q \delta \sum_{n=1}^N v(\x^n, \y^n) \cdot \llb y_k^n = 0 \rrb \cdot \exp( \beta  (g(\x^n; \w_k) + b)) \cdot \beta \cdot 
     \frac{\partial \, g(\x^n; \w_k)}{\partial \, \w_k} 
     \Bigg|_{\W = \W_\textsc{mr}, \, b = \bt_\textsc{m}} \\
&= -\alpha \gamma \cdot \exp \left( -\alpha \bt_\textsc{m} \right) \left[
   P \sum_{m=1}^N u(\x^m, \y^m) \cdot \llb y_k^m = 1 \rrb \cdot \exp(-\alpha g(\x^m; \w_k)) 
     \frac{\partial \, g(\x^m; \w_k)}{\partial \, \w_k} \right. \\
&  \hspace{10em} - \left.
   Q \cdot \exp \left( (\alpha + \beta) \bt_\textsc{m} \right)
   \sum_{n=1}^N v(\x^n, \y^n) \cdot \llb y_k^n = 0 \rrb \cdot \exp( \beta  g(\x^n; \w_k)) 
   \frac{\partial \, g(\x^n; \w_k)}{\partial \, \w_k} \right]
   \Bigg|_{\W = \W_\textsc{mr}} \\
&= -\alpha \gamma \cdot \exp \left( -\alpha \bt_\textsc{m} \right) \cdot 0 \\
&= 0.
\end{aligned}
\end{equation}

Finally, by Eq.~(\ref{eq:grad_mc_bias}) and (\ref{eq:grad_mc_w}), 
we know that $(\W_\textsc{mr}, \bt_\textsc{m})$ is a minimiser of $\RCal_\textsc{mc}(\W, b)$, 
\ie $(\W_\textsc{mr}, \bt_\textsc{m}) \in \argmin_{\W,b} \, \RCal_\textsc{mc}(\W, b)$.
\end{proof}



\subsection{Generalise the P-Norm push loss}

If we generalise the P-Norm push loss to the multi-label setting as:
\begin{equation}
\label{eq:pnorm-ml}
\begin{aligned}
\RCal_\textsc{mpn}(\W, b) 
&= \sum_{n=1}^N \frac{1}{K_-^n} \sum_{j:y_j^n=0} \left( \frac{1}{K_+^n} \sum_{i:y_i^n=1} \exp(-(f(\x^n; \w_i) - f(\x^n; \w_j))) \right)^p \\
&= \sum_{n=1}^N \left[
   \left( \frac{1}{K_+^n} \sum_{i:y_i^n=1} \exp(-f(\x^n; \w_i)) \right)^p 
   \left( \frac{1}{K_-^n} \sum_{j:y_j^n=0} \exp(p \cdot f(\x^n; \w_j)) \right) \right],
\end{aligned}
\end{equation}
then we have the follow theorem:
\begin{theorem}
\label{theorem:pnorm-ml-ub}
If $p$ is a positive integer, \ie $p \in \Z_+$, then 
\begin{equation}
\label{eq:pnorm-ml-ub}
\RCal_\textsc{mpn}(\W, b) \le 
\left( \sum_{m=1}^N \frac{1}{K_+^m} \sum_{i:y_i^m=1} \exp(-f(\x^m; \w_i)) \right)^p 
\left( \sum_{n=1}^N \frac{1}{K_-^n} \sum_{j:y_j^n=0} \exp(p \cdot f(\x^n; \w_j)) \right).
\end{equation}
\end{theorem}


\begin{proof}
We prove by mathematical induction.

First, Inequality~(\ref{eq:pnorm-ml-ub}) is trivially satisfied when $N=1$.
Now suppose (\ref{eq:pnorm-ml-ub}) is satisfied when $N=M$, \ie
\begin{equation}
\label{eq:induction-step}
\begin{aligned}
& \sum_{n=1}^M \left[
  \left( \frac{1}{K_+^n} \sum_{i:y_i^n=1} \exp(-f(\x^n; \w_i)) \right)^p 
  \left( \frac{1}{K_-^n} \sum_{j:y_j^n=0} \exp(p \cdot f(\x^n; \w_j)) \right) \right] \\
&\le 
 \left( \sum_{m=1}^M \frac{1}{K_+^m} \sum_{i:y_i^m=1} \exp(-f(\x^m; \w_i)) \right)^p 
 \left( \sum_{n=1}^M \frac{1}{K_-^n} \sum_{j:y_j^n=0} \exp(p \cdot f(\x^n; \w_j)) \right).
\end{aligned}
\end{equation}
When $N=M+1$, by Inequality~(\ref{eq:induction-step}) and noting that\footnote{This can be easily proved by mathematical induction.} 
$(c + d)^p \ge c^p + d^p, \, p \in \Z_+, \, c,d \in \R_+$,
we have
\begin{equation*}
\begin{aligned}
&  \left( \sum_{m=1}^{M+1} \frac{1}{K_+^m} \sum_{i:y_i^m=1} \exp(-f(\x^m; \w_i)) \right)^p 
   \left( \sum_{n=1}^{M+1} \frac{1}{K_-^n} \sum_{j:y_j^n=0} \exp(p \cdot f(\x^n; \w_j)) \right) \\
=& \left( \sum_{m=1}^M \frac{1}{K_+^m} \sum_{i:y_i^m=1} \exp(-f(\x^m; \w_i)) + 
   \frac{1}{K_+^{M+1}} \sum_{i:y_i^{M+1}=1} \exp(-f(\x^{M+1}; \w_i)) \right)^p \\
&  \left( \sum_{n=1}^M \frac{1}{K_-^n} \sum_{j:y_j^n=0} \exp(p \cdot f(\x^n; \w_j)) +
   \frac{1}{K_-^{M+1}} \sum_{j:y_j^{M+1}=0} \exp(p \cdot f(\x^{M+1}; \w_j)) \right) \\
\ge&
   \left[ \left( \sum_{m=1}^M \frac{1}{K_+^m} \sum_{i:y_i^m=1} \exp(-f(\x^m; \w_i)) \right)^p + 
   \left( \frac{1}{K_+^{M+1}} \sum_{i:y_i^{M+1}=1} \exp(-f(\x^{M+1}; \w_i)) \right)^p \right] \\
&  \left( \sum_{n=1}^M \frac{1}{K_-^n} \sum_{j:y_j^n=0} \exp(p \cdot f(\x^n; \w_j)) +
   \frac{1}{K_-^{M+1}} \sum_{j:y_j^{M+1}=0} \exp(p \cdot f(\x^{M+1}; \w_j)) \right) \\
\ge& 
   \left( \sum_{m=1}^M \frac{1}{K_+^m} \sum_{i:y_i^m=1} \exp(-f(\x^m; \w_i)) \right)^p
   \left( \sum_{n=1}^M \frac{1}{K_-^n} \sum_{j:y_j^n=0} \exp(p \cdot f(\x^n; \w_j)) \right) \\
& +\left( \frac{1}{K_+^{M+1}} \sum_{i:y_i^{M+1}=1} \exp(-f(\x^{M+1}; \w_i)) \right)^p
   \left( \frac{1}{K_-^{M+1}} \sum_{j:y_j^{M+1}=0} \exp(p \cdot f(\x^{M+1}; \w_j)) \right) \\
\ge& 
   \sum_{n=1}^M \left[ \left( \frac{1}{K_+^n} \sum_{i:y_i^n=1} \exp(-f(\x^n; \w_i)) \right)^p 
   \left( \frac{1}{K_-^n} \sum_{j:y_j^n=0} \exp(p \cdot f(\x^n; \w_j)) \right) \right] \\
& +\left( \frac{1}{K_+^{M+1}} \sum_{i:y_i^{M+1}=1} \exp(-f(\x^{M+1}; \w_i)) \right)^p
   \left( \frac{1}{K_-^{M+1}} \sum_{j:y_j^{M+1}=0} \exp(p \cdot f(\x^{M+1}; \w_j)) \right) \\
=& \sum_{n=1}^{M+1} \left( \frac{1}{K_+^n} \sum_{i:y_i^n=1} \exp(-f(\x^n; \w_i)) \right)^p 
   \left( \frac{1}{K_-^n} \sum_{j:y_j^n=0} \exp(p \cdot f(\x^n; \w_j)) \right).
\end{aligned}
\end{equation*}
Thus, by mathematical induction, Inequality~(\ref{eq:pnorm-ml-ub}) holds.
\end{proof}
