\subsection{Practical strategies}

We have so far described two closely related methods for music recommendation,
in this section, we provide details for solving the optimisation problems underlining
the two proposed methods.

First, we want to emphasise that although Theorem~\ref{th:rank2clf} showed that
$\RCal_\textsc{clf}$ and $\widetilde\RCal_\textsc{rank}$ share the same minimisers, 
but there is no regularisation to play,
if we add the regularisation term, $\RCal_\textsc{clf} + R(\V, \W, \mubm)$ and 
$\widetilde\RCal_\textsc{rank} + R(\V, \W, \mubm)$ do not share the same minimisers in general.

Second, $\widetilde\RCal_\textsc{rank}$ is only an approximation of $\RCal_\textsc{rank}$,
given the exponential surrogate and the Log-Sum-Exp approximation of the \emph{max} operator.
While the approximation can be arbitrarily precise for large enough $p$, 
there are numerical issues when $p$ is big, \ie numerical overflow, 
which leads to impractical methods.

To optimise the objective of multitask classification, note that 
\begin{equation}
\label{eq:clfobj}
\begin{aligned}
& R(\V, \W, \mu) + \RCal_\textsc{clf} \\
&= \frac{\lambda_1}{2U} \sum_{u=1}^U \bv_u^\top \bv_u + \frac{\lambda_2}{2N} \sum_{i=1}^N \w_i^\top \w_i + \frac{\lambda_3}{2} \mubm^\top \mubm \\
& \quad
   + \frac{1}{N} \sum_{i=1}^N \left( \frac{1}{M_+^i} \sum_{m: y_m^i = 1} e^{-f(u(i), i,m)} + \frac{1}{M_-^i p} \sum_{n: y_n^i = 0} e^{p f(u(i), i,n)} \right),
\end{aligned}
\end{equation}
which means we want to optimise an unconstrained convex and differentiable objective.
There is rich set of techniques (\eg gradient based, quasi-newton methods etc.) to achieve this, and here we use the well-known LBFGS method.
It is straightforward to compute the derivative of objective~(\ref{eq:clfobj}) with respect to its parameters $\V$, $\W$, $\mu$,
and we leave the detail in appendix.

To optimise the objective of multitask ranking, note that
\begin{equation}
\label{eq:rankobj}
\begin{aligned}
& R(\V, \W, \mu) + \RCal_\textsc{rank} \\
&= \frac{\lambda_1}{2U} \sum_{u=1}^U \bv_u^\top \bv_u + \frac{\lambda_2}{2N} \sum_{i=1}^N \w_i^\top \w_i + \frac{\lambda_3}{2} \mubm^\top \mubm \\
& \quad
   + \frac{1}{N} \sum_{i=1}^N \frac{1}{M_-^i} \sum_{n: y_n^i = 0} \ell \left( \min_{m: y_m^i = 0} f(u(i), i, m) - f_(u(i), i, n) \right).
\end{aligned}
\end{equation}

First, we need to choose a convex surrogate $\ell(\cdot)$ for the 0-1 loss,
we have shown that exponential surrogate $\ell(f, y) = e^{-fy}$ is a viable option,
and we also mentioned the exponential function is prone to numerical issues in practice (\eg numerical overflow/underflow).
Other options including the logistic loss $\ell(f, y) = \log(1 + e^{-fy}$,
the squared hinge loss $\ell(f, y) = \left( \max \{0, 1 - fy\} \right)^2$ (which is also known as truncated quadratic loss),
or simply hinge loss $\ell(f, y) = \max \{0, 1 - fy\}$ 
(although it is non-differential at point $fy = 1$, we will describe later how to deal with this issue.).

Second, we need to deal with the non-differential \emph{max} operator.
As described above, we prefer not to approximate it, 
which leaves us with techniques such as sub-gradient methods or 
reformulating it as a constrained optimisation problem, which is the method of choice in this paper.

\eat{
\begin{equation}
\label{eq:cvxopt}
\begin{aligned}
\min_{\V, \W, \mubm} \
& \frac{\lambda_1}{2U} \sum_{u=1}^U \bv_u^\top \bv_u + \frac{\lambda_2}{2N} \sum_{i=1}^N \w_i^\top \w_i + \frac{\lambda_3}{2} \mubm^\top \mubm \\
& + \frac{1}{N} \sum_{i=1}^N \frac{1}{M_+^i} 
    \exp \left( \max_{n: y_n^i = 0} (\bv_{u(i)} + \w_i + \mubm)^\top \x_n \right) \\
& \hspace{5em} \sum_{m: y_m^i = 1} \exp \left( - (\bv_{u(i)} + \w_i + \mubm)^\top \x_m \right).
\end{aligned}
\end{equation}
}


\subsubsection{Formulate a constrained optimisation problem}
If we introduce a slack variable $\xi_i \ge f(u(i), i, n)$
we can reformulate problem~\ref{eq:unconsopt} as a constrained optimisation problem:
\begin{equation}
\label{eq:cvxopt}
\begin{aligned}
\min_{\V, \W, \mubm, \xibm} \ & R(\V, \W, \mubm)
  + \frac{1}{N} \sum_{i=1}^N \frac{1}{M_-^i} \sum_{n: y_n^i = 0} \ell \left( \xi_i - f(u(i), i, n) \right) \\
s.t. \quad & 
\xi_i \le f(u(i), i, m), \\
& i \in \{1,\dots,N\}, \, m \in \{1,\dots,M\} \ \mathrm{and} \ y_m^i = 1.
\end{aligned}
\end{equation}

This is a convex optimisation problem with inequality constraints, 
and the number of constraints is 
$$
\sum_{i=1}^N \sum_{n=1}^M \llb y_m^i = 1 \rrb = MN - \sum_{i=1}^N \sum_{m=1}^M \llb y_n^i = 0 \rrb.
$$

Suppose we use the hinge loss $\ell(f, y) = \max \{ 0, \, 1 - fy \}$ as the convex surrogate of 0-1 loss, we have
\begin{equation*}
\ell \left( \xi_i - f(u(i), i, n) \right) = \max \left\{ 0, \, 1 - \xi_i + f(u(i), i, n) \right\}.
\end{equation*}

Inspired by the one-slack formulation of structured Support Vector Machine~\cite{joachims2009cutting}, 
we introduce another variable $\delta_{i,n}$ such that
\begin{equation*}
\begin{aligned}
\delta_{i,n} & \ge 0, \\
\delta_{i,n} & \ge 1 - \xi_i + f(u(i), i, n),
\end{aligned}
\end{equation*}
as a result,
\begin{equation*}
\ell \left( \xi_i - f(u(i), i, n) \right) = \delta_{i,n},
\end{equation*}
we further let
\begin{equation*}
\delta_i = \frac{1}{M_-^i} \sum_{n: y_n^i = 0} \delta_{i,n} = \frac{1}{M_-^i} \sum_{n: y_n^i = 0} \ell \left( \xi_i - f(u(i), i, n) \right),
\end{equation*}
then we can reformulate convex optimisation problem~\ref{eq:cvxopt} into a quadratic program,
\begin{equation}
\label{eq:qp}
\begin{aligned}
\min_{\V, \W, \mubm, \xibm, \deltabm} \ & R(\V, \W, \mubm) + \frac{1}{N} \sum_{i=1}^N \delta_i \\
s.t. \quad 
& \xi_i \le f(u(i), i, m), \\
& \delta_i \ge 1 - \xi_i + \frac{1}{M_-^i} \sum_{n: y_n^i = 0} f(u(i), i, n), \ \delta_i \ge 0 \\
& i \in \{1,\dots,N\}, \\
& m, n \in \{1,\dots,M\} \ \mathrm{and} \ \y_m^i = 1, \,  y_n^i = 0. 
\end{aligned}
\end{equation}

The number of variables in problem~\ref{eq:qp} is $(U + N + 1) D + 2N$ and the number of constraints is
$$
\sum_{i=1}^N \sum_{n=1}^M \llb y_m^i = 1 \rrb + N.
$$
%
this can be a large number (\eg about 271k even for the smaller dataset in our experiment (Section~\ref{sec:experiment})),
given that the number of songs in each playlist is very small compared with the total number of songs $M$ in music collection,
%asymptotically, it is of order $O(MN)$ which can be very big for a system with a large number of songs and playlists,
%and this is generally true for production systems deployed by music streaming service providers.


\subsubsection{Cutting-plane optimisation}
% use a piece of pseudo code to describe it?

To address this issue, we propose to use a cutting plane method which starts by using a small number of constraints,
optimise the above objective using an off-the-shelf QP solver,
then search new constraints using the following greedy strategy,
\begin{equation*}
\widetilde m_i = \argmin_{m: y_i^m = 1} f(u(i), i, m),
\end{equation*}
and add this constraint $\xi_i \le f(u(i), i, \widetilde m_i)$ to form a new QP with the same objective,
we then solve this new QP, and keep doing this until a solution that does not violate any constraints has been found.
