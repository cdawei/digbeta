\section{Generalise to the multi-label setting}
\label{sec:ml}

Given a multi-label dataset $\DCal = \{\x^n, \y^n\}_{n=1}^N$, where $\x^n \in \R^D$ is a feature vector for the $n$-th example,
and $\y^n \in \{0,1\}^K$ are $K$ labels for the $n$-th example.
We can generalise Lemma~\ref{lemma:basic}, Theorem~\ref{theorem:bc2br} and \ref{theorem:br2bc} from the binary setting to multi-label setting.

\subsection{Variant I}

Let function $f(\cdot; \cdot)$ be
$$
f(\x^n; \w) = g(\x^n; \w) + b, \ n \in \{1,\dots,N\}
$$
where $\w$ is the weight vector, $b$ is the bias parameter,
and function $g(\x^n; \w) \in \R^K$ is differentiable and bounded.

Suppose $\alpha, \beta, c, P, Q \in \R_+$ are \emph{finite} positive numbers, 
we define $\RCal_\textsc{mc}\pb{1}$ be the following risk:
\begin{equation}
\label{eq:mc1}
\RCal_\textsc{mc}\pb{1}(\w, b) 
= \frac{P}{\alpha} \sum_{(m,i): y_i^m = 1} \exp(-\alpha f_i(\x^m; \w)) \, + \,
  \frac{Q}{\beta}  \sum_{(n,j): y_j^n = 0} \exp( \beta  f_j(\x^n; \w)),
\end{equation}
and let $\RCal_\textsc{mr}\pb{1}$ be a risk defined as
\begin{equation}
\label{eq:mr1}
\RCal_\textsc{mr}\pb{1}(\w, b) 
= \left[ \sum_{(m,i): y_i^m = 1} \exp(-\alpha f_i(\x^m; \w)) \right]^\frac{c}{\alpha}  
  \left[ \sum_{(n,j): y_j^n = 0} \exp( \beta  f_j(\x^n; \w)) \right]^\frac{c}{\beta}.
\end{equation}

Note that 
\begin{equation}
\label{eq:mr_derive}
\begin{aligned}
\RCal_\textsc{mr}\pb{1}(\w, b) 
&= \left[ \sum_{(m,i): y_i^m = 1} \left( \sum_{(n,j): y_j^n = 0} 
   \exp(-\beta(f_i(\x^m; \w) - f_j(\x^n; \w) ) ) \right)^\frac{\alpha}{\beta} \right]^\frac{c}{\alpha} \\
&= \left[ \sum_{(n,j): y_j^n = 0} \left( \sum_{(m,i): y_i^m = 1} 
   \exp(-\alpha( f_i(\x^m; \w) - f_j(\x^n; \w) ) ) \right)^\frac{\beta}{\alpha} \right]^\frac{c}{\beta}.
\end{aligned}
\end{equation}
It is obvious that $\RCal_\textsc{mr}\pb{1}$ is independent of $b$ since $f_i(\x^m; \w) - f_j(\x^n; \w) = g_i(\x^m; \w) - g_j(\x^n; \w)$,
so we denote it as $\RCal_\textsc{mr}\pb{1}(\w)$.


\begin{theorem}
Let $(\w_\textsc{mc}\pb{1}, b_\textsc{mc}\pb{1}) \in \argmin_{\w, b} \RCal_\textsc{mc}\pb{1}(\w, b)$ (assuming minimisers exist),
then $\w_\textsc{mc}\pb{1} \in \argmin_{\w} \RCal_\textsc{mr}\pb{1}(\w)$.
\end{theorem}


\begin{proof}
Suppose $(\w_\textsc{mc}\pb{1}, b_\textsc{mc}\pb{1}) \in \argmin_{\w, b} \RCal_\textsc{mc}\pb{1}(\w, b)$, then
\begin{equation*}
\begin{aligned}
0 
&= \frac{\partial \, \RCal_\textsc{mc}\pb{1}(\w, b)} {\partial \, b} \Bigg|_{\w = \w_\textsc{mc}\pb{1}, \, b = b_\textsc{mc}\pb{1}} \\
&= \frac{P}{\alpha} \sum_{(m,i): y_i^m = 1} \exp(-\alpha f_i(\x^m; \w)) (-\alpha) +
   \frac{Q}{\beta } \sum_{(n,j): y_j^n = 1} \exp( \beta  f_j(\x^n; \w)) \beta
   \Big|_{\w = \w_\textsc{mc}\pb{1}, \, b = b_\textsc{mc}\pb{1}} \\
&= \frac{P}{\alpha} \sum_{(m,i): y_i^m = 1} \exp(-\alpha (g_i(\x^m; \w) + b)) (-\alpha) +
   \frac{Q}{\beta } \sum_{(n,j): y_j^n = 1} \exp( \beta  (g_j(\x^n; \w) + b)) \beta
   \Big|_{\w = \w_\textsc{mc}\pb{1}, \, b = b_\textsc{mc}\pb{1}},
\end{aligned}
\end{equation*}
so we have
\begin{equation}
\label{eq:skew_ml}
\sum_{(m,i): y_i^m = 1} \exp(-\alpha g_i(\x^m; \w_\textsc{mc}\pb{1})) 
= \frac{Q}{P} \exp((\alpha + \beta) b_\textsc{mc}\pb{1}) \sum_{(n,j): y_j^n = 1} \exp( \beta  g_j(\x^n; \w_\textsc{mc}\pb{1})).
\end{equation}

Further,
\begin{equation*}
\begin{aligned}
0 
&= \frac{\partial \, \RCal_\textsc{mc}\pb{1}(\w, b)} {\partial \, \w} \Bigg|_{\w = \w_\textsc{mc}\pb{1}, \, b = b_\textsc{mc}\pb{1}} \\
&= \frac{P}{\alpha} \sum_{(m,i): y_i^m = 1} \exp(-\alpha f_i(\x^m; \w)) (-\alpha) \frac{\partial \, g_i(\x^m; \w)}{\partial \, \w} +
   \frac{Q}{\beta } \sum_{(n,j): y_j^n = 0} \exp( \beta  f_j(\x^n; \w)) \beta     \frac{\partial \, g_j(\x^n; \w)}{\partial \, \w}
   \Bigg|_{\w = \w_\textsc{mc}\pb{1}, \, b = b_\textsc{mc}\pb{1}} \\
&= \frac{P}{\alpha} \sum_{(m,i): y_i^m = 1} \exp(-\alpha (g_i(\x^m; \w) + b)) (-\alpha) \frac{\partial \, g_i(\x^m; \w)}{\partial \, \w} +
   \frac{Q}{\beta } \sum_{(n,j): y_j^n = 0} \exp( \beta  (g_j(\x^n; \w) + b)) \beta     \frac{\partial \, g_j(\x^n; \w)}{\partial \, \w}
   \Bigg|_{\w = \w_\textsc{mc}\pb{1}, \, b = b_\textsc{mc}\pb{1}},
\end{aligned}
\end{equation*}
so we have
\begin{equation}
\label{eq:grad_mc_eq0}
\begin{aligned}
\sum_{(m,i): y_i^m = 1} \exp(-\alpha g_i(\x^m; \w_\textsc{mc}\pb{1})) \frac{\partial \, g_i(\x^m; \w_\textsc{mc}\pb{1})}{\partial \, \w} 
= \frac{Q}{P} \exp((\alpha + \beta) b_\textsc{mc}\pb{1}) 
  \sum_{(n,j): y_j^n = 0} \exp(\beta g_j(\x^n; \w_\textsc{mc}\pb{1})) \frac{\partial \, g_j(\x^n; \w_\textsc{mc}\pb{1})}{\partial \, \w}.
\end{aligned}
\end{equation}

Note that
\begin{equation}
\label{eq:grad_mr}
\begin{aligned}
\frac{\partial \, \RCal_\textsc{mr}\pb{1}(\w)} {\partial \, \w}
=& \left[ \sum_{(m,i): y_i^m = 1} \exp(-\alpha f_i(\x^m; \w)) \right]^{\frac{c}{\alpha} - 1} \frac{c}{\alpha} 
   \sum_{(m,i): y_i^m = 1} \exp(-\alpha f_i(\x^m; \w)) (-\alpha) \frac{\partial \, g_i(\x_m; \w)}{\partial \, \w}
   \left[ \sum_{(n,j): y_j^n = 0} \exp( \beta  f_j(\x^n; \w)) \right]^\frac{c}{\beta} \\
 & + \left[ \sum_{(m,i): y_i^m = 1} \exp(-\alpha f_i(\x^m; \w)) \right]^\frac{c}{\alpha}
     \left[ \sum_{(n,j): y_j^n = 0} \exp( \beta  f_j(\x^n; \w)) \right]^{\frac{c}{\beta} - 1} \frac{c}{\beta}
     \sum_{(n,j): y_j^n = 0} \exp( \beta  f_j(\x^n; \w)) \beta \frac{\partial \, g_j(\x_n; \w)}{\partial \, \w} \\
=& \left[ \sum_{(m,i): y_i^m = 1} \exp(-\alpha f_i(\x^m; \w)) \right]^{\frac{c}{\alpha} - 1}
   \left[ \sum_{(n,j): y_j^n = 0} \exp( \beta  f_j(\x^n; \w)) \right]^{\frac{c}{\beta} - 1} (-c) 
   \exp((\beta - \alpha) b) \\
 & \left[ 
   \sum_{(m,i): y_i^m = 1} \exp(-\alpha g_i(\x^m; \w)) \frac{\partial \, g_i(\x_m; \w)}{\partial \, \w}
   \sum_{(n,j): y_j^n = 0} \exp( \beta  g_j(\x^n; \w)) \right. \\
 & \left. - 
   \sum_{(m,i): y_i^m = 1} \exp(-\alpha g_i(\x^m; \w)) 
   \sum_{(n,j): y_j^n = 0} \exp( \beta  g_j(\x^n; \w)) \frac{\partial \, g_j(\x_n; \w)}{\partial \, \w} \right]
\end{aligned}
\end{equation}

By first using Eq.~(\ref{eq:skew_ml}) and then (\ref{eq:grad_mc_eq0}), we have
\begin{equation}
\label{eq:mr_equal}
\begin{aligned}
&  \sum_{(m,i): y_i^m = 1} \exp(-\alpha g_i(\x^m; \w)) 
   \sum_{(n,j): y_j^n = 0} \exp( \beta  g_j(\x^n; \w)) \frac{\partial \, g_j(\x_n; \w)}{\partial \, \w}
   \Bigg|_{\w = \w_\textsc{mc}\pb{1}} \\ 
&= \frac{Q}{P} \exp((\alpha + \beta) b_\textsc{mc}\pb{1}) 
   \sum_{(n,j): y_j^n = 1} \exp( \beta  g_j(\x^n; \w_\textsc{mc}\pb{1}))
   \sum_{(n,j): y_j^n = 0} \exp(\beta g_j(\x^n; \w_\textsc{mc}\pb{1})) \frac{\partial \, g_j(\x_n; \w_\textsc{mc}\pb{1})}{\partial \, \w} \\
&= \sum_{(m,i): y_i^m = 1} \exp(-\alpha g_i(\x^m; \w_\textsc{mc}\pb{1})) \frac{\partial \, g_i(\x^m; \w_\textsc{mc}\pb{1})}{\partial \, \w}
   \sum_{(n,j): y_j^n = 1} \exp( \beta  g_j(\x^n; \w_\textsc{mc}\pb{1})).
\end{aligned}
\end{equation}

Finally, by Eq.~(\ref{eq:grad_mr}) and (\ref{eq:mr_equal}), we have
$$
\frac{\partial \, \RCal_\textsc{mr}\pb{1}(\w)} {\partial \, \w} \Bigg|_{\w = \w_\textsc{mc}\pb{1}} = 0.
$$
This means $\w_\textsc{mc}\pb{1}$ is a minimiser of $\RCal_\textsc{mr}\pb{1}(\w)$.
\end{proof}


\begin{theorem}
\label{theorem:mr2mc}
If $\w_\textsc{mr}\pb{1} \in \argmin_\w \RCal_\textsc{mr}\pb{1}(\w)$ (assuming minimisers exist),
then 
$$
(\w_\textsc{mr}\pb{1}, b_\textsc{m}\pb{1}) \in \argmin_{\w,b} \, \RCal_\textsc{mc}\pb{1}(\w, b),
$$ 
where
$$
b_\textsc{m}\pb{1} = \frac{1}{\alpha + \beta} \ln 
      \frac{P \sum_{(m,i): y_i^m = 1} \exp(-\alpha g_i(\x^m; \w_\textsc{mr}\pb{1}))}
           {Q \sum_{(n,j): y_j^n = 0} \exp( \beta  g_j(\x^n; \w_\textsc{mr}\pb{1}))}.
$$
\end{theorem}

