\clearpage
\newpage

\section{Experiment}
\label{sec:experiment}

We first evaluate the proposed method on the task of tag recommendation from text data,
which was formulated as multi-label classification problem~\cite{katakis2008multilabel},
then on two music playlist recommendation tasks using two playlist dataset.


\subsection{Tag recommendation as multi-label classification}
\label{ssec:mlc}
\TODO section title: tag recommendation or standard multi-label classification?

We experiment on two dataset, \texttt{bibtex} and \texttt{bookmarks}~\cite{katakis2008multilabel}.
The performance are evaluated on classification metrics, \ie F$_1$ scores averaged over either examples or labels 
(which are also known as instance-F$_1$ and macro-F$_1$, respectively),
as well as ranking metric, \ie R-Precision (averaged over either examples or labels).

\paragraph{Baselines}
We compare our method with four baselines.
\begin{itemize}
\item BR~\cite{tsoumakas2006multi}: the binary relevance method which independently learns a logistic regression classifier for each label.
\item PRLR~\cite{lin2014multi}: a multi-label classifier with a regulariser which encourages sparse and low-rank predictions.
\item SPEN~\cite{belanger2016structured}: a structured prediction framework which employs a deep network to represent the energy function,
      and predictions are produced by minimising the energy.
\item DVN~\cite{gygli2017deep}: a structured prediction method which uses a deep value network to distill the knowledge of a given loss function,
      which is the F$_1$ score (averaged over examples) in this task.
\end{itemize}

\paragraph{Experimental design}
We implemented Logistic regression using scikit-learn~\cite{scikit-learn}.
The results of SPEN and DVN are reproduced using the coded released the authors,
and the results of PRLR are taken from \cite{lin2014multi}.

%$\RCal_\textsc{example}$ 
For the proposed method, we used a linear score function $f(\x) = \w_k^\top \x + b$ for the $k$-th label,
and the empirical risk was minimised with L2 regularisation using LBFGS in SciPy~\cite{scipy,lbfgsb},
and hyper-parameters were tuned using 5-fold cross validation.

\paragraph{Evaluation metrics}
We evaluate the performance of each method using F$_1$ and R-Precision~\cite{manning2008introIR},
averaged both over examples and labels\footnote{They are also known as the instance-F$_1$ and macro-F$_1$ respectively in the case of F$_1$.}


\paragraph{Results}
The results on test set are summarised in Table~\ref{tab:perf_mlc},
\begin{itemize}
\item $\RCal_\textsc{example}$ outperform the independent logistic regression baseline by a large margin.
\item $\RCal_\textsc{example}$ also achieves better performance than PRLR~\cite{lin2014multi} which regularisation specific to multi-label classification.
\item Finally, it is encouraging that our method performs better than (\cite{belanger2016structured}) and (\cite{gygli2017deep}),
both work learn complex non-linear functions using deep neural networks to achieve state-of-the-art performance, while our method uses a linear function.
\end{itemize}

\TODO
\begin{itemize}
\item evaluation metric: add R-Precision averaged over both examples and labels, remove AUC.
\item results of more variants: $\RCal_\textsc{label}$ and $\RCal_\textsc{both}$?
\end{itemize}

\begin{table}[!h]
\centering
\newcommand{\nan}{{\scriptsize N/A}}
\caption{Performance on multi-label dataset (F$_1$ and R-Precision $\times 10^2$)}
\label{tab:perf_mlc}
\small
\begin{tabular}{l|*{4}{r}|*{4}{r}}
\toprule
{} & \multicolumn{4}{c|}{\textbf{bibtex}} & \multicolumn{4}{c}{\textbf{bookmarks}} \\
{} & F$_{1\,\text{exp}}$ & F$_{1\,\text{lab}}$ & R-Prec$_{\,\text{exp}}$ & R-Prec$_{\,\text{lab}}$ 
   & F$_{1\,\text{exp}}$ & F$_{1\,\text{lab}}$ & R-Prec$_{\,\text{exp}}$ & R-Prec$_{\,\text{lab}}$ \\
\midrule
BR~\cite{tsoumakas2006multi}       &  $37.9$  &  $30.1$  &  $43.1$  &  $32.1$  &  $29.5$  &  $21.0$  &  $35.6$  &  $21.2$ \\
PRLR~\cite{lin2014multi}           &  $44.2$  &  $37.2$  &    \nan  &    \nan  &  $34.9$  &  $23.0$  &    \nan  &    \nan \\
SPEN~\cite{belanger2016structured} &  $41.3$  &  $33.7$  &  $45.6$  &  $34.4$  &  $35.5$  &  $24.1$  &  $39.6$  &  $24.9$ \\
DVN~\cite{gygli2017deep}           &  $44.7$  &  $32.4$  &  $50.3$  &  $37.7$  &  $37.2$  &  $23.7$  &  $42.2$  &  $26.3$ \\
\rowcolor[gray]{0.8}
MLR (Ours)                         &  $47.0$  &  $38.8$  &  $51.3$  &  $40.5$  &  $37.7$  &  $28.4$  &  $42.3$  &  $29.5$ \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[!h]
\centering
\caption{Performance on multi-label dataset}
\label{tab:perf_mlc}
%\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{2pt} % tweak the space between columns
%\begin{tabular}{l*{6}{c}}
\begin{tabular}{l|ccc|ccc}
\toprule
{} & \multicolumn{3}{c|}{\textbf{bibtex}} & \multicolumn{3}{c}{\textbf{bookmarks}} \\
{} &   F$_1$ Example & F$_1$ Label &    AUC &      F$_1$ Example & F$_1$ Label &    AUC \\
\midrule
Binary Relevance~\cite{}           &          $37.9$ &      $30.1$ & $85.3$ &             $29.5$ &      $21.0$ & $87.2$ \\
PRLR~\cite{lin2014multi}           &          $44.2$ &      $37.2$ &    N/A &             $34.9$ &      $23.0$ &    N/A \\
SPEN~\cite{belanger2016structured} &          $41.3$ &      $33.7$ & $92.6$ &             $35.5$ &      $24.1$ & $90.8$ \\
DVN~\cite{gygli2017deep}           &          $44.7$ &      $32.4$ & $86.7$ &             $37.2$ &      $23.7$ & $76.9$ \\
MLR (Ours)                         &          ${\bf 47.0}$ & ${\bf 38.8}$ & ${\bf 93.3}$ & ${\bf 37.7}$ & ${\bf 28.4}$ & ${\bf 91.8}$ \\
\bottomrule
\end{tabular}
%}
\end{table}


\subsection{Music playlist dataset}
We make use of publicly available playlist dataset: the AotM-2011~\cite{mcfee2012hypergraph} and 30Music~\cite{30music2015} playlist dataset. \\
%
{\bf AotM-2011 Dataset} is a collection of playlists shared by users\footnote{\url{http://www.artofthemix.org}} ranging from 1998 to 2011, 
songs in the dataset had been matched to those in the Million Song Dataset (MSD)~\cite{msd2011}.
We filtered out playlists with less than 5 songs, which results in roughly 84K playlists over 114K songs from 14K users. \\
%
{\bf 30Music Dataset} is a collection of listening events and playlists retrieved from Last.fm\footnote{\url{https://www.last.fm}}.
We utilise the playlists data by first intersecting with the MSD, leveraging the Last.fm dataset~\cite{lastfmdataset} 
which matched songs from Last.fm with those in MSD, then filtering out playlists with less than 5 songs, 
which results in roughly 17K playlists over 45K songs from 8K users.

We make use of the audio features of songs provided by MSD, 
and genre data from the Top-MAGD genre dataset~\cite{schindler2012facilitating} and tagtraum genre annotations for MSD~\cite{schreiber2015improving},
which results in 202 audio features and 15 one-hot encoding for genres.

%% details for compute song features.
%% - temporal audio features: use 5-number (percentiles) summary: min, max, median, Q1 and Q3.
%% - missing genre: imputed using the mean values of the genre


\subsection{Evaluation}
We evaluate performance using metrics that are commonly used for music and playlist recommendation 
tasks~\cite{schedl2017,hariri2012context,jannach2015beyond}:
\begin{itemize}
\item R-Precision~\cite{manning2008introIR}, which computes the ratio of correctly recommended songs in top-$n$ recommendation, 
      where $n$ is the number of songs held for a given playlist.
\item Hit-Rate@K~\cite{hariri2012context}, which computes the ratio of correctly recommended songs among top-$K$ recommendation, 
      where $K$ is a given number, \eg $10$, $100$. 
      This metric is also known as Precision@K in information retrieval literature~\cite{manning2008introIR}.
\end{itemize}
We average over all playlists in test set for each metric.


\subsection{New song recommendation}
\label{ssec:newsongrec}

\paragraph{Motivation:} 
We are interested in the task of recommending newly released songs to users,
in particular, to augment users' existing playlists with these songs,
which is a cold-start problem.

\paragraph{Machine learning task:}
We formulate the task of recommending newly released songs to augment existing playlists
as a multi-label classification problem, where we predict, for each song, 
whether it will be included in a given playlist.
This formulation is illustrated in Figure~\ref{fig:mlr},
where rows represent songs (from top to bottom, sorted by the release date in ascending order)
and columns represent playlists (no specific order).
Further, rows with white colour represent songs in training set, and rows with grey colour represent songs in test set.
If entry $(i, j)$ is \texttt{1} (or \texttt{0}), it means the $i$-th song is (or not) found in the $j$-th playlist,
otherwise, we do not know whether the $i$-th song is found in the $j$-th playlist (\ie entry $(i, j)$ is a question mark \texttt{?}).
As a remark, we do not care about the order of songs in a playlist.

\input{fig_mlr}

\paragraph{Solutions:}
{\it Formulas for each variant. $\RCal_\textsc{example}$ and $\RCal_\textsc{label}$ etc}


\paragraph{Experimental design:}
In both the AotM-2011 and 30Music dataset, we hold (a random) half of the 40\% latest released songs for test,
and other half as validation set, the remaining 60\% of songs are used for training.
As a remark, in each dataset, the set of users and playlists are the same for training/dev/test split,
these splits differ in the set of songs.

Table~\ref{tab:stats_newsongrec} summarises the statistics of the two dataset used for this task.

\begin{table}[!h]
\centering
\caption{Statistics of dataset for new song recommendation}
\label{tab:stats_newsongrec}
%\resizebox{\linewidth}{!}{
\small
\begin{tabular}{l|rrrr}
\toprule
Dataset & \#Users & \#Songs (train/dev/test) & \#Playlists & \#Song Features \\
\midrule
AotM-2011 & 14,182  & 68,657 / 22,885 / 22,886 & 84,710 & 217 \\
30Music   & 8,070   & 27,281 / 9,093 / 9,094   & 17,457 & 217 \\
\bottomrule
\end{tabular}
%}
\end{table}


\subsubsection{A few conclusions}

\paragraph{Which type of loss is most helpful?}
\begin{table}[!h]
\centering
\caption{Empirical results (R-Precision $\times 10^3$)}
%\resizebox{\linewidth}{!}{
\small
\begin{tabular}{l|cccc}
\toprule
{}            & $\RCal_\textsc{example}$ & $\RCal_\textsc{label}$ & $\RCal_\textsc{both}$ & BR \\
\midrule
AotM-2011     &  &  &  & 0.92 \\
30Music       &  &  &  & 7.24 \\
%AotM-2011     & 0.64792  & 0.67782 & 0.59602  & 0.62226 \\
%30Music       & 0.6768   & 0.70917 & 0.70914  & 0.6654 \\
\bottomrule
\end{tabular}
%}
\end{table}

\paragraph{Experimental design:}
C: 1, 1, 1, p: 1, no multi-task regularisation.

\paragraph{Is multi-task regularisation helpful?}

\begin{table}[!h]
\centering
\caption{Empirical results}
%\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccc}
\toprule
{}            & Multi-task Reg. + $\RCal_\textsc{example}$ & Multi-task reg. + $\RCal_\textsc{label}$ \\
\midrule
%AotM-2011     & 0.65778     & 0.6884618 \\
%30Music       & 0.68179     & 0.7149 \\
%30Music       & 0.549567    & 0.557156 \\
\bottomrule
\end{tabular}
%}
\end{table}



\subsection{Playlist augmentation}
\label{ssec:pla}

\paragraph{Motivation:}
We are interested in augmenting user created playlists with songs from a music library,
in particular, for a partial playlist with the first $K$ tracks, 
we would like to add more songs from an existing collection of songs.
The ACM RecSys Challenge 2018 will use a similar setting for continuing music playlists 
automatically\footnote{\url{https://recsys-challenge.spotify.com}}.

The difference between this task from the task in Section~\ref{ssec:newsongrec} is that,
it is possible to choose any songs from the entire collection of songs, which is not a cold-start problem,
while the task described in Section~\ref{ssec:newsongrec} restricts that 
we choose songs from a subset of the entire collection (the newly released songs), 
which is a cold-start problem.

\paragraph{Machine learning task:}
We formulate the task of augmenting existing playlist as a multi-label classification problem,
that is, for each song that is not in the given playlist, 
we predict whether it will be added to augment the given playlist.
This formulation is illustrated in Figure~\ref{fig:pla},
where rows represent songs (no specific order) and columns represent playlists (no specific order).
Further, columns with white colour represent playlists in training set, 
and columns with grey colour represent playlists that should be augmented (\ie test set).
Similar to the formulation in Section~\ref{ssec:newsongrec}, if entry $(i, j)$ is \texttt{1} (or \texttt{0}), 
it means the $i$-th song is (or not) found in the $j$-th playlist, 
and a question mark \texttt{?} means that we do not know whether the $i$-th song is found in the $j$-th playlist.
As a remark, columns represent playlists in test set contain only \texttt{1} and \texttt{?} entries.

\input{fig_pla}

\paragraph{Experimental design:}
We use the AotM-2011~\cite{mcfee2012hypergraph} and 30Music~\cite{30music2015} playlist dataset,
In each dataset, we create the test set such that it contains 20\% of each user's playlists (chosen uniformly at random),
if the user has 5 or more playlists. Further, for the $j$-th playlist in test set, we hold all tracks except the first $K$ ones,
where $K$ is chosen from $[1, L_j-1]$ uniformly at random, and $L_j$ is the number of tracks in the $j$-th playlist.
The validation set is constructed the same as the test test.
All remaining playlists are used for training.
As a remark, we observed the entire collection of songs during training, 
and all users in test set have playlists in training set.

Besides making use of the audio features and genres of songs,
we also use the popularity of a given song as a feature, 
which is defined as the number of occurrence of the song (\ie a listening event of the particular song) in all playlists,
including the partial playlists in test set.
Table~\ref{tab:stats_pla} summarises the statistics of the two dataset used for this task.

\begin{table}[!h]
\centering
\caption{Statistics of dataset for playlist augmentation}
\label{tab:stats_pla}
%\resizebox{\linewidth}{!}{
\small
\begin{tabular}{l|rrrrr}
\toprule
Dataset & \#Users & \#Songs & \#Playlists (train/dev/test)  & \#Listen events held (dev/test) & \#Song Features \\
\midrule
AotM-2011 & 14,182 & 114,428 & 60,260 / 12,225 / 12,225 & 62,396 / 62,844 & 218 \\
30Music   & 8,070  & 45,468  & 15,591 / 933 / 933       & 8,459 / 8,487   & 218 \\
\bottomrule
\end{tabular}
%}
\end{table}

\paragraph{Solutions:}
{\it different type of losses}


\subsubsection{A few conclusions}

\paragraph{Which type of loss is most helpful?}

\TODO
{\it Formulas for each variant. $\RCal_\textsc{example}$ and $\RCal_\textsc{label}$ etc}

$\RCal_\textsc{example}$: weighting by the number of positive/negative labels for each example.
\ie we perform a classification/bipartite ranking task on each multilabel example 
which forms a dataset of examples with binary labels: $\{(\x_n, y_k\}_{k=1}^K$ for the $n$-th multilabel example.

\begin{equation*}
%\resizebox{\linewidth}{!}{$
\RCal_\textsc{example} 
= \displaystyle \sum_s 
  \frac{1}{K_+^s} \sum_{s \in pl} e^{-(\w_{pl}^\top \phi(s) + b)} +
  \frac{1}{K_-^s} \sum_{s \notin pl} \frac{1}{p} e^{p \w_{pl}^\top \phi(s)}.
%$}
\end{equation*}
where normalising factor $K_+^s$ is the number of playlists that include song $s$,
and $K_-^s$ is the number of playlists that do not include song $s$.


$\RCal_\textsc{label}$: weighting by the number of positive/negative examples for each label.
\ie we perform a classification/bipartite ranking task on each label which forms a binary dataset:
$\{\x_n, y_k\}_{n=1}^N$ for the $k$-th label.

\begin{equation*}
%\resizebox{\linewidth}{!}{$
\RCal_\textsc{label} 
= \displaystyle \sum_{pl}
  \frac{1}{N_+^{pl}} \sum_{s \in pl} e^{-(\w_{pl}^\top \phi(s) + b)} +
  \frac{1}{N_-^{pl}} \sum_{s \notin pl} \frac{1}{p} e^{p \w_{pl}^\top \phi(s)}.
%$}
\end{equation*}
where normalising factor $N_+^{pl}$ is the number of songs in playlist $pl$,
and $N_-^{pl}$ is the number of songs in a music library that playlist $pl$ does not include.


$\RCal_\textsc{both}$: the summation of both: $\RCal_\textsc{example} + C \RCal_\textsc{label}$ 
where $C$ is a trade-off parameter.

The binary relevance baseline is learning a logistic regression for each playlist independently.


\begin{table}[!h]
\centering
\caption{Empirical results (R-Precision $\times 10^3$)}
%\resizebox{\linewidth}{!}{
\small
\begin{tabular}{l|ccccc}
\toprule
{}            & $\RCal_\textsc{example}$ & $\RCal_\textsc{label}$ & $\RCal_\textsc{both}$ & BR & \textsc{PopRank} \\
\midrule
AotM-2011     &  &  &  & 2.05 & 3.67 \\
30Music       &  &  &  & 6.88 & 4.30 \\
%AotM-2011     &  &  &  & 2.69 & 3.69 \\
%30Music       & 5.53 & 9.02 &  & 9.44 & 4.49 \\
%AotM-2011     & 0.68459 & 0.747755 & 0.7429  & 0.6924 & 0.80199 \\
%30Music       & 0.7168  & 0.76867  & 0.76917 & 0.7225 & 0.7165 \\
%AotM-2011     & 0.6827396 & 0.743770 & 0.7385298 & 0.6924 & 0.80199 \\
%30Music       & 0.56766179 & 0.6350941 & 0.63555 & 0.575567 & 0.80558 \\
\bottomrule
\end{tabular}
%}
\end{table}

\paragraph{Experimental design:}

\paragraph{Is multi-task regularisation helpful?}

Multi-task regularisation: we regularise the difference of playlist parameters 
such that $\|\w_j - \w_k\|_2$ is small if playlist $j$ and $k$ belong to the same user.

\begin{equation*}
\RCal_\textsc{reg} = \frac{1}{\sum_u N_u (N_u - 1)} \sum_u \sum_{j, k \in u} (\w_j - \w_k)^\top (\w_j - \w_k)
\end{equation*}
where $N_u$ is the number of playlist user $u$ has.

\begin{table}[!h]
\centering
\caption{Empirical results}
%\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccc}
\toprule
{}            & Multi-task Reg. + $\RCal_\textsc{example}$ & Multi-task reg. + $\RCal_\textsc{label}$ \\
\midrule
%AotM-2011     & 0.69167 & 0.7819 \\
%30Music       & 0.7177  & 0.7840 \\
%AotM-2011     & 0.6882099 & 0.7615590 \\
%30Music       & 0.581426 & 0.6597  \\
%30Music       & 0.574175 & 0.667804  \\
%AUC           & 0.66583  & 0.68517   \\
\bottomrule
\end{tabular}
%}
\end{table}
