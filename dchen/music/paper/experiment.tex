\clearpage
\newpage

\section{Experiment}
\label{sec:experiment}

We first evaluate the proposed method on the task of tag recommendation from text data,
which was formulated as multi-label classification problem~\cite{katakis2008multilabel}.

\subsection{Tag recommendation as multi-label classification}

We experiment on two dataset, \texttt{bibtex} and \texttt{bookmarks}~\cite{katakis2008multilabel}.
The performance are evaluated on classification metrics, \ie F$_1$ scores averaged over either examples or labels 
(which are also known as instance-F$_1$ and macro-F$_1$, respectively),
as well as ranking metric, \ie R-Precision (averaged over either examples or labels).

\paragraph{Baselines}
We compare our method with four baselines.
\begin{itemize}
\item Logistic regression: independently learn a logistic regression classifier for each label, (a.k.a binary relevance).
\item PRLR~\cite{lin2014multi}: a multi-label classifier with a regulariser which encourages sparse and low-rank predictions.
\item SPEN~\cite{belanger2016structured}: a structured prediction framework which employs a deep network to represent the energy function,
      and predictions are produced by minimising the energy.
\item DVN~\cite{gygli2017deep}: a structured prediction method which uses a deep value network to distill the knowledge of a given loss function,
      which is the F$_1$ score (averaged over examples) in this task.
\end{itemize}

We implemented Logistic regression using scikit-learn~\cite{}.
The results of SPEN and DVN are reproduced using the coded released the authors,
and the results of PRLR are taken from \cite{lin2014multi}.

%$\RCal_\textsc{example}$ 
For the proposed method, we used a linear score function $f(\x) = \w_k^\top \x + b$ for the $k$-th label,
and the empirical risk was minimised with L2 regularisation using LBFGS provided by Scipy~\cite{},
and hyper-parameters were tuned using 5-fold cross validation.

\paragraph{Result analysis}
The results on test set are summarised in Table~\ref{tab:perf_mlc},
\begin{itemize}
\item $\RCal_\textsc{example}$ outperform the independent logistic regression baseline by a large margin.
\item $\RCal_\textsc{example}$ also achieves better performance than PRLR~\cite{lin2014multi} which regularisation specific to multi-label classification.
\item Finally, it is encouraging that our method performs better than (\cite{belanger2016structured}) and (\cite{gygli2017deep}),
both work learn complex non-linear functions using deep neural networks to achieve state-of-the-art performance, while our method uses a linear function.
\end{itemize}

\TODO
\begin{itemize}
\item evaluation metric: add R-Precision averaged over both examples and labels, remove AUC.
\item results of more variants: $\RCal_\textsc{label}$ and $\RCal_\textsc{both}$?
\end{itemize}


\begin{table}[!h]
\centering
\caption{Performance on multi-label dataset}
\label{tab:perf_mlc}
%\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{2pt} % tweak the space between columns
%\begin{tabular}{l*{6}{c}}
\begin{tabular}{l|ccc|ccc}
\toprule
{} & \multicolumn{3}{c|}{\textbf{bibtex}} & \multicolumn{3}{c}{\textbf{bookmarks}} \\
{} &   F$_1$ Example & F$_1$ Label &    AUC &      F$_1$ Example & F$_1$ Label &    AUC \\
\midrule
Binary Relevance~\cite{}           &          $37.9$ &      $30.1$ & $85.3$ &             $29.5$ &      $21.0$ & $87.2$ \\
PRLR~\cite{lin2014multi}           &          $44.2$ &      $37.2$ &    N/A &             $34.9$ &      $23.0$ &    N/A \\
SPEN~\cite{belanger2016structured} &          $41.3$ &      $33.7$ & $92.6$ &             $35.5$ &      $24.1$ & $90.8$ \\
DVN~\cite{gygli2017deep}           &          $44.7$ &      $32.4$ & $86.7$ &             $37.2$ &      $23.7$ & $76.9$ \\
MLR (Ours)                         &          ${\bf 47.0}$ & ${\bf 38.8}$ & ${\bf 93.3}$ & ${\bf 37.7}$ & ${\bf 28.4}$ & ${\bf 91.8}$ \\
\bottomrule
\end{tabular}
%}
\end{table}



\subsection{New song recommendation}
\label{ssec:newsongrec}

\paragraph{Task:} 
We are interested in the task of recommending newly released songs to users,
in particular, to augment users' existing playlists with these songs,
which is a cold-start problem.

\paragraph{Problem formulation:}
We formulate the task of recommending newly released songs to augment existing playlists
as a multi-label classification problem, where we predict, for each song, 
whether it will be included in a given playlist.
This formulation is illustrated in Figure~\ref{fig:mlr},
where rows represent songs (from top to bottom, sorted by the release date in ascending order)
and columns represent playlists (no specific order).
Further, rows with white colour represent songs in training set, and rows with grey colour represent songs in test set.
If entry $(i, j)$ is \texttt{1} (or \texttt{0}), it means the $i$-th song is (or not) found in the $j$-th playlist,
otherwise, we do not know whether the $i$-th song is found in the $j$-th playlist (\ie entry $(i, j)$ is a question mark \texttt{?}).


\TODO
{\it Formulas for each variant.
The objective is the same as $\RCal_\textsc{row}$ and $\RCal_\textsc{col}$ except that,
for the playlists that we choose to hold the later half, all we observed is the first half, 
all other songs for these playlists are unobserved (they can be positive/negative examples),
this is different from the case that we explicitly observed that songs are not in playlists (they are negative examples).
}


\input{fig_mlr}

\paragraph{Dataset:}
We make use of publicly available playlist dataset: the AotM-2011~\cite{mcfee2012hypergraph} and 30Music~\cite{30music2015} playlist dataset. \\
%
{\bf AotM-2011 Dataset} is a collection of playlists shared by users\footnote{\url{http://www.artofthemix.org}} ranging from 1998 to 2011, 
songs in the dataset had been matched to those in the Million Song Dataset (MSD)~\cite{msd2011}.
We filtered out playlists with less than 5 songs, which results in roughly 84K playlists over 114K songs from 14K users. \\
%
{\bf 30Music Dataset} is a collection of listening events and playlists retrieved from Last.fm\footnote{\url{https://www.last.fm}}.
We utilise the playlists data by first intersecting with the MSD, leveraging the Last.fm dataset~\cite{lastfmdataset} 
which matched songs from Last.fm with those in MSD, then filtering out playlists with less than 5 songs, 
which results in roughly 17K playlists over 45K songs from 8K users.

We make use of the audio features of songs provided by MSD, 
and genre data from the Top-MAGD genre dataset~\cite{schindler2012facilitating} and tagtraum genre annotations for MSD~\cite{schreiber2015improving},
which results in 202 audio features and 15 one-hot encoding for genres.

%% details for compute song features.
%% - temporal audio features: use 5-number (percentiles) summary: min, max, median, Q1 and Q3.
%% - missing genre: imputed using the mean values of the genre

Table~\ref{tab:stats_newsongrec} summarises the statistics of the two dataset used for this task.

\begin{table}[!h]
\centering
\caption{Statistics of dataset for new song recommendation}
\label{tab:stats_newsongrec}
%\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccc}
\toprule
Dataset & \#Users & \#Songs (train/dev/test) & \#Playlists & \#Song Features \\
\midrule
AotM-2011 & 14,182  & 68,657 / 22,885 / 22,886 & 84,710 & 217 \\
30Music   & 8,070   & 27,281 / 9,093 / 9,094   & 17,457 & 217 \\
\bottomrule
\end{tabular}
%}
\end{table}


\paragraph{Experimental design:}
In each dataset, we hold (a random) half of the 40\% latest released songs for test,
and other half as validation set, the remaining 60\% of songs are used for training.
All playlists in the dataset are used for this task.


\subsubsection{A few conclusions}

\paragraph{Which type of loss is most helpful?}
\begin{table}[!h]
\centering
\caption{Empirical results (AUC)}
%\resizebox{\linewidth}{!}{
\begin{tabular}{l|cccc}
\toprule
{}            & $\RCal_\textsc{example}$ & $\RCal_\textsc{label}$ & $\RCal_\textsc{both}$ & Independent L.R. \\
\midrule
AotM-2011     & 0.64792  & 0.67782 & 0.59602  & 0.62226 \\
30Music       & 0.6768   & 0.70917 & 0.70914  & 0.6654 \\
%30Music       & 0.54413  & 0.55894 & 0.55864  & 0.53698 \\
\bottomrule
\end{tabular}
%}
\end{table}

\paragraph{Experimental design:}
C: 1, 1, 1, p: 1, no multi-task regularisation.

\paragraph{Is multi-task regularisation helpful?}

\begin{table}[!h]
\centering
\caption{Empirical results}
%\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccc}
\toprule
{}            & Multi-task Reg. + $\RCal_\textsc{example}$ & Multi-task reg. + $\RCal_\textsc{label}$ \\
\midrule
AotM-2011     & 0.65778     & 0.6884618 \\
30Music       & 0.68179     & 0.7149 \\
%30Music       & 0.549567    & 0.557156 \\
\bottomrule
\end{tabular}
%}
\end{table}



\subsection{Playlist augmentation}
\label{ssec:pla}

\paragraph{Task:}
We are interested in augmenting user created playlists with songs from a music library,
in particular, for a partial playlist, we would like to add more songs from an existing collection of songs.
The difference between this task from the task in Section~\ref{ssec:newsongrec} is that,
it is possible to choose any songs from the entire collection of songs, which is not a cold-start problem,
while the task described in Section~\ref{ssec:newsongrec} restricts that 
we choose songs from a subset of the entire collection (the newly released songs), 
which is a cold-start problem.

\paragraph{Problem formulation:}
We formulate the task of augmenting existing playlist as a multi-label classification problem,
that is, for each song that is not in the given playlist, 
we predict whether it will be added to augment the given playlist.
This formulation is illustrated in Figure~\ref{fig:pla},
where rows represent songs (no specific order) and columns represent playlists (no specific order).
Further, columns with white colour represent playlists in training set, 
and columns with grey colour represent playlists that should be augmented (\ie test set).
Similar to the formulation in Section~\ref{ssec:newsongrec}, if entry $(i, j)$ is \texttt{1} (or \texttt{0}), 
it means the $i$-th song is (or not) found in the $j$-th playlist, 
and a question mark \texttt{?} means that we do not know whether the $i$-th song is found in the $j$-th playlist.
As a remark, columns represent playlists in test set contain only \texttt{1} and \texttt{?} entries.

\input{fig_pla}

\paragraph{Dataset:}
We again use the AotM-2011~\cite{mcfee2012hypergraph} and 30Music~\cite{30music2015} playlist dataset,
the pre-process of the two dataset is the same as that in Section~\ref{ssec:newsongrec}.
Besides making use of the audio features and genres of songs,
we also use the popularity of a given song as a feature, which is defined as the number of occurrence in all playlists,
including the partial playlists in test set.

Table~\ref{tab:stats_pla} summarises the statistics of the two dataset used for this task.

\begin{table}[!h]
\centering
\caption{Statistics of dataset for playlist augmentation}
\label{tab:stats_pla}
%\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccc}
\toprule
Dataset & \#Users & \#Songs & \#Playlists (train/dev/test)  & \#Song Features \\
\midrule
AotM-2011 & 14,182 & 114,428 & 60,260 / 12,225 / 12,225 & 218 \\
30Music   & 8,070  & 45,468  & 15,591 / 933 / 933       & 218 \\
\bottomrule
\end{tabular}
%}
\end{table}

\paragraph{Experimental design:}
In each dataset, we create the test set such that it contains 20\% of each user's playlists (chosen uniformly at random),
if the user has 5 or more playlists, the validation set is constructed the same as the test test.
All remaining playlists are used for training.
As a remark, we observed all songs during training.


\subsubsection{A few conclusions}

\paragraph{Which type of loss is most helpful?}

{\bf Row-wise loss}: weighting by the number of positive/negative labels for each example.
\ie we perform a classification/bipartite ranking task on each multilabel example 
which forms a dataset of examples with binary labels: $\{(\x_n, y_k\}_{k=1}^K$ for the $n$-th multilabel example.

\begin{equation*}
%\resizebox{\linewidth}{!}{$
\RCal_\textsc{row} 
= \displaystyle \sum_s 
  \frac{1}{K_+^s} \sum_{s \in pl} e^{-(\w_{pl}^\top \phi(s) + b)} +
  \frac{1}{K_-^s} \sum_{s \notin pl} \frac{1}{p} e^{p \w_{pl}^\top \phi(s)}.
%$}
\end{equation*}
where normalising factor $K_+^s$ is the number of playlists that include song $s$,
and $K_-^s$ is the number of playlists that do not include song $s$.


{\bf Column-wise loss}: weighting by the number of positive/negative examples for each label.
\ie we perform a classification/bipartite ranking task on each label which forms a binary dataset:
$\{\x_n, y_k\}_{n=1}^N$ for the $k$-th label.

\begin{equation*}
%\resizebox{\linewidth}{!}{$
\RCal_\textsc{col} 
= \displaystyle \sum_{pl}
  \frac{1}{N_+^{pl}} \sum_{s \in pl} e^{-(\w_{pl}^\top \phi(s) + b)} +
  \frac{1}{N_-^{pl}} \sum_{s \notin pl} \frac{1}{p} e^{p \w_{pl}^\top \phi(s)}.
%$}
\end{equation*}
where normalising factor $N_+^{pl}$ is the number of songs in playlist $pl$,
and $N_-^{pl}$ is the number of songs in a music library that playlist $pl$ does not include.


{\bf Row-wise + column-wise loss}: the summation of both: $\RCal_\textsc{row} + C \RCal_\textsc{col}$ 
where $C$ is a trade-off parameter.

The binary relevance baseline is learning a logistic regression for each playlist independently.


\begin{table}[!h]
\centering
\caption{Empirical results}
%\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccccc}
\toprule
{}            & $\RCal_\textsc{example}$ & $\RCal_\textsc{label}$ & $\RCal_\textsc{both}$ & Independent L.R. & Pop-rank \\
\midrule
%AotM-2011     & 0.6827396 & 0.743770 & 0.7385298 & 0.6924 & 0.80199 \\
AotM-2011     & 0.68459 & 0.747755 & 0.7429  & 0.6924 & 0.80199 \\
30Music       & 0.7168  & 0.76867  & 0.76917 & 0.7225 & 0.7165 \\
%30Music       & 0.56766179 & 0.6350941 & 0.63555 & 0.575567 & 0.80558 \\
\bottomrule
\end{tabular}
%}
\end{table}

\paragraph{Experimental design:}
C: 1, 1, 1, p: 1, no multi-task regularisation

\paragraph{Is multi-task regularisation helpful?}

multi-task regularisation: we regularise the difference of playlist parameters 
such that $\|\w_j - \w_k\|_2$ is small if playlist $j$ and $k$ belong to the same user.

\begin{equation*}
\RCal_\textsc{reg} = \frac{1}{\sum_u N_u (N_u - 1)} \sum_u \sum_{j, k \in u} (\w_j - \w_k)^\top (\w_j - \w_k)
\end{equation*}
where $N_u$ is the number of playlist user $u$ has.

\begin{table}[!h]
\centering
\caption{Empirical results}
%\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccc}
\toprule
{}            & Multi-task Reg. + $\RCal_\textsc{example}$ & Multi-task reg. + $\RCal_\textsc{label}$ \\
\midrule
%AotM-2011     & 0.6882099 & 0.7615590 \\
AotM-2011     & 0.69167 & 0.7819 \\
30Music       & 0.7177  & 0.7840 \\
%30Music       & 0.581426 & 0.6597  \\
%30Music       & 0.574175 & 0.667804  \\
%AUC           & 0.66583  & 0.68517   \\
\bottomrule
\end{tabular}
%}
\end{table}



\TODO
measure performance by AUC and HitRate@K,
compare with baselines such as independent logistic regression (\ie binary relevance), popularity based recommendation,
and matrix factorisation.



\subsection{Discussion}

{\it the choice of playlist dataset?}
