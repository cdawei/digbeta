\clearpage
\newpage

\section{Experiment}
\label{sec:experiment}

We first evaluate the proposed method on the task of tag recommendation from text data,
which was formulated as multi-label classification problem~\cite{katakis2008multilabel}.

\subsection{Tag recommendation as multi-label classification}

We experiment on two dataset, \texttt{bibtex} and \texttt{bookmarks}~\cite{katakis2008multilabel}.
The performance are evaluated on classification metrics, \ie F$_1$ scores averaged over either examples or labels 
(which are also known as instance-F$_1$ and macro-F$_1$, respectively),
as well as ranking metric, \ie R-Precision (averaged over either examples or labels).

\paragraph{Baselines}
We compare our method with four baselines.
\begin{itemize}
\item Logistic regression: independently learn a logistic regression classifier for each label, (a.k.a binary relevance).
\item PRLR~\cite{lin2014multi}: a multi-label classifier with a regulariser which encourages sparse and low-rank predictions.
\item SPEN~\cite{belanger2016structured}: a structured prediction framework which employs a deep network to represent the energy function,
      and predictions are produced by minimising the energy.
\item DVN~\cite{gygli2017deep}: a structured prediction method which uses a deep value network to distill the knowledge of a given loss function,
      which is the F$_1$ score (averaged over examples) in this task.
\end{itemize}

We implemented Logistic regression using scikit-learn~\cite{}.
The results of SPEN and DVN are reproduced using the coded released the authors,
and the results of PRLR are taken from \cite{lin2014multi}.

%$\RCal_\textsc{example}$ 
For the proposed method, we used a linear score function $f(\x) = \w_k^\top \x + b$ for the $k$-th label,
and the empirical risk was minimised with L2 regularisation using LBFGS provided by Scipy~\cite{},
and hyper-parameters were tuned using 5-fold cross validation.

\paragraph{Result analysis}
The results on test set are summarised in Table~\ref{tab:perf_mlc},
\begin{itemize}
\item $\RCal_\textsc{example}$ outperform the independent logistic regression baseline by a large margin.
\item $\RCal_\textsc{example}$ also achieves better performance than PRLR~\cite{lin2014multi} which regularisation specific to multi-label classification.
\item Finally, it is encouraging that our method performs better than (\cite{belanger2016structured}) and (\cite{gygli2017deep}),
both work learn complex non-linear functions using deep neural networks to achieve state-of-the-art performance, while our method uses a linear function.
\end{itemize}

\TODO
\begin{itemize}
\item evaluation metric: add R-Precision averaged over both examples and labels, remove AUC.
\item results of more variants: $\RCal_\textsc{label}$ and $\RCal_\textsc{both}$?
\end{itemize}


\begin{table}[!h]
\centering
\caption{Performance on multi-label dataset}
\label{tab:perf_mlc}
%\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{2pt} % tweak the space between columns
%\begin{tabular}{l*{6}{c}}
\begin{tabular}{l|ccc|ccc}
\toprule
{} & \multicolumn{3}{c|}{\textbf{bibtex}} & \multicolumn{3}{c}{\textbf{bookmarks}} \\
{} &   F$_1$ Example & F$_1$ Label &    AUC &      F$_1$ Example & F$_1$ Label &    AUC \\
\midrule
Binary Relevance~\cite{}           &          $37.9$ &      $30.1$ & $85.3$ &             $29.5$ &      $21.0$ & $87.2$ \\
PRLR~\cite{lin2014multi}           &          $44.2$ &      $37.2$ &    N/A &             $34.9$ &      $23.0$ &    N/A \\
SPEN~\cite{belanger2016structured} &          $41.3$ &      $33.7$ & $92.6$ &             $35.5$ &      $24.1$ & $90.8$ \\
DVN~\cite{gygli2017deep}           &          $44.7$ &      $32.4$ & $86.7$ &             $37.2$ &      $23.7$ & $76.9$ \\
MLR (Ours)                         &          ${\bf 47.0}$ & ${\bf 38.8}$ & ${\bf 93.3}$ & ${\bf 37.7}$ & ${\bf 28.4}$ & ${\bf 91.8}$ \\
\bottomrule
\end{tabular}
%}
\end{table}



\subsection{New song recommendation}

{\bf Task:} 
We are interested in the task of recommending newly released songs to users,
in particular, to augment users' existing playlists with newly released songs.

{\bf Experimental design:} 
We hold 40\% of latest released songs in AotM-2011~\cite{mcfee2012hypergraph} playlist dataset,
and half of that were used for testing and the other half for validation.
Given a new song which does not appeared in any playlist, 
we predict, for each playlist, whether to include it or not.

The objective is the same as $\RCal_\textsc{row}$ and $\RCal_\textsc{col}$ except that,
for the playlists that we choose to hold the later half, all we observed is the first half, 
all other songs for these playlists are unobserved (they can be positive/negative examples),
this is different from the case that we explicitly observed that songs are not in playlists (they are negative examples).


\subsubsection{A few conclusions}
\begin{table}[!h]
\centering
\caption{Statistics of dataset}
%\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccc}
\toprule
Dataset & \#Users & \#Songs (train/dev/test) & \#Playlists & \#Song Features \\
\midrule
AotM-2011 & 14,182  & 68,657 / 22,885 / 22,886 & 84,710 & 217 \\
30Music   & 8,070   & 27,281 / 9,093 / 9,094   & 17,457 & 217 \\
%30Music   & 11,339  & 31,296 / 10,431 / 10,432 & 29,803 & 217 \\
\bottomrule
\end{tabular}
%}
\end{table}

\paragraph{Which type of loss is most helpful?}
\begin{table}[!h]
\centering
\caption{Empirical results (AUC)}
%\resizebox{\linewidth}{!}{
\begin{tabular}{l|cccc}
\toprule
{}            & $\RCal_\textsc{example}$ & $\RCal_\textsc{label}$ & $\RCal_\textsc{both}$ & Independent L.R. \\
\midrule
AotM-2011     & 0.64792  & 0.67782 & 0.59602  & 0.62226 \\
30Music       & 0.6768   & 0.70917 & 0.70914  & 0.6654 \\
%30Music       & 0.54413  & 0.55894 & 0.55864  & 0.53698 \\
\bottomrule
\end{tabular}
%}
\end{table}

\paragraph{Experimental design:}
C: 1, 1, 1, p: 1, no multi-task regularisation.

\paragraph{Is multi-task regularisation helpful?}

\begin{table}[!h]
\centering
\caption{Empirical results}
%\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccc}
\toprule
{}            & Multi-task Reg. + $\RCal_\textsc{example}$ & Multi-task reg. + $\RCal_\textsc{label}$ \\
\midrule
AotM-2011     & 0.65778     & 0.6884618 \\
30Music       & 0.68179     & 0.7149 \\
%30Music       & 0.549567    & 0.557156 \\
\bottomrule
\end{tabular}
%}
\end{table}



\subsection{Playlist augmentation}

{\bf Task:}
We are interested in augment user created playlists with songs from a music library.
In particular, for a partial playlist, we would like to augment it using songs from an existing library of songs.

{\bf Experimental design:}
We hold the last half of about 20\% of all user created playlists, 
which are chosen uniformly at random from all playlists in the AtoM-2011 playlist dataset.
We filter out playlist which has less than 5 songs that appeared in the Million Song Dataset~\cite{}.


\subsubsection{A few conclusions}
\begin{table}[!h]
\centering
\caption{Statistics of dataset}
%\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccc}
\toprule
Dataset & \#Users & \#Songs & \#Playlists (train/dev/test)  & \#Song Features \\
\midrule
AotM-2011 & 14,182 & 114,428 & 50,826 / 16,942 / 16,942 & 218 \\
30Music   & 8,070  & 45,468  & 15,591 / 933 / 933       & 218 \\
%30Music   & 11,339 (train/dev/test?) & 52,159  & 17,883 / 5,960  / 5,960  & 217 \\
\bottomrule
\end{tabular}
%}
\end{table}


\paragraph{Which type of loss is most helpful?}

{\bf Row-wise loss}: weighting by the number of positive/negative labels for each example.
\ie we perform a classification/bipartite ranking task on each multilabel example 
which forms a dataset of examples with binary labels: $\{(\x_n, y_k\}_{k=1}^K$ for the $n$-th multilabel example.

\begin{equation*}
%\resizebox{\linewidth}{!}{$
\RCal_\textsc{row} 
= \displaystyle \sum_s 
  \frac{1}{K_+^s} \sum_{s \in pl} e^{-(\w_{pl}^\top \phi(s) + b)} +
  \frac{1}{K_-^s} \sum_{s \notin pl} \frac{1}{p} e^{p \w_{pl}^\top \phi(s)}.
%$}
\end{equation*}
where normalising factor $K_+^s$ is the number of playlists that include song $s$,
and $K_-^s$ is the number of playlists that do not include song $s$.


{\bf Column-wise loss}: weighting by the number of positive/negative examples for each label.
\ie we perform a classification/bipartite ranking task on each label which forms a binary dataset:
$\{\x_n, y_k\}_{n=1}^N$ for the $k$-th label.

\begin{equation*}
%\resizebox{\linewidth}{!}{$
\RCal_\textsc{col} 
= \displaystyle \sum_{pl}
  \frac{1}{N_+^{pl}} \sum_{s \in pl} e^{-(\w_{pl}^\top \phi(s) + b)} +
  \frac{1}{N_-^{pl}} \sum_{s \notin pl} \frac{1}{p} e^{p \w_{pl}^\top \phi(s)}.
%$}
\end{equation*}
where normalising factor $N_+^{pl}$ is the number of songs in playlist $pl$,
and $N_-^{pl}$ is the number of songs in a music library that playlist $pl$ does not include.


{\bf Row-wise + column-wise loss}: the summation of both: $\RCal_\textsc{row} + C \RCal_\textsc{col}$ 
where $C$ is a trade-off parameter.

The binary relevance baseline is learning a logistic regression for each playlist independently.


\begin{table}[!h]
\centering
\caption{Empirical results}
%\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccccc}
\toprule
{}            & $\RCal_\textsc{example}$ & $\RCal_\textsc{label}$ & $\RCal_\textsc{both}$ & Independent L.R. & Pop-rank \\
\midrule
%AotM-2011     & 0.6827396 & 0.743770 & 0.7385298 & 0.6924 & 0.80199 \\
AotM-2011     & 0.68459 & 0.747755 & 0.7429  & 0.6924 & 0.80199 \\
30Music       & 0.7168  & 0.76867  & 0.76917 & 0.7225 & 0.7165 \\
%30Music       & 0.56766179 & 0.6350941 & 0.63555 & 0.575567 & 0.80558 \\
\bottomrule
\end{tabular}
%}
\end{table}

\paragraph{Experimental design:}
C: 1, 1, 1, p: 1, no multi-task regularisation

\paragraph{Is multi-task regularisation helpful?}

multi-task regularisation: we regularise the difference of playlist parameters 
such that $\|\w_j - \w_k\|_2$ is small if playlist $j$ and $k$ belong to the same user.

\begin{equation*}
\RCal_\textsc{reg} = \frac{1}{\sum_u N_u (N_u - 1)} \sum_u \sum_{j, k \in u} (\w_j - \w_k)^\top (\w_j - \w_k)
\end{equation*}
where $N_u$ is the number of playlist user $u$ has.

\begin{table}[!h]
\centering
\caption{Empirical results}
%\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccc}
\toprule
{}            & Multi-task Reg. + $\RCal_\textsc{example}$ & Multi-task reg. + $\RCal_\textsc{label}$ \\
\midrule
%AotM-2011     & 0.6882099 & 0.7615590 \\
AotM-2011     & 0.69167 & 0.7819 \\
30Music       & 0.7177  & 0.7840 \\
%30Music       & 0.581426 & 0.6597  \\
%30Music       & 0.574175 & 0.667804  \\
%AUC           & 0.66583  & 0.68517   \\
\bottomrule
\end{tabular}
%}
\end{table}



\TODO
measure performance by AUC and HitRate@K,
compare with baselines such as independent logistic regression (\ie binary relevance), popularity based recommendation,
and matrix factorisation.



\subsection{Discussion}

{\it the choice of playlist dataset?}
