\section{Experiments}
\label{sec:experiment}

In this section, we describe experiments that empirically evaluate the proposed method and a number of 
well known baseline approaches in the three cold-start settings described in Section~\ref{sec:problem}.

\begin{figure*}[!t]
    \centering
    \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=.95\linewidth]{fig/hist_pluser.pdf}
        \caption{Histogram of the number of playlists per user}
        \label{fig:hist_pluser}
    \end{minipage}%
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=.95\linewidth]{fig/hist_songpop.pdf}
        \caption{Histogram of song popularity}
        \label{fig:hist_songpop}
    \end{minipage}
\end{figure*}


\subsection{Dataset}
We make use of two publicly available playlist datasets: the 30Music~\cite{30music2015} and AotM-2011~\cite{mcfee2012hypergraph} playlist dataset.
The Million Song Dataset~\cite{msd2011} serves as an underlying dataset where all songs in playlists are intersected with,
and it also provides a few song features which we will detail later.



{\bf Million Song Dataset} (MSD) is a collection of one million songs, in which information of each song such as the name, 
artist, year of release are available.
It also provides acoustic features computed from a sample section of audio file of each song. % more description

{\bf 30Music Dataset} is a collection of listening events and playlists retrieved from Last.fm\footnote{\url{https://www.last.fm}}.
We utilise the playlists data by first intersecting with the MSD, leveraging the Last.fm dataset~\cite{lastfmdataset}
which matched songs from Last.fm with those in MSD, then filtering out playlists with less than 5 songs.
This results in roughly 17K playlists over 45K songs from 8K users.

{\bf AotM-2011 Dataset} is a collection of playlists shared by users\footnote{\url{http://www.artofthemix.org}} ranging from 1998 to 2011,
where songs in the dataset have been matched to those in the Million Song Dataset (MSD).
We filtered out playlists with less than 5 songs, which results in roughly 84K playlists over 114K songs from 14K users.

Table~\ref{tab:stats_pldata} summarises the two playlist datasets used in this work.
The histograms of the number of playlists per user as well as song popularity 
(\ie the number of occurrences of a song in all playlists)
of the two datasets are shown in Figure~\ref{fig:hist_pluser} and Figure~\ref{fig:hist_songpop},
respectively.
We can see from Figure~\ref{fig:hist_pluser} and \ref{fig:hist_songpop} that both the number
of playlists per user and song popularity follow a long-tailed distribution, which imposes further challenge to the learning task as the amount
of data for learning is limited for users (or songs) at the tail.


\begin{table}[hbt]
\centering
\caption{Music playlist dataset}
\label{tab:stats_pldata}
\resizebox{\linewidth}{!}{
\begin{tabular}{lrcrcc}
\toprule
Dataset   & Songs & Playlists & Users & Songs/Playlist & Playlists/User \\
\midrule
30Music   & 45,468  & 17,457  & 8,070   & 16.3 & 2.2 \\
AotM-2011 & 114,428 & 84,710  & 14,182  & 10.1 & 6.0 \\
\bottomrule
\end{tabular}
}
\end{table}


\subsection{Experimental setup}

We describe how playlist data are split into training and test set for the three cold-start settings,
the features of songs used to learn the multitask objective, as well as baseline approaches and evaluation metrics.

{\bf Dataset split}.
To empirically evaluate the performance of recommending new songs to extend existing playlists (\ie the \emph{cold songs} setting),
we hold 5K of the latest released songs in 30Music dataset. This results in about 8K playlists with songs
in both training and test set.
In the AotM-2011 dataset with a larger number of songs, we hold 10K of the latest released songs.
This leads to about 19K playlists with songs in both training and test set.
Playlists where all songs have been held are removed from both training and test set.
Table~\ref{tab:stats0} summarises the statistics of this training/test split.

For the task of recommending songs to form new playlists for existing users (\ie the \emph{cold playlists} setting),
%To empirically evaluate the performance of our proposed recommendation approaches for existing users,
we hold playlists from about 20\% users in both datasets for test, and all other playlists are used for training.
The test set is formed by sampling playlists in which each song each song has been included in at least five playlists among the whole dataset,
and we also make sure each song in the test set also appears in the training set,
as well as all users in the test set also have a few playlists in the training set.
This results in a test set with about 2.1K playlists from 1.6K users in 30Music dataset,
and a test set with about 9.2K playlists from 2.7K users in AotM-2011 dataset.
The statistics of this training/test split are shown in Table~\ref{tab:stats1}.

To evaluate the task of recommending songs for new users to form playlists (\ie the \emph{cold users} setting),
%To evaluate the performance of music recommendation approaches for new users,
we sampled 30\% of all users and hold all their playlists as test set in both datasets.
Similarly, we require songs in the test set also exist in the training set,
a user will thus not be included in the test set if holding all of her playlists breaks this requirement.
This results in a test set with about 3.4K playlists from 2.4K users in 30Music dataset,
and a test set with about 8.2K playlists from 4.2K users in AotM-2011 dataset.
Table~\ref{tab:stats2} describes the statistics of this training/test split.


{\bf Features}.
Song features used in the experiments include metadata, audio data, genre and artist information, and song/artist popularity.
%
The metadata of songs (\eg duration, year of release) and audio features (\eg loudness, mode, tempo) are provided by MSD.
We use genre data from the Top-MAGD genre dataset~\cite{schindler2012facilitating}
and tagtraum genre annotations for MSD~\cite{schreiber2015improving} via one-hot encoding.
If the genre data of a song is not available, we apply the mean imputation using genre counts of other songs in training set.
To encode artist information as features,
we trained a word2vec\footnote{https://github.com/dav/word2vec} model using sequences of artist identifiers in playlists.

Song popularity is also a feature in the \emph{cold playlists} and \emph{cold users} settings, as we assume no popularity information 
is available for new songs.
%In the task of recommending new songs to extend existing playlists (setting (i)), where song popularity is not available,
%we use artist popularity (we use the number of occurrences of all songs from a artist in training playlists as a proxy of her popularity).
Finally, we add a constant feature (with value $1$) for each song to account for bias.


\input{tab_stats}


{\bf Baselines}.
We compare the performance of our proposed approach (referred as {\it Multitask Classification}) 
with a number of baseline methods for the task of recommending music to form playlists:
\begin{itemize}
\item The {\it Popularity Ranking} method scores each song using only its popularity in training set.
      In the \emph{cold songs} setting where song popularity is not available, we use artist popularity instead.
\item The {\it Same Artists - Greatest Hits} (SAGH)~\cite{mcfee2012million} method scores scores each song
      by its popularity if the artist of the song has appeared in the given user's playlist in training set;
      otherwise the song is scored zero.
      Similarly, we replace song popularity with artist popularity in the \emph{cold songs} setting.
\item The {\it Collocated Artists - Greatest Hits} (CAGH)~\cite{bonnin2013evaluating} method is a variant of SAGH.
      It scores each song using its popularity, but weighted by the frequency of the collocation between the artist of the song
      and those artists that appeared in the given user's playlist in training set.
      In the \emph{cold songs} setting, we again replace song popularity with artist popularity,
      and in the \emph{cold users} setting, we use the top 10 most popular artists instead of artists in the user's listening history 
      as required by this method.
\item The {\it Logistic Regression} baseline is specific for the \emph{cold songs} setting, where we independently train
      an logistic regression classifier for each playlist, which is used to classify whether we should add each new song to this playlist.
      This method is also known as binary relevance in multi-label classification.
\end{itemize}


{\bf Evaluation}.
We evaluate the performance of all approaches using two metrics that are commonly employed in playlist recommendation tasks:
Hit-Rate@K~\cite{hariri2012context} and Area under the ROC curve (AUC)~\cite{manning2008introIR}.
%
%R-Precision (RPREC) is the number of correctly recommended songs in the top-$n$ recommendation over $n$,
%where $n$ is the number of songs in the ground truth playlist.
%It is one of several metrics used to evaluate performance on playlist continuation tasks
%in the ACM RecSys Challenge 2018\footnote{https://recsys-challenge.spotify.com/rules}.
%
Hit-Rate@K (or Hit Ratio) is the number of correctly recommended songs amongst the top-$K$ recommendations over $L$,
where $K$ is the number of recommendations, and $L$ is the number of songs in the ground truth playlist.
It has been employed to evaluate several playlist generation and next song recommendation
methods~\cite{hariri2012context,bonnin2013evaluating,bonnin2015automated,jannach2015beyond}.
This metric is also known as Recall@K~\cite{schedl2017}.
%
The area under the ROC curve (AUC) is widely used in measuring performance of classifiers,
it has also been used for evaluating performance of playlist generation method when the task
is cast as a (sequence of) classification problems~\cite{ben2017groove}.


\subsection{Results and discussion}

We can see from Table~\ref{tab:perf0} that learning based methods (\ie {\it Multitask classification}
and {\it Logistic Regression}) perform significantly better than other baselines in the \emph{cold songs} setting.
%in the setting of recommending new songs for extending playlists (setting (i)).
This suggests learning from existing playlists helps the task of recommending new songs.
Among the (artist) popularity based approaches, 
simply ranking songs by its artist popularity achieves decent performance,
and taking into account artist collocation information does not seem to improve the performance.
Further, the SAGH method which considers only songs from the top 10 most popular artists,
is the worst performer in terms of HitRate@100 and AUC.

Figure~\ref{fig:hr0} shows the hit rates when the number of recommendations $K$ varies from 0 to 1000.
We can see that the performance of all methods improves as the number of recommendations increase,
and learning based methods always perform better than ranking based on artist information.
It is interesting that both {\it Multitask Classification} and {\it Logistic Regression} perform 
almost the same when $K$ is small (100 on 30Music and 50 on AotM-2011 dataset), 
and the former only performs better when $K$ is bigger than that.
Another interesting observation is that SAGH and CAGH perform similarly on AotM-2011 dataset when $K$
is less 50, and both outperform {\it Popularity Ranking}. However, SAGH suffers from slow improvement
as $K$ increases, and this effect is observed on both datasets.


\begin{table}[b]
\centering
\caption{Performance in the \emph{cold songs} setting}
\label{tab:perf0}
\resizebox{\columnwidth}{!}{
\input{tab0}
}
\end{table}

\begin{figure}[b]
\centering
\includegraphics[width=.975\linewidth]{fig/hitrate0.pdf}
\caption{Hit rates in the \emph{cold songs} setting}
\label{fig:hr0}
\end{figure}


\begin{table*}[!t]
    \centering
    \begin{minipage}{.5\textwidth}
        \centering
        \caption{Performance in the \emph{cold playlists} setting}
        \label{tab:perf1}
        \resizebox{.95\textwidth}{!}{
        \input{tab1}
        }
    \end{minipage}%
    \begin{minipage}{0.5\textwidth}
        \centering
        \caption{Performance in the \emph{cold users} setting}
        \label{tab:perf2}
        \resizebox{.95\textwidth}{!}{
        \input{tab2}
        }
    \end{minipage}
\end{table*}


\begin{figure*}[t]
    \centering
    \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=.925\linewidth]{fig/hitrate1.pdf}
        \caption{Hit rates in the \emph{cold playlists} setting}
        \label{fig:hr1}
    \end{minipage}%
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=.925\linewidth]{fig/hitrate2.pdf}
        \caption{Hit rates in the \emph{cold users} setting}
        \label{fig:hr2}
    \end{minipage}
\end{figure*}



Table~\ref{tab:perf1} and Figure~\ref{fig:hr1} show the performance in the \emph{cold playlists} setting.
%on the task of recommending a set of songs for an existing user (setting (ii)).
First, we can see that methods based on song popularity and artist information achieve good performance,
which is in line with discoveries in~\cite{bonnin2013evaluating,jannach2015beyond,bonnin2015automated} which 
show ranking based on popularity is a strong baseline.
Further, methods which make use of both song popularity and artist information
outperform {\it Popularity Ranking} (except SAGH on 30Music), which shows artist information is helpful for music recommendation,
which is consistent with results reported in~\cite{bonnin2013evaluating,bonnin2015automated}.
%
%It is interesting to see that {\it Popularity Ranking}) perform worse than other methods in terms of HitRate@100,
%but SAGH performs worse than other methods in terms of AUC,
%which might suggest that SAGH achieves good recall but {\it Popularity Ranking} performs better on average.
It is interesting to observe that SAGH is one of the best performer on AotM-2011 dataset (Figure~\ref{fig:hr1}), 
while it performs the worst in terms of HitRate@K on 30Music dataset, when $K$ is greater than 300.
%The performance of SAGH is also saturated much earlier than other methods.
%
%performance of all methods improves when $K$ increases, and {\it Multitask Classification}
%achieves the best performance on both datasets.
%However, while CAGH performs as good as {\it Multitask Classification} on 30Music,
%it is SAGH that achieves the best performance among all methods that ranks song by popularity and artist information
%on the AotM-2011 dataset, this might reflect the different characteristics between the two playlist dataset.



Finally, the performance in the \emph{cold users} setting %of recommending music to form playlists for new users (setting (iii)),
%however, 
is quite different from the other two cold-start settings.
%
We can see from Table~\ref{tab:perf2} and Figure~\ref{fig:hr2} that {\it Popularity Ranking} is one of the best performing methods,
outperforming both baselines that make use of song popularity and artist information (\ie SAGH and CAGH).
The performance of {\it Multitask Classification} is almost identical to that of {\it Popularity Ranking} in terms of all metrics,
which suggests {\it Multitask Classification} might degenerate to simply rank songs using popularity.
Another interesting observation is that, although both SAGH and CAGH make use of song popularity and artist information,
the later performs significantly better than the former in terms of AUC and HitRate@K (when $K$ is greater than 100).
This suggests the way to make use of artist information matters,
and collocation of artists could be more helpful for music recommendation than simply filtering out songs 
that are not from popular artists.

Last but not the least, our proposed method {\it Multitask Classification}, unlike any other method in consideration,
is always the (tied) best performer in all three cold-start settings on both datasets.
This suggests our proposed approach can work effectively for the cold-start settings considered in this paper.
As a remark, the performance of all methods in terms of HitRate@K improves as the number of recommendations increases,
and the rate of improvement decreases as more songs are recommended.
In particular, the performance of SAGH tends to saturate much earlier than other methods 
(except in the \emph{cold playlists} setting on AotM-2011 dataset),
which might suggest that, as more songs are recommended, artists in users' listening history (or popular artists) become less informative,
and other factors such as song popularity regain their dominance for the task of music recommendation.
