\section{Experiment}
\label{sec:experiment}

We first evaluate the proposed method on the task of tagging text, 
using two standard multi-label classification dataset~\cite{katakis2008multilabel}.

\subsection{Multi-label classification}

Performance on multi-label dataset\footnote{Results of PRLR are taken from~\citep{lin2014multi}.}
are shown in terms of F$_1$ scores averaged over both examples and labels, 
as well as precision at $k$ which focus on the top-$k$ predictions.

The results are summarised in Table~\ref{tab:perf_mlc},
where the multi-label p-classification method output performance the binary relevance baseline by a large margin.
Further, it achieves better performance than (\citet{lin2014multi}) which encourages sparse and low-rank predictions.
Finally, it is encouraging that our method performs better than (\citet{belanger2016structured}) and (\citet{gygli2017deep}),
both work learn complex non-linear functions using deep neural networks, compared with our method which uses a linear function.

\TODO show the performance with row-wise weighting, column-wise weighting and with both.

\begin{table}[!h]
\centering
\caption{Performance on multi-label dataset}
\label{tab:perf_mlc}
\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{2pt} % tweak the space between columns
%\begin{tabular}{l*{6}{c}}
\begin{tabular}{l|ccc|ccc}
\toprule
{} & \multicolumn{3}{c|}{\textbf{bibtex}} & \multicolumn{3}{c}{\textbf{bookmarks}} \\
{} &   F$_1$ Example & F$_1$ Label &    AUC &      F$_1$ Example & F$_1$ Label &    AUC \\
\midrule
Binary Relevance~\cite{}           &          $37.9$ &      $30.1$ & $85.3$ &             $29.5$ &      $21.0$ & $87.2$ \\
PRLR~\cite{lin2014multi}           &          $44.2$ &      $37.2$ &    N/A &             $34.9$ &      $23.0$ &    N/A \\
SPEN~\cite{belanger2016structured} &          $41.3$ &      $33.7$ & $92.6$ &             $35.5$ &      $24.1$ & $90.8$ \\
DVN~\cite{gygli2017deep}           &          $44.7$ &      $32.4$ & $86.7$ &             $37.2$ &      $23.7$ & $76.9$ \\
MLR (Ours)                         &          ${\bf 47.0}$ & ${\bf 38.8}$ & ${\bf 93.3}$ & ${\bf 37.7}$ & ${\bf 28.4}$ & ${\bf 91.8}$ \\
\bottomrule
\end{tabular}
}
\end{table}


\subsection{Playlist augmentation}

{\bf Task:}
We are interested in augment user created playlists with songs from a music library.
In particular, for a partial playlist, we would like to augment it using songs from an existing library of songs.

{\bf Experimental design:}
We hold the last half of about 20\% of all user created playlists, 
which are chosen uniformly at random from all playlists in the AtoM-2011 playlist dataset.
We filter out playlist which has less than 5 songs that appeared in the Million Song Dataset~\cite{}.


\newpage

\subsubsection{A few conclusions on a subset}
%\begin{table}[!h]
%\centering
%\caption{Statistics of subset}
Statistics of subset \\
\resizebox{\linewidth}{!}{
\begin{tabular}{cccccc}
\toprule
\#Users & \#Songs & \#Playlists (all)  & \#Playlists (incomplete) & \#Unknown entries &\#Song Features (audio + genre) \\
\midrule
15      & 786     & 108                & 24   & 18,723  & 217 \\
\bottomrule
\end{tabular}
}
%\end{table}


\paragraph{Which type of loss is most helpful?}

{\bf Row-wise loss}: weighting by the number of positive/negative labels for each example.
\ie we perform a classification/bipartite ranking task on each multilabel example 
which forms a dataset of examples with binary labels: $\{(\x_n, y_k\}_{k=1}^K$ for the $n$-th multilabel example.

\begin{equation*}
\resizebox{\linewidth}{!}{$
\RCal_\textsc{row} 
= \displaystyle \sum_s 
  \frac{1}{K_+^s} \sum_{s \in pl} e^{-(\w_{pl}^\top \phi(s) + b)} +
  \frac{1}{K_-^s} \sum_{s \notin pl} \frac{1}{p} e^{p \w_{pl}^\top \phi(s)}.
$}
\end{equation*}
where normalising factor $K_+^s$ is the number of playlists that include song $s$,
and $K_-^s$ is the number of playlists that do not include song $s$.


{\bf Column-wise loss}: weighting by the number of positive/negative examples for each label.
\ie we perform a classification/bipartite ranking task on each label which forms a binary dataset:
$\{\x_n, y_k\}_{n=1}^N$ for the $k$-th label.

\begin{equation*}
\resizebox{\linewidth}{!}{$
\RCal_\textsc{col} 
= \displaystyle \sum_{pl}
  \frac{1}{N_+^{pl}} \sum_{s \in pl} e^{-(\w_{pl}^\top \phi(s) + b)} +
  \frac{1}{N_-^{pl}} \sum_{s \notin pl} \frac{1}{p} e^{p \w_{pl}^\top \phi(s)}.
$}
\end{equation*}
where normalising factor $N_+^{pl}$ is the number of songs in playlist $pl$,
and $N_-^{pl}$ is the number of songs in a music library that playlist $pl$ does not include.


{\bf Row-wise + column-wise loss}: the summation of both: $\RCal_\textsc{row} + C \RCal_\textsc{col}$ 
where $C$ is a trade-off parameter.

The binary relevance baseline is learning a logistic regression for each playlist independently.


%\begin{table}[!h]
%\centering
%\caption{Empirical results on subset}
Empirical results on subset \\
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cccc}
\toprule
{}            & $\RCal_\textsc{row}$ & $\RCal_\textsc{col}$ & $\RCal_\textsc{row} + C\RCal_{col}$ & Binary Relevance \\
\midrule
AUC           &    & $0.6585$  & & $0.6127$ \\
\bottomrule
\end{tabular}
}
%\end{table}

\paragraph{Experimental design:}
C: 1, 1, 1, p: 1, no multi-task regularisation


\paragraph{Is multi-task regularisation helpful?}

multi-task regularisation: we regularise the difference of playlist parameters 
such that $\|\w_j - \w_k\|_2$ is small if playlist $j$ and $k$ belong to the same user.

\begin{equation*}
\RCal_\textsc{reg} = \frac{1}{\sum_u N_u (N_u - 1)} \sum_u \sum_{j, k \in u} (\w_j - \w_k)^\top (\w_j - \w_k)
\end{equation*}
where $N_u$ is the number of playlist user $u$ has.

%\begin{table}[!h]
%\centering
%\caption{Empirical results on subset}
Empirical results on subset \\
\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccc}
\toprule
{}            & Multi-task Reg. + Row-wise Loss & Multi-task reg. + Column-wise Loss & Multi-task reg. + Row-wise + Column-wise Loss  \\
\midrule
\bottomrule
\end{tabular}
}
%\end{table}


\subsection{New song recommendation}

{\bf Task:} 
We are interested in the task of recommending newly released songs to users,
in particular, to augment users' existing playlists with newly released songs.

{\bf Experimental design:} 
We hold 20\% of latest released songs in AotM-2011~\cite{mcfee2012hypergraph} playlist dataset.
Given a new song which does not appeared in any playlist, 
we predict, for each playlist, whether to include it or not.

The objective is the same as $\RCal_\textsc{row}$ and $\RCal_\textsc{col}$ except that,
for the playlists that we choose to hold the later half, all we observed is the first half, 
all other songs for these playlists are unobserved (they can be positive/negative examples),
this is different from the case that we explicitly observed that songs are not in playlists (they are negative examples).


\subsubsection{A few conclusions on a subset}
%\begin{table}[!h]
%\centering
%\caption{Statistics of subset}
Statistics of subset \\
\resizebox{\linewidth}{!}{
\begin{tabular}{cccccc}
\toprule
\#Users & \#Songs (train/test) & \#Playlists & \#Unknown entries &\#Song Features (audio + genre) \\
\midrule
15      & 629 / 157  & 108 & 16,956  & 217 \\
\bottomrule
\end{tabular}
}
%\end{table}

\paragraph{Which type of loss is most helpful?}
%\begin{table}[!h]
%\centering
%\caption{Empirical results on subset}
Empirical results on subset \\
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cccc}
\toprule
{}            & Row-wise Loss & Column-wise Loss & Row-wise + Column-wise loss & Independent Logistic Regression \\
\midrule
Run 1         & 0.6395              & 0.6448           & 0.6431     & 0.6022 \\
Run 2         & 0.6474              & 0.6565           & 0.6567     & 0.6197 \\
Run 3         & 0.6172              & 0.6144           & 0.6168     & 0.5366 \\
Run 4         & 0.6741              & 0.6840           & 0.6818     & 0.6067 \\
Run 5         & 0.6506              & 0.6449           & 0.6498     & 0.5716 \\
Mean          & 0.6458              & 0.6489           & 0.6496     & 0.5874 \\
Std           & 0.0205              & 0.0251           & 0.0235     & 0.0334 \\
\bottomrule
\end{tabular}
}
%\end{table}

\paragraph{Experimental design:}
C: 1, 1, 1, p: 1, no multi-task regularisation.



\paragraph{Is multi-task regularisation helpful?}

%\begin{table}[!h]
%\centering
%\caption{Empirical results on subset}
Empirical results on subset \\
\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccc}
\toprule
{}            & Multi-task Reg. + Row-wise Loss & Multi-task reg. + Column-wise Loss & Multi-task reg. + Row-wise + Column-wise Loss  \\
\midrule
Run 1         & 0.6543        & 0.6358           & 0.6472 \\
Run 2         & 0.6327        & 0.6572           & 0.6482 \\
Run 3         & 0.6633        & 0.6722           & 0.6709 \\
Run 4         & 0.6267        & 0.6765           & 0.6615 \\
Run 5         & 0.5967        & 0.6104           & 0.6062 \\
Mean          & 0.6347        & 0.6504           & 0.6468 \\
Std           & 0.0260        & 0.0274           & 0.0247 \\
\bottomrule
\end{tabular}
}
%\end{table}

\paragraph{Experimental design:}
C: 1, 1, 1, p: 1, with multi-task regularisation, no user specific regularisation parameter.



\paragraph{Is user-specific multi-task regularisation parameter (based on \#playlists) helpful?}
%\begin{table}[!h]
%\centering
%\caption{Empirical results on subset: user-specific regularisation parameter = 1/\#playlists}
Empirical results on subset: user-specific regularisation parameter = 1/\#playlists \\
\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccc}
\toprule
{}            & Multi-task Reg. + Row-wise Loss + user-spec reg. & Multi-task reg. + Column-wise Loss + user-spec reg. & Multi-task reg. + Row-wise + Column-wise Loss + user-spec reg. \\
\midrule
Run 1         & 0.6714             & 0.6744     & 0.6777 \\
Run 2         & 0.6560             & 0.6706     & 0.6646 \\
Run 3         & 0.6592             & 0.6641     & 0.6611 \\
Run 4         & 0.6444             & 0.6528     & 0.6493 \\
Run 5         & 0.6558             & 0.6648     & 0.6627 \\
Mean          & 0.6573             & 0.6653     & 0.6631 \\
Std           & 0.0097             & 0.0082     & 0.0101 \\
\bottomrule
\end{tabular}
}
%\end{table}


\TODO
measure performance by AUC and HitRate@K,
compare with baselines such as independent logistic regression (\ie binary relevance), popularity based recommendation,
and matrix factorisation.



\subsection{Discussion}

{\it the choice of playlist dataset?}
