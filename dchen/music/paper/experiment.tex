\section{Experiment}
\label{sec:experiment}

We first evaluate the proposed method on the task of tagging text, 
using two standard multi-label classification dataset~\cite{katakis2008multilabel}.

\subsection{Multi-label classification}

Performance on multi-label dataset\footnote{Results of PRLR are taken from~\citep{lin2014multi}.}
are shown in terms of F$_1$ scores averaged over both examples and labels, 
as well as precision at $k$ which focus on the top-$k$ predictions.

The results are summarised in Table~\ref{tab:perf_mlc},
where the multi-label p-classification method output performance the binary relevance baseline by a large margin.
Further, it achieves better performance than (\citet{lin2014multi}) which encourages sparse and low-rank predictions.
Finally, it is encouraging that our method performs better than (\citet{belanger2016structured}) and (\citet{gygli2017deep}),
both work learn complex non-linear functions using deep neural networks, compared with our method which uses a linear function.

\TODO show the performance with row-wise weighting, column-wise weighting and with both.

\begin{table}[!h]
\centering
\caption{Performance on multi-label dataset}
\label{tab:perf_mlc}
\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{2pt} % tweak the space between columns
%\begin{tabular}{l*{6}{c}}
\begin{tabular}{l|ccc|ccc}
\toprule
{} & \multicolumn{3}{c|}{\textbf{bibtex}} & \multicolumn{3}{c}{\textbf{bookmarks}} \\
{} &   F$_1$ Example & F$_1$ Label &    AUC &      F$_1$ Example & F$_1$ Label &    AUC \\
\midrule
Binary Relevance~\cite{}           &          $37.9$ &      $30.1$ & $85.3$ &             $29.5$ &      $21.0$ & $87.2$ \\
PRLR~\cite{lin2014multi}           &          $44.2$ &      $37.2$ &    N/A &             $34.9$ &      $23.0$ &    N/A \\
SPEN~\cite{belanger2016structured} &          $41.3$ &      $33.7$ & $92.6$ &             $35.5$ &      $24.1$ & $90.8$ \\
DVN~\cite{gygli2017deep}           &          $44.7$ &      $32.4$ & $86.7$ &             $37.2$ &      $23.7$ & $76.9$ \\
MLR (Ours)                         &          ${\bf 47.0}$ & ${\bf 38.8}$ & ${\bf 93.3}$ & ${\bf 37.7}$ & ${\bf 28.4}$ & ${\bf 91.8}$ \\
\bottomrule
\end{tabular}
}
\end{table}



\subsection{New song recommendation}

{\bf Task:} 
We are interested in the task of recommending newly released songs to users,
in particular, to augment users' existing playlists with newly released songs.

{\bf Experimental design:} 
We hold 40\% of latest released songs in AotM-2011~\cite{mcfee2012hypergraph} playlist dataset,
and half of that were used for testing and the other half for validation.
Given a new song which does not appeared in any playlist, 
we predict, for each playlist, whether to include it or not.

The objective is the same as $\RCal_\textsc{row}$ and $\RCal_\textsc{col}$ except that,
for the playlists that we choose to hold the later half, all we observed is the first half, 
all other songs for these playlists are unobserved (they can be positive/negative examples),
this is different from the case that we explicitly observed that songs are not in playlists (they are negative examples).


\subsubsection{A few conclusions}
\begin{table}[!h]
\centering
\caption{Statistics of dataset}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccc}
\toprule
Dataset & \#Users & \#Songs (train/dev/test) & \#Playlists & \#Song Features (audio + genre) \\
\midrule
AotM-2011 & 14,182  & 68,657 / 22,885 / 22,886 & 84,710 & 217 \\
30Music   & 11,339  & 31,296 / 10,431 / 10,432 & 29,803 & 217 \\
\bottomrule
\end{tabular}
}
\end{table}

\paragraph{Which type of loss is most helpful?}
\begin{table}[!h]
\centering
\caption{Empirical results (AUC)}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cccc}
\toprule
{}            & $\RCal_\textsc{example}$ & $\RCal_\textsc{label}$ & $\RCal_\textsc{both}$ & Independent Logistic Regression \\
\midrule
AotM-2011     & 0.64792  & 0.67782 & 0.59602  & 0.62226 \\
30Music       & 0.54413  & 0.55894 & 0.55864  & 0.53698 \\
\bottomrule
\end{tabular}
}
\end{table}

\paragraph{Experimental design:}
C: 1, 1, 1, p: 1, no multi-task regularisation.

\paragraph{Is multi-task regularisation helpful?}

\begin{table}[!h]
\centering
\caption{Empirical results}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccc}
\toprule
{}            & Multi-task Reg. + $\RCal_\textsc{example}$ & Multi-task reg. + $\RCal_\textsc{label}$ \\
AotM-2011     &  & \\
30Music       & 0.549567    & 0.557156 \\
\midrule
\bottomrule
\end{tabular}
}
\end{table}

\paragraph{Experimental design:}
C: 1, 1, 1, p: 1, with multi-task regularisation, no user specific regularisation parameter.

\paragraph{Is user-specific multi-task regularisation parameter (based on \#playlists) helpful?}
\begin{table}[!h]
\centering
\caption{Empirical results}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccc}
\toprule
{}            & Multi-task Reg. + Row-wise Loss + user-spec reg. & Multi-task reg. + Column-wise Loss + user-spec reg. & Multi-task reg. + Row-wise + Column-wise Loss + user-spec reg. \\
\midrule
\bottomrule
\end{tabular}
}
\end{table}



\subsection{Playlist augmentation}

{\bf Task:}
We are interested in augment user created playlists with songs from a music library.
In particular, for a partial playlist, we would like to augment it using songs from an existing library of songs.

{\bf Experimental design:}
We hold the last half of about 20\% of all user created playlists, 
which are chosen uniformly at random from all playlists in the AtoM-2011 playlist dataset.
We filter out playlist which has less than 5 songs that appeared in the Million Song Dataset~\cite{}.


\subsubsection{A few conclusions}
\begin{table}[!h]
\centering
\caption{Statistics of dataset}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccc}
\toprule
Dataset & \#Users & \#Songs & \#Playlists (train/dev/test)  & \#Song Features (audio + genre) \\
\midrule
AotM-2011 & 14,182 (train/dev/test?) & 114,428 & 50,826 / 16,942 / 16,942 & 217 \\
30Music   & 11,339 (train/dev/test?) & 52,159  & 17,883 / 5,960  / 5,960  & 217 \\
\bottomrule
\end{tabular}
}
\end{table}


\paragraph{Which type of loss is most helpful?}

{\bf Row-wise loss}: weighting by the number of positive/negative labels for each example.
\ie we perform a classification/bipartite ranking task on each multilabel example 
which forms a dataset of examples with binary labels: $\{(\x_n, y_k\}_{k=1}^K$ for the $n$-th multilabel example.

\begin{equation*}
\resizebox{\linewidth}{!}{$
\RCal_\textsc{row} 
= \displaystyle \sum_s 
  \frac{1}{K_+^s} \sum_{s \in pl} e^{-(\w_{pl}^\top \phi(s) + b)} +
  \frac{1}{K_-^s} \sum_{s \notin pl} \frac{1}{p} e^{p \w_{pl}^\top \phi(s)}.
$}
\end{equation*}
where normalising factor $K_+^s$ is the number of playlists that include song $s$,
and $K_-^s$ is the number of playlists that do not include song $s$.


{\bf Column-wise loss}: weighting by the number of positive/negative examples for each label.
\ie we perform a classification/bipartite ranking task on each label which forms a binary dataset:
$\{\x_n, y_k\}_{n=1}^N$ for the $k$-th label.

\begin{equation*}
\resizebox{\linewidth}{!}{$
\RCal_\textsc{col} 
= \displaystyle \sum_{pl}
  \frac{1}{N_+^{pl}} \sum_{s \in pl} e^{-(\w_{pl}^\top \phi(s) + b)} +
  \frac{1}{N_-^{pl}} \sum_{s \notin pl} \frac{1}{p} e^{p \w_{pl}^\top \phi(s)}.
$}
\end{equation*}
where normalising factor $N_+^{pl}$ is the number of songs in playlist $pl$,
and $N_-^{pl}$ is the number of songs in a music library that playlist $pl$ does not include.


{\bf Row-wise + column-wise loss}: the summation of both: $\RCal_\textsc{row} + C \RCal_\textsc{col}$ 
where $C$ is a trade-off parameter.

The binary relevance baseline is learning a logistic regression for each playlist independently.


\begin{table}[!h]
\centering
\caption{Empirical results}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccccc}
\toprule
{}            & $\RCal_\textsc{example}$ & $\RCal_\textsc{label}$ & $\RCal_\textsc{both}$ & Independent Logistic Regression & Popularity based ranking \\
\midrule
AotM-2011     & 0.6827396 & 0.743770 & 0.7385298 & 0.62709 & 0.7952622 \\
30Music       & 0.566196  & 0.633003 & 0.632768  & 0.57372 & 0.81998 \\
\bottomrule
\end{tabular}
}
\end{table}

\paragraph{Experimental design:}
C: 1, 1, 1, p: 1, no multi-task regularisation

\paragraph{Is multi-task regularisation helpful?}

multi-task regularisation: we regularise the difference of playlist parameters 
such that $\|\w_j - \w_k\|_2$ is small if playlist $j$ and $k$ belong to the same user.

\begin{equation*}
\RCal_\textsc{reg} = \frac{1}{\sum_u N_u (N_u - 1)} \sum_u \sum_{j, k \in u} (\w_j - \w_k)^\top (\w_j - \w_k)
\end{equation*}
where $N_u$ is the number of playlist user $u$ has.

\begin{table}[!h]
\centering
\caption{Empirical results}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccc}
\toprule
{}            & Multi-task Reg. + $\RCal_\textsc{example}$ & Multi-task reg. + $\RCal_\textsc{label}$ \\
\midrule
AotM-2011     & 0.6882099 & 0.7615590 \\
30Music       & 0.574175 & 0.667804  \\
%AUC           & 0.66583  & 0.68517   \\
\bottomrule
\end{tabular}
}
\end{table}



\TODO
measure performance by AUC and HitRate@K,
compare with baselines such as independent logistic regression (\ie binary relevance), popularity based recommendation,
and matrix factorisation.



\subsection{Discussion}

{\it the choice of playlist dataset?}
