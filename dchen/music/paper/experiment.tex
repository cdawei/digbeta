\section{Experiments}
\label{sec:experiment}

In this section, we describe experiments that empirically evaluate the proposed method and a number of 
well known baseline approaches in the three cold-start settings described in Section~\ref{sec:problem}.

\begin{figure*}[!t]
    \centering
    \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=.95\linewidth]{fig/hist_pluser.pdf}
        \caption{Histogram of the number of playlists per user}
        \label{fig:hist_pluser}
    \end{minipage}%
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=.95\linewidth]{fig/hist_songpop.pdf}
        \caption{Histogram of song popularity}
        \label{fig:hist_songpop}
    \end{minipage}
\end{figure*}


\subsection{Dataset}
We make use of two publicly available playlist datasets: the 30Music~\cite{30music2015} and AotM-2011~\cite{mcfee2012hypergraph} dataset.
The Million Song Dataset~\cite{msd2011} serves as an underlying dataset where songs in all playlists are intersected.
It also provides a few song features which will be described later in this section.



{\bf Million Song Dataset} (MSD) is a collection of one million songs, where each song is described by its metadata 
(\eg song title, artist name, year of release) and a number of precomputed audio features from the Echo Nest\footnote{\url{http://the.echonest.com}}.
%It also provides acoustic features computed from a sample section of audio file of each song. % more description

{\bf 30Music Dataset} is a collection of listening events and playlists retrieved from Last.fm\footnote{\url{https://www.last.fm}}.
We first intersect the playlists data with songs in the MSD, leveraging the Last.fm dataset~\cite{lastfmdataset}
which matched songs from Last.fm with those in the MSD, then filter out playlists with less than 5 songs.
This results in about 17K playlists over 45K songs from 8K users.

{\bf AotM-2011 Dataset} is a collection of playlists shared by Art of the Mix\footnote{\url{http://www.artofthemix.org}} 
users during the period from 1998 to 2011. Songs in playlists have been matched to those in the MSD.
We filtered out playlists with less than 5 songs, which results in roughly 84K playlists over 114K songs from 14K users.

Table~\ref{tab:stats_pldata} summarises the two playlist datasets used in this work.
The histograms of the number of playlists per user as well as song popularity 
(\ie the number of occurrences of a song in all playlists)
of the two datasets are shown in Figure~\ref{fig:hist_pluser} and Figure~\ref{fig:hist_songpop},
respectively.
We can see from Figure~\ref{fig:hist_pluser} and \ref{fig:hist_songpop} that both the number
of playlists per user and song popularity follow a long-tailed distribution, which imposes further challenge to the learning task 
as the amount of data is very limited for users (or songs) at the tail.


\begin{table}[hbt]
\centering
\caption{Music playlist dataset}
\label{tab:stats_pldata}
\resizebox{\linewidth}{!}{
\begin{tabular}{lrcrcc}
\toprule
Dataset   & Songs & Playlists & Users & Songs/Playlist & Playlists/User \\
\midrule
30Music   & 45,468  & 17,457  & 8,070   & 16.3 & 2.2 \\
AotM-2011 & 114,428 & 84,710  & 14,182  & 10.1 & 6.0 \\
\bottomrule
\end{tabular}
}
\end{table}


\subsection{Experimental setup}

We describe how playlists in the two datasets are split into training and test set for the three cold-start settings,
and the features of songs used to learn the multitask objective, as well as baseline approaches and evaluation metrics.

{\bf Dataset split}.
To empirically evaluate the performance of recommending new songs to extend existing playlists (\ie the \emph{cold songs} setting),
we hold 5K of the latest released songs in the 30Music dataset. 
This results in about 8K playlists with songs in both the training and test set.
In the AotM-2011 dataset where more songs are available, we hold 10K of the latest released songs.
This leads to about 19K playlists with songs in both the training and test set.
We remove playlists where all songs have been held for test. %are removed from both the training and test set.
Table~\ref{tab:stats0} summarises the statistics of this training/test split.

For the task of recommending songs to form new playlists for existing users (\ie the \emph{cold playlists} setting),
%To empirically evaluate the performance of our proposed recommendation approaches for existing users,
we hold playlists from about 20\% users in both datasets for test, and all other playlists are used for training.
The test set is formed by sampling playlists where each song has been included in at least five playlists among the whole dataset.
We also make sure each song in the test set also appears in the training set,
and all users in the test set also have a number of playlists in the training set.
This results in a test set with about 2.1K playlists from 1.6K users in 30Music dataset,
and a test set with about 9.2K playlists from 2.7K users in AotM-2011 dataset.
The statistics of this training/test split are shown in Table~\ref{tab:stats1}.

To evaluate the task of recommending songs for new users to form playlists (\ie the \emph{cold users} setting),
%To evaluate the performance of music recommendation approaches for new users,
we sampled 30\% of all users and hold all their playlists in both datasets.
Similarly, we require songs in the test set also exist in the training set,
a user will thus not be included in the test set if holding all of her playlists breaks this requirement.
This results in a test set with about 3.4K playlists from 2.4K users in 30Music dataset,
and a test set with about 8.2K playlists from 4.2K users in AotM-2011 dataset.
Table~\ref{tab:stats2} describes the statistics of this training/test split.


{\bf Features}
of songs used in the experiments include metadata, audio data, genre and artist information, as well as song and 
artist popularity (\ie the number of occurrences of all songs from the artist in training set).
%
The metadata of songs (\eg duration, year of release) and audio features (\eg loudness, mode, tempo) are provided by the MSD.
We use genre data from the Top-MAGD genre dataset~\cite{schindler2012facilitating}
and tagtraum genre annotations for the MSD~\cite{schreiber2015improving} via one-hot encoding.
If the genre data of a song is not available, we apply the mean imputation using genre counts of other songs in the training set.
To encode artist information as features,
we trained a word2vec\footnote{https://github.com/dav/word2vec} model using sequences of artist identifiers in playlists.

We assume no popularity information is available for newly released songs,
thus song popularity is used as a feature only in the \emph{cold playlists} and \emph{cold users} settings.
%In the task of recommending new songs to extend existing playlists (setting (i)), where song popularity is not available,
%we use artist popularity (we use the number of occurrences of all songs from a artist in training playlists as a proxy of her popularity).
Finally, we add a constant feature (with value $1$) for each song to account for bias.


\input{tab_stats}


{\bf Baselines}.
We compare the performance of our proposed approach (referred as {\it Multitask Classification}) 
with a number of baseline methods in the three cold-start settings:
%for the task of recommending music to form playlists:
\begin{itemize}
\item The {\it Popularity Ranking} method scores a song using only its popularity in the training set.
      In the \emph{cold songs} setting where song popularity is not available, we score a song using the popularity
      of the corresponding artist.
\item The {\it Same Artists - Greatest Hits} (SAGH)~\cite{mcfee2012million} method scores a song
      by its popularity if the artist of the song has appeared in the given user's playlists (in training set);
      otherwise the song is scored zero.
      In the {\it cold songs} setting, this method considers only songs from artists appeared in the given playlist,
      and score them using the corresponding artist popularity.
%      Similarly, we replace song popularity with artist popularity in the \emph{cold songs} setting.
\item The {\it Collocated Artists - Greatest Hits} (CAGH)~\cite{bonnin2013evaluating} method is a variant of SAGH.
      It scores a song using its popularity, but weighted by the frequency of the collocation between the artist of the song
      and artists appeared in the given user's playlists (in training set).
      In the \emph{cold users} setting, we use the 10 most popular artists instead of artists in the user's listening history.
      The \emph{cold songs} setting is addressed in the same way as in SAGH.
%      as required by this method.
\item The {\it Logistic Regression} baseline is specific for the \emph{cold songs} setting, where we independently learn 
      a logistic regression classifier for each playlist, which is used to classify whether we should add each new song to 
      this playlist\footnote{This method is also known as binary relevance in multi-label classification.}.
\end{itemize}


{\bf Evaluation}.
We evaluate the performance of all approaches using two metrics that have been adopted in playlist recommendation tasks:
HitRate@K~\cite{hariri2012context} and Area under the ROC curve (AUC)~\cite{manning2008introIR}.
%
%R-Precision (RPREC) is the number of correctly recommended songs in the top-$n$ recommendation over $n$,
%where $n$ is the number of songs in the ground truth playlist.
%It is one of several metrics used to evaluate performance on playlist continuation tasks
%in the ACM RecSys Challenge 2018\footnote{https://recsys-challenge.spotify.com/rules}.
%
HitRate@K (or Hit Ratio) is the number of correctly recommended songs amongst the top-$K$ recommendations over
%where $K$ is the number of recommendations, and $L$ is 
the number of songs in the ground truth playlist\footnote{This metric is also known as Recall@K~\cite{schedl2017}.}.
It has been employed to evaluate several playlist generation and next song recommendation
methods~\cite{hariri2012context,bonnin2013evaluating,bonnin2015automated,jannach2015beyond}.
%
The area under the ROC curve (AUC) is widely used in measuring performance of classifiers,
it has also been used for evaluating performance of playlist generation method when the task
is cast as a (sequence of) classification problems~\cite{ben2017groove}.


\subsection{Results and discussion}

We can see from Table~\ref{tab:perf0} that learning based methods (\ie {\it Multitask classification}
and {\it Logistic Regression}) perform significantly better than other baselines in the \emph{cold songs} setting.
%in the setting of recommending new songs for extending playlists (setting (i)).
This suggests learning from playlists data helps the task of recommending new songs to extend existing playlist.
Among the (artist) popularity based approaches, 
simply ranking songs by its artist popularity achieves decent performance,
and taking into account artist collocation information does not always improve performance.
In particular, the SAGH method which considers only songs from artists in existing playlist
perform the worst in both datasets.
%is the worst performer in terms of HitRate@100 and AUC.

Figure~\ref{fig:hr0} shows the hit rates of all methods when the number of recommendations $K$ varies from 0 to 1000.
We can see that the performance of all methods improves as the number of recommendations increases,
and learning based methods always perform better than ranking approaches based on artist information.
It is interesting to observe that both {\it Multitask Classification} and {\it Logistic Regression} perform 
almost the same when $K$ is small (less 100 on 30Music and less 50 on AotM-2011 dataset), 
and the former only performs better when $K$ becomes bigger.
Another interesting observation is that SAGH and CAGH perform similarly on the AotM-2011 dataset when $K$
is less 50, and both outperform {\it Popularity Ranking}. However, SAGH suffers from slow improvement
as $K$ increases, and this effect is observed on both datasets.


\begin{table}[b]
\centering
\caption{Performance in the \emph{cold songs} setting}
\label{tab:perf0}
\resizebox{\columnwidth}{!}{
\input{tab0}
}
\end{table}

\begin{figure}[b]
\centering
\includegraphics[width=.975\linewidth]{fig/hitrate0.pdf}
\caption{Hit rates in the \emph{cold songs} setting}
\label{fig:hr0}
\end{figure}


\begin{table*}[!t]
    \centering
    \begin{minipage}{.5\textwidth}
        \centering
        \caption{Performance in the \emph{cold playlists} setting}
        \label{tab:perf1}
        \resizebox{.95\textwidth}{!}{
        \input{tab1}
        }
    \end{minipage}%
    \begin{minipage}{0.5\textwidth}
        \centering
        \caption{Performance in the \emph{cold users} setting}
        \label{tab:perf2}
        \resizebox{.95\textwidth}{!}{
        \input{tab2}
        }
    \end{minipage}
\end{table*}


\begin{figure*}[t]
    \centering
    \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=.925\linewidth]{fig/hitrate1.pdf}
        \caption{Hit rates in the \emph{cold playlists} setting}
        \label{fig:hr1}
    \end{minipage}%
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=.925\linewidth]{fig/hitrate2.pdf}
        \caption{Hit rates in the \emph{cold users} setting}
        \label{fig:hr2}
    \end{minipage}
\end{figure*}



Figure~\ref{fig:hr1} and Table~\ref{tab:perf1} show the performance of all methods in the \emph{cold playlists} setting.
%on the task of recommending a set of songs for an existing user (setting (ii)).
First, we can see that methods based on song popularity and artist information achieve good performance.
This is consistent with discoveries in~\cite{bonnin2013evaluating,jannach2015beyond,bonnin2015automated} which 
show ranking based on song popularity is a strong baseline.
Further, methods that make use of both song popularity and artist information
outperform {\it Popularity Ranking} (except SAGH on 30Music), which shows artist information is helpful for 
recommending music to form new playlists (for existing users).
This is in line with results reported in~\cite{bonnin2013evaluating,bonnin2015automated}.
%
%It is interesting to see that {\it Popularity Ranking}) perform worse than other methods in terms of HitRate@100,
%but SAGH performs worse than other methods in terms of AUC,
%which might suggest that SAGH achieves good recall but {\it Popularity Ranking} performs better on average.
It is interesting to observe that SAGH is one of the best performer on the AotM-2011 dataset (Figure~\ref{fig:hr1}), 
while it performs the worst in terms of HitRate@K on the 30Music dataset when $K$ is greater than 300.
%The performance of SAGH is also saturated much earlier than other methods.
%
%performance of all methods improves when $K$ increases, and {\it Multitask Classification}
%achieves the best performance on both datasets.
%However, while CAGH performs as good as {\it Multitask Classification} on 30Music,
%it is SAGH that achieves the best performance among all methods that ranks song by popularity and artist information
%on the AotM-2011 dataset, this might reflect the different characteristics between the two playlist dataset.



The performance in the \emph{cold users} setting %of recommending music to form playlists for new users (setting (iii)),
%however, 
is quite different from the other two cold-start settings, as shown in Figure~\ref{fig:hr2} and Table~\ref{tab:perf2}.
{\it Popularity Ranking} is one of the best performing methods,
outperforming both baselines that make use of song popularity and artist information (\ie SAGH and CAGH).
The performance of {\it Multitask Classification} is almost identical to that of {\it Popularity Ranking} in terms of both metrics,
which suggests {\it Multitask Classification} might degenerate to simply rank songs according to their popularity.
Another interesting observation is CAGH performs significantly better than SAGH,
% in terms of AUC and HitRate@K (when $K$ is greater than 100)
although both methods exploit song popularity as well as artist information.
This suggests the way to make use of artist information matters,
and collocation of artists could be more helpful than simply filtering out songs that are not from popular artists
when recommending music for new users.

Last but not the least, our proposed method {\it Multitask Classification}, unlike any other method in consideration,
is always the (tied) best performer in all three cold-start settings on both datasets.
This suggests our proposed approach can work effectively for the cold-start settings considered in this paper.

As a remark, the performance of all methods in terms of HitRate@K improves as the number of recommendations increases,
and the rate of improvement decreases as more songs are recommended.
In particular, the performance of SAGH tends to saturate much earlier than other methods 
(except in the \emph{cold playlists} setting on the AotM-2011 dataset),
which might suggest that, as more songs are recommended, artists in users' listening history (or popular artists) become less informative,
and other factors such as song popularity regain the dominance for the task of playlist recommendation in cold start settings.
