{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label classification -- top-push loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, time\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import check_grad\n",
    "from scipy.misc import logsumexp\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer, label_ranking_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('src')\n",
    "from evaluate import avgPrecisionK, evaluatePrecision, evaluateF1, evaluateRankingLoss, f1_score_nowarn, calcLoss\n",
    "from datasets import create_dataset, dataset_names, nLabels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yeast', 'scene', 'bibtex', 'bookmarks', 'delicious', 'mediamill']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ix = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bibtex 159\n"
     ]
    }
   ],
   "source": [
    "dataset_name = dataset_names[data_ix]\n",
    "nLabels = nLabels_dict[dataset_name]\n",
    "print(dataset_name, nLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "SEED = 918273645\n",
    "fmodel_base = os.path.join(data_dir, 'tp-' + dataset_name + '-base.pkl')\n",
    "fmodel_prec = os.path.join(data_dir, 'tp-' + dataset_name + '-prec.pkl')\n",
    "fmodel_noclsw = os.path.join(data_dir, 'tp-' + dataset_name + '-noclsw.pkl')\n",
    "fperf_base = os.path.join(data_dir, 'perf-tp-base.pkl')\n",
    "fperf_prec = os.path.join(data_dir, 'perf-tp-prec.pkl')\n",
    "fperf_noclsw = os.path.join(data_dir, 'perf-tp-noclsw.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = create_dataset(dataset_name, train_data=True, shuffle=True, random_state=SEED)\n",
    "X_test,  Y_test  = create_dataset(dataset_name, train_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mean = np.mean(X_train, axis=0).reshape((1, -1))\n",
    "X_train_std = np.std(X_train, axis=0).reshape((1, -1)) + 10 ** (-6)\n",
    "X_train -= X_train_mean\n",
    "X_train /= X_train_std\n",
    "X_test  -= X_train_mean\n",
    "X_test  /= X_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_info(X_train, Y_train, X_test, Y_test):\n",
    "    N_train, D = X_train.shape\n",
    "    K = Y_train.shape[1]\n",
    "    N_test = X_test.shape[0]\n",
    "    print('%-45s %s' % ('Number of training examples:', '{:,}'.format(N_train)))\n",
    "    print('%-45s %s' % ('Number of test examples:', '{:,}'.format(N_test)))\n",
    "    print('%-45s %s' % ('Number of features:', '{:,}'.format(D)))\n",
    "    print('%-45s %s' % ('Number of labels:', '{:,}'.format(K)))\n",
    "    avgK_train = np.mean(np.sum(Y_train, axis=1))\n",
    "    avgK_test  = np.mean(np.sum(Y_test, axis=1))\n",
    "    print('%-45s %.3f (%.2f%%)' % ('Average number of positive labels (train):', avgK_train, 100*avgK_train / K))\n",
    "    print('%-45s %.3f (%.2f%%)' % ('Average number of positive labels (test):', avgK_test, 100*avgK_test / K))\n",
    "    #print('%-45s %.4f%%' % ('Average label occurrence (train):', np.mean(np.sum(Y_train, axis=0)) / N_train))\n",
    "    #print('%-45s %.4f%%' % ('Average label occurrence (test):', np.mean(np.sum(Y_test, axis=0)) / N_test))\n",
    "    print('%-45s %.3f%%' % ('Sparsity (percent) (train):', 100 * np.sum(Y_train) / np.prod(Y_train.shape)))\n",
    "    print('%-45s %.3f%%' % ('Sparsity (percent) (test):', 100 * np.sum(Y_test) / np.prod(Y_test.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:                                      bibtex\n",
      "Number of training examples:                  4,880\n",
      "Number of test examples:                      2,515\n",
      "Number of features:                           1,836\n",
      "Number of labels:                             159\n",
      "Average number of positive labels (train):    2.380 (1.50%)\n",
      "Average number of positive labels (test):     2.444 (1.54%)\n",
      "Sparsity (percent) (train):                   1.497%\n",
      "Sparsity (percent) (test):                    1.537%\n"
     ]
    }
   ],
   "source": [
    "print('%-45s %s' % ('Dataset:', dataset_name))\n",
    "print_dataset_info(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top-push loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-label learning with top push loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n = 100000\n",
    "#m = 8000\n",
    "#randM = np.random.rand(n, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "#print(np.sum(randM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "#print(np.dot(np.ones(n), np.dot(randM, np.ones(m))))  # more efficient than np.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_toppush(w, X, Y, C, r=1, weighting=True):\n",
    "    \"\"\"\n",
    "        Objective with L2 regularisation and top push loss\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "            - C: regularisation constant, C = 1 / lambda\n",
    "            - r: parameter for log-sum-exp approximation\n",
    "            - weighting: if True, divide K+ in top-push loss\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "    assert(r > 0)\n",
    "    assert(C > 0)\n",
    "    \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    # instead of using diagonal matrix to scale each row of a matrix with a different factor,\n",
    "    # we use Mat * Vec[:, None] which is more memory efficient\n",
    "    \n",
    "    if weighting is True:\n",
    "        KPosAll = np.sum(Y, axis=1)  # number of positive labels for each example, N by 1\n",
    "    else:\n",
    "        KPosAll = np.ones(N)\n",
    "        \n",
    "    A_diag = 1.0 / KPosAll\n",
    "    AY = Y * A_diag[:, None]\n",
    "    \n",
    "    T1 = np.dot(X, W.T)  # N by K\n",
    "    #m0 = np.max(T1)  # underflow in np.exp(r*T1 - m1)\n",
    "    m0 = 0.5 * (np.max(T1) + np.min(T1))\n",
    "    m1 = r * m0\n",
    "    #print('----------------')\n",
    "    #print(np.min(T1), np.max(T1), m0)\n",
    "    #print(np.min(r*T1), np.max(r*T1), m1)\n",
    "    #print(np.min(r * T1 - m1), np.max(r * T1 - m1))\n",
    "    T2 = np.multiply(1 - Y, np.exp(r * T1 - m1))  # N by K\n",
    "    B_tilde_diag = np.dot(T2, np.ones(K))\n",
    "    #print(np.max(B_tilde_diag), np.min(B_tilde_diag))  # big numbers here, can cause overflow in T3\n",
    "    \n",
    "    #T3 = np.exp(-T1 + m0) * np.power(B_tilde_diag, 1.0 / r)[:, None]\n",
    "    #T4 = np.multiply(AY, np.log1p(T3))\n",
    "    T3 = (-T1 + m0) + (1.0 / r) * np.log(B_tilde_diag)[:, None]\n",
    "    #print(np.min(T3), np.max(T3))\n",
    "    m2 = 0.5 * (np.min(T3) + np.max(T3))\n",
    "    #T4 = np.logaddexp(0, T3)\n",
    "    T4 = np.logaddexp(-m2, T3-m2) + m2\n",
    "    T5 = np.multiply(AY, T4)  \n",
    "    \n",
    "    J = np.dot(w, w) * 0.5 / C + np.dot(np.ones(N), np.dot(T5, np.ones(K))) / N\n",
    "    \n",
    "    #T5 = 1.0 / (1.0 + np.divide(1.0, T3))\n",
    "    #T5 = np.divide(T3, 1 + T3)\n",
    "    T6 = np.exp(T3 - T4)\n",
    "    O_diag = np.dot(np.multiply(Y, T6), np.ones(K))\n",
    "    T7 = A_diag * (1.0 / B_tilde_diag) * O_diag\n",
    "    \n",
    "    G1 = np.dot(np.multiply(AY, T6).T, -X)\n",
    "    G2 = np.dot((T2 * T7[:, None]).T, X)\n",
    "    \n",
    "    G = W / C + (G1 + G2) / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_toppush_loop(w, X, Y, C, r=1, weighting=True):\n",
    "    \"\"\"\n",
    "        Objective with L2 regularisation and top push loss\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "            - C: regularisation constant, C = 1 / lambda\n",
    "            - r: parameter for log-sum-exp approximation\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "    assert(r > 0)\n",
    "    assert(C > 0)\n",
    "    \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    if weighting is True:\n",
    "        KPosAll = np.sum(Y, axis=1)  # number of positive labels for each example, N by 1\n",
    "    else:\n",
    "        KPosAll = np.ones(N)\n",
    "    \n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            if Y[n, k] == 1:\n",
    "                s1 = np.sum([np.exp(r * np.dot(W[j, :] - W[k, :], X[n, :])) for j in range(K) if Y[n, j] == 0])\n",
    "                J += np.log1p(np.power(s1, 1.0 / r)) / KPosAll[n]\n",
    "    J = np.dot(w, w) * 0.5 / C + J / N\n",
    "    \n",
    "    for k in range(K):\n",
    "        for n in range(N):\n",
    "            if Y[n, k] == 1:\n",
    "                t1 = np.sum([np.exp(r * np.dot(W[j, :] - W[k, :], X[n, :])) for j in range(K) if Y[n, j] == 0])\n",
    "                t2 = -1.0 / (1 + np.power(t1, -1.0 / r))\n",
    "                G[k, :] = G[k, :] + X[n, :] * t2 / KPosAll[n]\n",
    "            else:\n",
    "                sk = 0.0\n",
    "                for k1 in range(K):\n",
    "                    if Y[n, k1] == 1:\n",
    "                        t3 = np.sum([np.exp(r * np.dot(W[j,:] - W[k1, :], X[n, :])) \\\n",
    "                                     for j in range(K) if Y[n, j] == 0])\n",
    "                        t4 = np.exp(r * np.dot(W[k, :] - W[k1, :], X[n, :]))\n",
    "                        sk += t4 / (np.power(t3, 1.0 - 1.0 / r) + t3)\n",
    "                G[k, :] = G[k, :] + X[n, :] * sk / KPosAll[n]\n",
    "                        \n",
    "    G = W / C + G / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aa = np.array([0,1,2, 0])\n",
    "#print(aa)\n",
    "#print([aa[i] for i in range(4) if aa[i] != 0])\n",
    "#print([aa[i] if aa[i] != 0 for i in range(4)])  # ERROR: invalid syntax\n",
    "#print([aa[i] if aa[i] != 0 else 10 for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train[:50, :]\n",
    "#Y_train = Y_train[:50, :]\n",
    "#C = 1\n",
    "##w0 = np.random.rand(Y_train.shape[1] * X_train.shape[1]) - 0.5\n",
    "#w0 = 0.001 * np.random.randn(Y_train.shape[1] * X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_grad(lambda w: obj_toppush(w, X_train, Y_train, C, r=2)[0], \n",
    "#           lambda w: obj_toppush(w, X_train, Y_train, C, r=2)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_grad(lambda w: obj_toppush_loop(w, X_train, Y_train, C, r=2)[0], \n",
    "#           lambda w: obj_toppush_loop(w, X_train, Y_train, C, r=2)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "eps = 1.49e-08\n",
    "w = np.zeros_like(w0)\n",
    "for i in range(len(w0)):\n",
    "    wi1 = w0.copy()\n",
    "    wi2 = w0.copy()\n",
    "    wi1[i] = wi1[i] - eps\n",
    "    wi2[i] = wi2[i] + eps\n",
    "    J1, _ = obj_toppush_loop(wi1, X_train, Y_train, C)\n",
    "    J2, _ = obj_toppush_loop(wi2, X_train, Y_train, C)\n",
    "    w[i] = (J2 - J1) / (2 * eps)\n",
    "    #print(w[i])\n",
    "J, w1 = obj_toppush_loop(w0, X_train, Y_train, C)\n",
    "diff = w1 - w\n",
    "np.sqrt(np.dot(diff, diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "print('%15s %15s %15s %15s' % ('J_Diff', 'J_loop', 'J_vec', 'G_Diff'))\n",
    "for e in range(-6, 10):\n",
    "    C = 10**(e)\n",
    "    #w0 = init_var(X_train, Y_train)\n",
    "    J,  G  = obj_toppush_loop(w0, X_train, Y_train, C, r=2)\n",
    "    J1, G1 = obj_toppush(w0, X_train, Y_train, C, r=2)\n",
    "    Gdiff = G1 - G\n",
    "    #print('%-15g %-15g %-15g' % (J1 - J, J, J1))\n",
    "    print('%15g %15g %15g %15g' % (J1 - J, J, J1, np.dot(Gdiff, Gdiff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ValueError: scoring must return a number, NOT an array\n",
    "def search_TH0(Y_true, Y_pred):\n",
    "    \"\"\"\n",
    "        For a given label, search the best threshold in 100 candidates that sampled uniformly at random\n",
    "        from all N+1 possible thresholds \n",
    "    \"\"\"\n",
    "    #SEED = 987654321\n",
    "    #np.random.seed(SEED)\n",
    "    N, K = Y_true.shape\n",
    "    THs = np.zeros(K)\n",
    "    for k in range(K):\n",
    "        # compute threshold as the mean of two successive scores\n",
    "        y = sorted(Y_pred[:, k])\n",
    "        y1 = np.zeros(N+1)\n",
    "        y2 = np.zeros(N+1)\n",
    "        y1[:N] = y\n",
    "        y1[N] = y[-1] + 1\n",
    "        y2[1:] = y\n",
    "        y2[0] = y[0] - 1\n",
    "        TH = 0.5 * (y1 + y2)\n",
    "        np.random.shuffle(TH)\n",
    "        bestTH = 0\n",
    "        bestF1 = 0\n",
    "        for th in TH[:100]:\n",
    "            F1 = f1_score_nowarn(Y_true[:, k], Y_pred[:, k] >= th)\n",
    "            if F1 > bestF1:\n",
    "                bestF1 = F1\n",
    "                bestTH = th\n",
    "        THs[k] = bestTH\n",
    "    return THs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TH(Y_true, Y_pred):\n",
    "    allPreds = sigmoid(Y_pred)\n",
    "    ranges = np.arange(0.01, 1, 0.01)\n",
    "    #F1 = [f1_score_nowarn(Y_true, allPreds >= th, average='samples') for th in ranges]\n",
    "    F1 = Parallel(n_jobs=4)(delayed(f1_score_nowarn)(Y_true, allPreds >= th, average='samples') for th in ranges)\n",
    "    bestix = np.argmax(F1)\n",
    "    return ranges[bestix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLC_toppush(BaseEstimator):\n",
    "    \"\"\"All methods are necessary for a scikit-learn estimator\"\"\"\n",
    "    \n",
    "    def __init__(self, C=1, r=1, weighting=True):\n",
    "        \"\"\"Initialisation\"\"\"\n",
    "        \n",
    "        assert C > 0\n",
    "        assert r > 0\n",
    "        assert type(weighting) == bool\n",
    "        self.C = C\n",
    "        self.r = r\n",
    "        self.weighting = weighting\n",
    "        self.trained = False\n",
    "        \n",
    "    def fit(self, X_train, Y_train):\n",
    "        \"\"\"Model fitting by optimising the objective\"\"\"\n",
    "        opt_method = 'L-BFGS-B' #'BFGS' #'Newton-CG'\n",
    "        options = {'disp': 1, 'maxiter': 10**5, 'maxfun': 10**5} # , 'iprint': 99}\n",
    "        print('\\nC: %g, r: %g' % (self.C, self.r))\n",
    "            \n",
    "        N, D = X_train.shape\n",
    "        K = Y_train.shape[1]\n",
    "        #w0 = np.random.rand(K * D) - 0.5  # initial guess in range [-1, 1]\n",
    "        w0 = 0.001 * np.random.randn(K * D)\n",
    "        opt = minimize(obj_toppush, w0, args=(X_train, Y_train, self.C, self.r, self.weighting), \\\n",
    "                       method=opt_method, jac=True, options=options)\n",
    "        if opt.success is True:\n",
    "            self.W = np.reshape(opt.x, (K, D))\n",
    "            self.trained = True\n",
    "        else:\n",
    "            sys.stderr.write('Optimisation failed')\n",
    "            print(opt.items())\n",
    "            self.trained = False\n",
    "            \n",
    "            \n",
    "    def decision_function(self, X_test):\n",
    "        \"\"\"Make predictions (score is real number)\"\"\"\n",
    "        \n",
    "        assert self.trained is True, \"Can't make prediction before training\"\n",
    "        D = X_test.shape[1]\n",
    "        return np.dot(X_test, self.W.T)\n",
    "        \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return self.decision_function(X_test)\n",
    "    #    \"\"\"Make predictions (score is boolean)\"\"\"   \n",
    "    #    preds = sigmoid(self.decision_function(X_test))\n",
    "    #    #return (preds >= 0)\n",
    "    #    assert self.TH is not None\n",
    "    #    return preds >= self.TH        \n",
    "        \n",
    "    # inherit from BaseEstimator instead of re-implement\n",
    "    #\n",
    "    #def get_params(self, deep = True):\n",
    "    #def set_params(self, **params):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_results(predictor, X_train, Y_train, X_test, Y_test, fname):\n",
    "    \"\"\"\n",
    "        Compute and save performance results\n",
    "    \"\"\"\n",
    "    preds_train = predictor.decision_function(X_train)\n",
    "    preds_test  = predictor.decision_function(X_test)\n",
    "    \n",
    "    print('Training set:')\n",
    "    perf_dict_train = evaluatePrecision(Y_train, preds_train, verbose=1)\n",
    "    print()\n",
    "    print('Test set:')\n",
    "    perf_dict_test = evaluatePrecision(Y_test, preds_test, verbose=1)\n",
    "    \n",
    "    print()\n",
    "    print('Training set:')\n",
    "    perf_dict_train.update(evaluateRankingLoss(Y_train, preds_train))\n",
    "    print(label_ranking_loss(Y_train, preds_train))\n",
    "    print()\n",
    "    print('Test set:')\n",
    "    perf_dict_test.update(evaluateRankingLoss(Y_test, preds_test))\n",
    "    print(label_ranking_loss(Y_test, preds_test))\n",
    "    \n",
    "    # compute F1 score w.r.t. different thresholds\n",
    "    #TH1 = predictor.cv_results_['mean_test_TH'][clf.best_index_]\n",
    "    #TH2 = np.mean(Y_train, axis=0)\n",
    "    #TH3 = np.mean(TH2)\n",
    "    \n",
    "    preds_train_bin = sigmoid(preds_train)\n",
    "    preds_test_bin  = sigmoid(preds_test)\n",
    "    \n",
    "    #F1_train1 = f1_score_nowarn(Y_train, sigmoid(preds_train) >= TH1, average='samples')\n",
    "    #F1_test1  = f1_score_nowarn(Y_test, sigmoid(preds_test) >= TH1, average='samples')\n",
    "    #print('\\nTrain: %.4f, %f' % (F1_train1, f1_score(Y_train, sigmoid(preds_train) >= TH1, average='samples')))\n",
    "    #print('\\nTest : %.4f, %f' % (F1_test1, f1_score(Y_test, sigmoid(preds_test) >= TH1, average='samples')))\n",
    "    \n",
    "    #F1_train2 = f1_score_nowarn(Y_train, (preds_train_bin - TH2) >= 0, average='samples')\n",
    "    #F1_test2  = f1_score_nowarn(Y_test, (preds_test_bin - TH2) >= 0, average='samples')\n",
    "    #print('\\nTrain: %.4f, %f' % (F1_train2, f1_score(Y_train, (preds_train_bin - TH2) >= 0, average='samples')))\n",
    "    #print('\\nTest : %.4f, %f' % (F1_test2, f1_score(Y_test, (preds_test_bin - TH2) >= 0, average='samples')))\n",
    "    \n",
    "    #F1_train3 = f1_score_nowarn(Y_train, preds_train_bin >= TH3, average='samples')\n",
    "    #F1_test3  = f1_score_nowarn(Y_test, preds_test_bin >= TH3, average='samples')\n",
    "    #print('\\nTrain: %.4f, %f' % (F1_train3, f1_score(Y_train, preds_train_bin >= TH3, average='samples')))\n",
    "    #print('\\nTest : %.4f, %f' % (F1_test3, f1_score(Y_test, preds_test_bin >= TH3, average='samples')))\n",
    "    \n",
    "    #perf_dict_train.update({'F1': [(F1_train1,), (F1_train2,), (F1_train3,)]})\n",
    "    #perf_dict_test.update( {'F1': [(F1_test1,),  (F1_test2,),  (F1_test3,)]})\n",
    "    #perf_dict_train.update({'F1': [(F1_train2,), (F1_train3,)]})\n",
    "    #perf_dict_test.update( {'F1': [(F1_test2,),  (F1_test3,)]})\n",
    "    \n",
    "    perf_dict = {'Train': perf_dict_train, 'Test': perf_dict_test}\n",
    "    if os.path.exists(fname):\n",
    "        _dict = pkl.load(open(fname, 'rb'))\n",
    "        if dataset_name not in _dict:\n",
    "            _dict[dataset_name] = perf_dict\n",
    "    else:\n",
    "        _dict = {dataset_name: perf_dict}\n",
    "    pkl.dump(_dict, open(fname, 'wb'))\n",
    "    \n",
    "    print()\n",
    "    print(pkl.load(open(fname, 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'ignore', 'invalid': 'ignore', 'over': 'ignore', 'under': 'ignore'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_settings = np.seterr(all='ignore')  # seterr to known value\n",
    "np.seterr(all='raise')\n",
    "#np.seterr(all='ignore')\n",
    "#np.seterr(**old_settings)  # restore settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%memit model.fit(X_train[:30], Y_train[:30])\n",
    "#%mprun -f minimize model.fit(X_train[:100], Y_train[:100])\n",
    "#%mprun -f _minimize_slsqp model.fit(X_train[:10], Y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(fmodel_base):\n",
    "    clf = pkl.load(open(fmodel_base, 'rb'))\n",
    "else:\n",
    "    clf = clf = MLC_toppush()\n",
    "    clf.fit(X_train, Y_train)\n",
    "    pkl.dump(clf, open(fmodel_base, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_results(clf, X_train, Y_train, X_test, Y_test, fperf_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation w.r.t. average precision@K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#mm = MLC_toppush(C=300000, r=4)\n",
    "#mm.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgTPLoss(allTruth, allPred):\n",
    "    return np.mean(calcLoss(allTruth, allPred, 'TopPush', njobs=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ranges = range(-6, 7)\n",
    "ranges = range(-3, 4)\n",
    "parameters = [{'C': sorted([10**(e) for e in ranges] + [3 * 10**(e) for e in ranges]),\n",
    "               'r': [8]}]\n",
    "#scorer = {'Prec': make_scorer(avgPrecisionK)}#, 'TH': make_scorer(get_TH)}\n",
    "scorer = {'Prec': make_scorer(avgTPLoss)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C: 0.001, r: 8\n",
      "\n",
      "C: 0.001, r: 8\n",
      "\n",
      "C: 0.001, r: 8\n",
      "\n",
      "C: 0.001, r: 8\n",
      "\n",
      "C: 0.001, r: 8\n",
      "\n",
      "C: 0.003, r: 8\n",
      "\n",
      "C: 0.003, r: 8\n",
      "\n",
      "C: 0.003, r: 8\n",
      "\n",
      "C: 0.003, r: 8\n",
      "\n",
      "C: 0.003, r: 8\n",
      "\n",
      "C: 0.01, r: 8\n",
      "\n",
      "C: 0.01, r: 8\n",
      "\n",
      "C: 0.01, r: 8\n",
      "\n",
      "C: 0.01, r: 8\n",
      "\n",
      "C: 0.01, r: 8\n",
      "\n",
      "C: 0.03, r: 8\n",
      "\n",
      "C: 0.03, r: 8\n",
      "\n",
      "C: 0.03, r: 8\n",
      "\n",
      "C: 0.03, r: 8\n",
      "\n",
      "C: 0.03, r: 8\n",
      "\n",
      "C: 0.1, r: 8\n",
      "\n",
      "C: 0.1, r: 8\n",
      "\n",
      "C: 0.1, r: 8\n",
      "\n",
      "C: 0.1, r: 8\n",
      "\n",
      "C: 0.1, r: 8\n",
      "\n",
      "C: 0.3, r: 8\n",
      "\n",
      "C: 0.3, r: 8\n",
      "\n",
      "C: 0.3, r: 8\n",
      "\n",
      "C: 0.3, r: 8\n",
      "\n",
      "C: 0.3, r: 8\n",
      "\n",
      "C: 1, r: 8\n",
      "\n",
      "C: 1, r: 8\n",
      "\n",
      "C: 1, r: 8\n",
      "\n",
      "C: 1, r: 8\n",
      "\n",
      "C: 1, r: 8\n",
      "\n",
      "C: 3, r: 8\n",
      "\n",
      "C: 3, r: 8\n",
      "\n",
      "C: 3, r: 8\n",
      "\n",
      "C: 3, r: 8\n",
      "\n",
      "C: 3, r: 8\n",
      "\n",
      "C: 10, r: 8\n",
      "\n",
      "C: 10, r: 8\n",
      "\n",
      "C: 10, r: 8\n",
      "\n",
      "C: 10, r: 8\n",
      "\n",
      "C: 10, r: 8\n",
      "\n",
      "C: 30, r: 8\n",
      "\n",
      "C: 30, r: 8\n",
      "\n",
      "C: 30, r: 8\n",
      "\n",
      "C: 30, r: 8\n",
      "\n",
      "C: 30, r: 8\n",
      "\n",
      "C: 100, r: 8\n",
      "\n",
      "C: 100, r: 8\n",
      "\n",
      "C: 100, r: 8\n",
      "\n",
      "C: 100, r: 8\n",
      "\n",
      "C: 100, r: 8\n",
      "\n",
      "C: 300, r: 8\n",
      "\n",
      "C: 300, r: 8\n",
      "\n",
      "C: 300, r: 8\n",
      "\n",
      "C: 300, r: 8\n",
      "\n",
      "C: 300, r: 8\n",
      "\n",
      "C: 1000, r: 8\n",
      "\n",
      "C: 1000, r: 8\n",
      "\n",
      "C: 1000, r: 8\n",
      "\n",
      "C: 1000, r: 8\n",
      "\n",
      "C: 1000, r: 8\n",
      "\n",
      "C: 3000, r: 8\n",
      "\n",
      "C: 3000, r: 8\n",
      "\n",
      "C: 3000, r: 8\n",
      "\n",
      "C: 3000, r: 8\n",
      "\n",
      "C: 3000, r: 8\n",
      "\n",
      "C: 0.001, r: 8\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(fmodel_prec):\n",
    "    clf = GridSearchCV(MLC_toppush(), parameters, scoring=scorer, cv=5, n_jobs=1, refit='Prec')\n",
    "    clf.fit(X_train, Y_train)\n",
    "    #pkl.dump(clf, open(fmodel_prec, 'wb'))\n",
    "else:\n",
    "    clf = pkl.load(open(fmodel_prec, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Average Precision@3: 0.3827, 0.003\n",
      "Average Precision@5: 0.2916, 0.002\n",
      "Average Precision@10: 0.1862, 0.002\n",
      "Average Precision@K: 0.5239, 0.006\n",
      "\n",
      "Test set:\n",
      "Average Precision@3: 0.2867, 0.005\n",
      "Average Precision@5: 0.2216, 0.003\n",
      "Average Precision@10: 0.1502, 0.002\n",
      "Average Precision@K: 0.3804, 0.007\n",
      "\n",
      "Training set:\n",
      "Average RankingLoss: 15.2928, 0.499\n",
      "0.0296222570546\n",
      "\n",
      "Test set:\n",
      "Average RankingLoss: 33.3666, 1.069\n",
      "0.0797738220608\n",
      "\n",
      "{'bibtex': {'Train': {'Precision@3': (0.62964480874316942, 0.004085705906020959), 'Precision@5': (0.43139344262295087, 0.0035121212714884715), 'Precision@K': (0.93740041123493978, 0.0025972356894115819), 'RankingLoss': (2.2200819672131149, 0.2085265910016372), 'F1': [(0.029299233417923957,), (0.029298835366993976,)]}, 'Test': {'Precision@3': (0.39257786613651424, 0.0056286626500821968), 'Precision@5': (0.2875546719681909, 0.0040758950776321129), 'Precision@K': (0.50769365547198553, 0.0077713693002391228), 'RankingLoss': (22.757057654075545, 0.84167583332299634), 'F1': [(0.030042441504210978,), (0.030042321042402571,)]}}, 'yeast': {'Train': {'Precision@3': (0.60199999999999998, 0.008118097447853129), 'Precision@5': (0.50626666666666675, 0.0065612022385241877), 'Precision@K': (0.56898540163540168, 0.0076074067624799227), 'F1': [(0.34385925298758113,), (0.46361188068473713,)]}, 'Test': {'Precision@3': (0.53871319520174477, 0.010565917513821665), 'Precision@5': (0.45343511450381679, 0.0083494869890093151), 'Precision@K': (0.50936542555953679, 0.009843428271229469), 'F1': [(0.34260919999294898,), (0.45987464615414814,)]}}, 'mediamill': {'Train': {'Precision@3': (0.71636916744955936, 0.0016037260185517866), 'Precision@5': (0.56501140786471604, 0.0013902231802054629), 'Precision@K': (0.65427486173647864, 0.0015186493281820019), 'F1': [(0.084943133363584333,), (0.085886339318819491,)]}, 'Test': {'Precision@3': (0.6972709394110832, 0.0025346447094082621), 'Precision@5': (0.54943829305746383, 0.0021784687955321022), 'Precision@K': (0.62902993346802449, 0.0024367050177822136), 'F1': [(0.085921460414869547,), (0.087005706285015486,)]}}}\n"
     ]
    }
   ],
   "source": [
    "dump_results(clf, X_train, Y_train, X_test, Y_test, fperf_prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation w.r.t. average precision@K, without weighting positive labels in objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ranges = range(-6, 5)\n",
    "parameters = [{'C': sorted([10**(e) for e in ranges] + [3 * 10**(e) for e in ranges]),\n",
    "               'r': [0.5, 1, 2, 4]}]\n",
    "if not os.path.exists(fmodel_noclsw):\n",
    "    clf = GridSearchCV(MLC_toppush(weighting=False), parameters, scoring=scorer, cv=5, n_jobs=1, refit='Prec')\n",
    "    clf.fit(X_train, Y_train)\n",
    "    pkl.dump(clf, open(fmodel_noclsw, 'wb'))\n",
    "else:\n",
    "    clf = pkl.load(open(fmodel_noclsw, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pak_train = []\n",
    "pak_test = []\n",
    "C_set = [0.01, 0.1, 1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 300, 1000, 3000]\n",
    "for C in C_set:\n",
    "    clf = MLC_toppush(C=C, r=8)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    pak_train.append(avgPrecisionK(Y_train, clf.decision_function(X_train)))\n",
    "    pak_test.append(avgPrecisionK(Y_test, clf.decision_function(X_test)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg training loss: 0.554516320144\n",
      "avg testing  loss: 0.688853496683\n"
     ]
    }
   ],
   "source": [
    "print('avg training loss:', np.mean(calcLoss(Y_train, clf.decision_function(X_train), metricType='TopPush')))\n",
    "print('avg testing  loss:', np.mean(calcLoss(Y_test, clf.decision_function(X_test), metricType='TopPush')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'C_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-2cf3ebb70b1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#ax.plot(np.arange(len(C_set)), pak_train, ls='-', c='g', label='Train')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#ax.plot(np.arange(len(C_set)), pak_test, ls='-', c='r', label='Test')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpak_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpak_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Precision@K'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'C_set' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f36b30f1400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.subplot('111')\n",
    "#ax.plot(np.arange(len(C_set)), pak_train, ls='-', c='g', label='Train')\n",
    "#ax.plot(np.arange(len(C_set)), pak_test, ls='-', c='r', label='Test')\n",
    "ax.plot(C_set, pak_train, ls='-', c='g', label='Train')\n",
    "ax.plot(C_set, pak_test, ls='-', c='r', label='Test')\n",
    "ax.set_ylabel('Precision@K')\n",
    "ax.set_xlabel('Regularisation constant C')\n",
    "ax.set_xscale('log')\n",
    "#ax.set_xticks(C_set)\n",
    "#plt.xticks(np.arange(len(C_set)), C_set)\n",
    "#plt.title('Learning curve of precision@k on bibtex dataset')\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import calcLoss\n",
    "from matplotlib.ticker import NullFormatter\n",
    "plot_loss_of_clf(clf, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_of_clf(clf, X_train, Y_train, X_test, Y_test):\n",
    "    preds_train = clf.decision_function(X_train)\n",
    "    tploss_train = calcLoss(Y_train, preds_train, 'TopPush', njobs=4)\n",
    "    pak_train = calcLoss(Y_train, preds_train, 'Precision@K', njobs=4)\n",
    "    preds_test = clf.decision_function(X_test)\n",
    "    tploss_test = calcLoss(Y_test, preds_test, 'TopPush', njobs=4)\n",
    "    pak_test = calcLoss(Y_test, preds_test, 'Precision@K', njobs=4)\n",
    "    #plot_loss(tploss_train, pak_train, 'Training set (' + dataset_name + ')')\n",
    "    \n",
    "    plot_loss(tploss_test, pak_test, 'Test set (' + dataset_name + ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss, pak, title):\n",
    "    # the data\n",
    "    x = loss\n",
    "    y = 1 - pak\n",
    "    \n",
    "    print('away from diagonal portion:', np.mean(loss != 1-pak))\n",
    "\n",
    "    nullfmt = NullFormatter()         # no labels\n",
    "\n",
    "    # definitions for the axes\n",
    "    left, width = 0.1, 0.65\n",
    "    bottom, height = 0.1, 0.65\n",
    "    bottom_h = left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.2]\n",
    "    rect_histy = [left_h, bottom, 0.2, height]\n",
    "\n",
    "    # start with a rectangular Figure\n",
    "    plt.figure(1, figsize=(8, 8))\n",
    "\n",
    "    axScatter = plt.axes(rect_scatter)\n",
    "    axHistx = plt.axes(rect_histx)\n",
    "    axHisty = plt.axes(rect_histy)\n",
    "\n",
    "    # no labels\n",
    "    axHistx.xaxis.set_major_formatter(nullfmt)\n",
    "    axHisty.yaxis.set_major_formatter(nullfmt)\n",
    "\n",
    "    # the scatter plot:\n",
    "    axScatter.scatter(x, y, color='b', alpha=0.5)\n",
    "    axScatter.plot([0, 1], [0, 1], ls='--', color='g')\n",
    "    axScatter.set_xlabel('Top push loss', fontdict={'fontsize': 12})\n",
    "    axScatter.set_ylabel('1 - precision@K', fontdict={'fontsize': 12})\n",
    "\n",
    "    # now determine nice limits by hand:\n",
    "    #binwidth = 0.25\n",
    "    #xymax = np.max([np.max(np.fabs(x)), np.max(np.fabs(y))])\n",
    "    #lim = (int(xymax/binwidth) + 1) * binwidth\n",
    "\n",
    "    #axScatter.set_xlim((-lim, lim))\n",
    "    #axScatter.set_ylim((-lim, lim))\n",
    "\n",
    "    #bins = np.arange(-lim, lim + binwidth, binwidth)\n",
    "\n",
    "    axHistx.hist(x, bins=10, color='g', alpha=0.3)\n",
    "    axHistx.set_yscale('log')\n",
    "    axHisty.hist(y, bins=10, color='g', alpha=0.3, orientation='horizontal')\n",
    "    axHisty.set_xscale('log')\n",
    "\n",
    "    #axHistx.set_xlim(axScatter.get_xlim())\n",
    "    #axHisty.set_ylim(axScatter.get_ylim())\n",
    "\n",
    "    axHistx.set_title(title, fontdict={'fontsize': 15}, loc='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute threshold for F1:\n",
    "- approach I: grid search in range(0.01, 1) with step 0.01 during cross-validation.\n",
    "- approach II: for each label, let the threshold be the average probability (to be +1) in training set.\n",
    "- approach III: let the threshold be the average of all thresholds computed in approach II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "\n",
    "#TH1 = clf.cv_results_['mean_test_TH'][clf.best_index_]\n",
    "#print(TH1)\n",
    "\n",
    "TH2 = np.mean(Y_train, axis=0)\n",
    "TH3 = np.mean(TH2)\n",
    "\n",
    "#F1_train1 = f1_score_nowarn(Y_train, sigmoid(preds_train) >= TH1, average='samples')\n",
    "#F1_test1  = f1_score_nowarn(Y_test, sigmoid(preds_test) >= TH1, average='samples')\n",
    "#print('Train: %.4f \\n' % F1_train1, f1_score(Y_train, sigmoid(preds_train) >= TH1, average='samples'))\n",
    "#print('Test : %.4f \\n' % F1_test1, f1_score(Y_test, sigmoid(preds_test) >= TH1, average='samples'))\n",
    "\n",
    "F1_train2 = f1_score_nowarn(Y_train, (preds_train - TH2) >= 0, average='samples')\n",
    "F1_test2  = f1_score_nowarn(Y_test, (preds_test - TH2) >= 0, average='samples')\n",
    "print('Train: %.4f \\n' % F1_train2, f1_score(Y_train, (preds_train - TH2) >= 0, average='samples'))\n",
    "print('Test : %.4f \\n' % F1_test2, f1_score(Y_test, (preds_test - TH2) >= 0, average='samples'))\n",
    "\n",
    "F1_train3 = f1_score_nowarn(Y_train, preds_train >= TH3, average='samples')\n",
    "F1_test3  = f1_score_nowarn(Y_test, preds_test >= TH3, average='samples')\n",
    "print('Train: %.4f \\n' % F1_train3, f1_score(Y_train, preds_train >= TH3, average='samples'))\n",
    "print('Test : %.4f \\n' % F1_test3, f1_score(Y_test, preds_test >= TH3, average='samples'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histogram of the $K$-th highest prediction scores, where $K$ is the number of positive labels for a given example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "\n",
    "K_train = Y_train.sum(axis=1).astype(np.int)\n",
    "K_test = Y_test.sum(axis=1).astype(np.int)\n",
    "\n",
    "K_pred_train = [sorted(preds_train[n, :])[::-1][K_train[n]-1] for n in range(X_train.shape[0])]\n",
    "K_pred_test = [sorted(preds_test[n, :])[::-1][K_test[n]-1] for n in range(X_test.shape[0])]\n",
    "\n",
    "fig = plt.figure(figsize=[10, 3])\n",
    "plot_data = [K_pred_train, K_pred_test]\n",
    "plt.suptitle('Histogram on %s dataset' % dataset_name)\n",
    "for i in range(2):\n",
    "    ax = plt.subplot(1,2,i+1)\n",
    "    ax.hist(plot_data[i], bins=20)\n",
    "    ax.set_title('%s set' % ('Training' if i == 0 else 'Test'))\n",
    "    ax.set_xlabel('K-th highest prediction score')\n",
    "    ax.set_ylabel('Frequency')\n",
    "plt.savefig(dataset_name + '_kscore_hist.svg')\n",
    "\n",
    "#print(K_pred_train[0], K_train[0])\n",
    "#print(K_pred_test[0], K_test[0])\n",
    "\n",
    "#preds_train[0, :]\n",
    "#preds_test[0, :]\n",
    "\n",
    "precisions_train = [avgPrecision(Y_train, preds_train, k) for k in range(1, nLabels+1)]\n",
    "precisions_test  = [avgPrecision(Y_test,  preds_test,  k) for k in range(1, nLabels+1)]\n",
    "\n",
    "precisionK_train = avgPrecisionK(Y_train, preds_train)\n",
    "precisionK_test  = avgPrecisionK(Y_test,  preds_test)\n",
    "\n",
    "plt.figure(figsize=[10,5])\n",
    "plt.plot(precisions_train, ls='--', c='r', label='Train')\n",
    "plt.plot(precisions_test,  ls='-',  c='g', label='Test')\n",
    "plt.plot([precisionK_train for k in range(nLabels)], ls='-', c='r', label='Train, Precision@K')\n",
    "plt.plot([precisionK_test  for k in range(nLabels)], ls='-', c='g', label='Test, Precision@K')\n",
    "plt.xticks(np.arange(nLabels), np.arange(1,nLabels+1))\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Precision@k')\n",
    "plt.legend(loc='best')\n",
    "plt.title('MLC w. Top-push Loss on ' + dataset_name + ' dataset')\n",
    "#plt.savefig(dataset_name + '_tp.svg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
