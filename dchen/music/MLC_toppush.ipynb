{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label classification -- top-push loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, time\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import check_grad\n",
    "from scipy.misc import logsumexp\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer, label_ranking_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('src')\n",
    "from evaluate import avgPrecisionK, evaluatePrecision, evaluateF1, evaluateRankingLoss, f1_score_nowarn\n",
    "from datasets import create_dataset, dataset_names, nLabels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ix = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = dataset_names[data_ix]\n",
    "nLabels = nLabels_dict[dataset_name]\n",
    "print(dataset_name, nLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "SEED = 918273645\n",
    "fmodel_base = os.path.join(data_dir, 'tp-' + dataset_name + '-base.pkl')\n",
    "fmodel_prec = os.path.join(data_dir, 'tp-' + dataset_name + '-prec.pkl')\n",
    "fmodel_noclsw = os.path.join(data_dir, 'tp-' + dataset_name + '-noclsw.pkl')\n",
    "fperf_base = os.path.join(data_dir, 'perf-tp-base.pkl')\n",
    "fperf_prec = os.path.join(data_dir, 'perf-tp-prec.pkl')\n",
    "fperf_noclsw = os.path.join(data_dir, 'perf-tp-noclsw.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = create_dataset(dataset_name, train_data=True, shuffle=True, random_state=SEED)\n",
    "X_test,  Y_test  = create_dataset(dataset_name, train_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%-45s %s' % ('Dataset:', dataset_name))\n",
    "print('%-45s %d' % ('Number of training examples:', X_train.shape[0]))\n",
    "print('%-45s %d' % ('Number of test examples:', X_test.shape[0]))\n",
    "print('%-45s %d' % ('Number of features:', X_train.shape[1]))\n",
    "print('%-45s %d' % ('Number of labels:', Y_train.shape[1]))\n",
    "print('%-45s %d' % ('Average number of positive labels (train):', np.mean(np.sum(Y_train, axis=1))))\n",
    "print('%-45s %d' % ('Average number of positive labels (test):', np.mean(np.sum(Y_test, axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top-push loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-label learning with top push loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n = 100000\n",
    "#m = 8000\n",
    "#randM = np.random.rand(n, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "#print(np.sum(randM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "#print(np.dot(np.ones(n), np.dot(randM, np.ones(m))))  # more efficient than np.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_toppush(w, X, Y, C, r=1, weighting=True):\n",
    "    \"\"\"\n",
    "        Objective with L2 regularisation and top push loss\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "            - C: regularisation constant, C = 1 / lambda\n",
    "            - r: parameter for log-sum-exp approximation\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "    assert(r > 0)\n",
    "    assert(C > 0)\n",
    "    \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    # instead of using diagonal matrix to scale each row of a matrix with a different factor,\n",
    "    # we use Mat * Vec[:, None] which is more memory efficient\n",
    "    \n",
    "    if weighting is True:\n",
    "        KPosAll = np.sum(Y, axis=1)  # number of positive labels for each example, N by 1\n",
    "    else:\n",
    "        KPosAll = np.ones(N)\n",
    "        \n",
    "    A_diag = 1.0 / KPosAll\n",
    "    AY = Y * A_diag[:, None]\n",
    "    \n",
    "    T1 = np.dot(X, W.T)  # N by K\n",
    "    #m0 = np.max(T1)  # underflow in np.exp(r*T1 - m1)\n",
    "    m0 = 0.5 * (np.max(T1) + np.min(T1))\n",
    "    m1 = r * m0\n",
    "    #print('----------------')\n",
    "    #print(m0, np.min(T1))\n",
    "    \n",
    "    T2 = np.multiply(1 - Y, np.exp(r * T1 - m1))  # N by K\n",
    "    B_tilde_diag = np.dot(T2, np.ones(K))\n",
    "    #print(np.max(B_tilde_diag), np.min(B_tilde_diag))  # big numbers here, can cause overflow in T3\n",
    "    \n",
    "    #T3 = np.exp(-T1 + m0) * np.power(B_tilde_diag, 1.0 / r)[:, None]\n",
    "    #T4 = np.multiply(AY, np.log1p(T3))\n",
    "    T3 = (-T1 + m0) + (1.0 / r) * np.log(B_tilde_diag)[:, None]\n",
    "    #print(np.min(T3), np.max(T3))\n",
    "    m2 = 0.5 * (np.min(T3) + np.max(T3))\n",
    "    #T4 = np.logaddexp(0, T3)\n",
    "    T4 = np.logaddexp(-m2, T3-m2) + m2\n",
    "    T5 = np.multiply(AY, T4)  \n",
    "    \n",
    "    J = np.dot(w, w) * 0.5 / C + np.dot(np.ones(N), np.dot(T5, np.ones(K))) / N\n",
    "    \n",
    "    #T5 = 1.0 / (1.0 + np.divide(1.0, T3))\n",
    "    #T5 = np.divide(T3, 1 + T3)\n",
    "    T6 = np.exp(T3 - T4)\n",
    "    O_diag = np.dot(np.multiply(Y, T6), np.ones(K))\n",
    "    T7 = A_diag * (1.0 / B_tilde_diag) * O_diag\n",
    "    \n",
    "    G1 = np.dot(np.multiply(AY, T6).T, -X)\n",
    "    G2 = np.dot((T2 * T7[:, None]).T, X)\n",
    "    \n",
    "    G = W / C + (G1 + G2) / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_toppush_loop(w, X, Y, C, r=1, weighting=True):\n",
    "    \"\"\"\n",
    "        Objective with L2 regularisation and top push loss\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "            - C: regularisation constant, C = 1 / lambda\n",
    "            - r: parameter for log-sum-exp approximation\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "    assert(r > 0)\n",
    "    assert(C > 0)\n",
    "    \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    if weighting is True:\n",
    "        KPosAll = np.sum(Y, axis=1)  # number of positive labels for each example, N by 1\n",
    "    else:\n",
    "        KPosAll = np.ones(N)\n",
    "    \n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            if Y[n, k] == 1:\n",
    "                s1 = np.sum([np.exp(r * np.dot(W[j, :] - W[k, :], X[n, :])) for j in range(K) if Y[n, j] == 0])\n",
    "                J += np.log1p(np.power(s1, 1.0 / r)) / KPosAll[n]\n",
    "    J = np.dot(w, w) * 0.5 / C + J / N\n",
    "    \n",
    "    for k in range(K):\n",
    "        for n in range(N):\n",
    "            if Y[n, k] == 1:\n",
    "                t1 = np.sum([np.exp(r * np.dot(W[j, :] - W[k, :], X[n, :])) for j in range(K) if Y[n, j] == 0])\n",
    "                t2 = -1.0 / (1 + np.power(t1, -1.0 / r))\n",
    "                G[k, :] = G[k, :] + X[n, :] * t2 / KPosAll[n]\n",
    "            else:\n",
    "                sk = 0.0\n",
    "                for k1 in range(K):\n",
    "                    if Y[n, k1] == 1:\n",
    "                        t3 = np.sum([np.exp(r * np.dot(W[j,:] - W[k1, :], X[n, :])) \\\n",
    "                                     for j in range(K) if Y[n, j] == 0])\n",
    "                        t4 = np.exp(r * np.dot(W[k, :] - W[k1, :], X[n, :]))\n",
    "                        sk += t4 / (np.power(t3, 1.0 - 1.0 / r) + t3)\n",
    "                G[k, :] = G[k, :] + X[n, :] * sk / KPosAll[n]\n",
    "                        \n",
    "    G = W / C + G / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aa = np.array([0,1,2, 0])\n",
    "#print(aa)\n",
    "#print([aa[i] for i in range(4) if aa[i] != 0])\n",
    "#print([aa[i] if aa[i] != 0 else 10 for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train[:50, :]\n",
    "#Y_train = Y_train[:50, :]\n",
    "#C = 1\n",
    "##w0 = np.random.rand(Y_train.shape[1] * X_train.shape[1]) - 0.5\n",
    "#w0 = 0.001 * np.random.randn(Y_train.shape[1] * X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_grad(lambda w: obj_toppush(w, X_train, Y_train, C, r=2)[0], \n",
    "           lambda w: obj_toppush(w, X_train, Y_train, C, r=2)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_grad(lambda w: obj_toppush_loop(w, X_train, Y_train, C, r=2)[0], \n",
    "#           lambda w: obj_toppush_loop(w, X_train, Y_train, C, r=2)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "eps = 1.49e-08\n",
    "w = np.zeros_like(w0)\n",
    "for i in range(len(w0)):\n",
    "    wi1 = w0.copy()\n",
    "    wi2 = w0.copy()\n",
    "    wi1[i] = wi1[i] - eps\n",
    "    wi2[i] = wi2[i] + eps\n",
    "    J1, _ = obj_toppush_loop(wi1, X_train, Y_train, C)\n",
    "    J2, _ = obj_toppush_loop(wi2, X_train, Y_train, C)\n",
    "    w[i] = (J2 - J1) / (2 * eps)\n",
    "    #print(w[i])\n",
    "J, w1 = obj_toppush_loop(w0, X_train, Y_train, C)\n",
    "diff = w1 - w\n",
    "np.sqrt(np.dot(diff, diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script false\n",
    "print('%15s %15s %15s %15s' % ('J_Diff', 'J_loop', 'J_vec', 'G_Diff'))\n",
    "for e in range(-6, 10):\n",
    "    C = 10**(e)\n",
    "    #w0 = init_var(X_train, Y_train)\n",
    "    J,  G  = obj_toppush_loop(w0, X_train, Y_train, C, r=2)\n",
    "    J1, G1 = obj_toppush(w0, X_train, Y_train, C, r=2)\n",
    "    Gdiff = G1 - G\n",
    "    #print('%-15g %-15g %-15g' % (J1 - J, J, J1))\n",
    "    print('%15g %15g %15g %15g' % (J1 - J, J, J1, np.dot(Gdiff, Gdiff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ValueError: scoring must return a number, NOT an array\n",
    "def search_TH0(Y_true, Y_pred):\n",
    "    \"\"\"\n",
    "        For a given label, search the best threshold in 100 candidates that sampled uniformly at random\n",
    "        from all N+1 possible thresholds \n",
    "    \"\"\"\n",
    "    #SEED = 987654321\n",
    "    #np.random.seed(SEED)\n",
    "    N, K = Y_true.shape\n",
    "    THs = np.zeros(K)\n",
    "    for k in range(K):\n",
    "        # compute threshold as the mean of two successive scores\n",
    "        y = sorted(Y_pred[:, k])\n",
    "        y1 = np.zeros(N+1)\n",
    "        y2 = np.zeros(N+1)\n",
    "        y1[:N] = y\n",
    "        y1[N] = y[-1] + 1\n",
    "        y2[1:] = y\n",
    "        y2[0] = y[0] - 1\n",
    "        TH = 0.5 * (y1 + y2)\n",
    "        np.random.shuffle(TH)\n",
    "        bestTH = 0\n",
    "        bestF1 = 0\n",
    "        for th in TH[:100]:\n",
    "            F1 = f1_score_nowarn(Y_true[:, k], Y_pred[:, k] >= th)\n",
    "            if F1 > bestF1:\n",
    "                bestF1 = F1\n",
    "                bestTH = th\n",
    "        THs[k] = bestTH\n",
    "    return THs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TH(Y_true, Y_pred):\n",
    "    allPreds = sigmoid(Y_pred)\n",
    "    ranges = np.arange(0.01, 1, 0.01)\n",
    "    #F1 = [f1_score_nowarn(Y_true, allPreds >= th, average='samples') for th in ranges]\n",
    "    F1 = Parallel(n_jobs=4)(delayed(f1_score_nowarn)(Y_true, allPreds >= th, average='samples') for th in ranges)\n",
    "    bestix = np.argmax(F1)\n",
    "    return ranges[bestix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLC_toppush(BaseEstimator):\n",
    "    \"\"\"All methods are necessary for a scikit-learn estimator\"\"\"\n",
    "    \n",
    "    def __init__(self, C=1, r=1, weighting=True):\n",
    "        \"\"\"Initialisation\"\"\"\n",
    "        \n",
    "        assert C > 0\n",
    "        assert r > 0\n",
    "        assert type(weighting) == bool\n",
    "        self.C = C\n",
    "        self.r = r\n",
    "        self.weighting = weighting\n",
    "        self.trained = False\n",
    "        \n",
    "    def fit(self, X_train, Y_train):\n",
    "        \"\"\"Model fitting by optimising the objective\"\"\"\n",
    "        opt_method = 'L-BFGS-B' #'BFGS' #'Newton-CG'\n",
    "        options = {'disp': 1, 'maxiter': 10**5, 'maxfun': 10**5} # , 'iprint': 99}\n",
    "        print('\\nC: %g, r: %g' % (self.C, self.r))\n",
    "            \n",
    "        N, D = X_train.shape\n",
    "        K = Y_train.shape[1]\n",
    "        #w0 = np.random.rand(K * D) - 0.5  # initial guess in range [-1, 1]\n",
    "        w0 = 0.001 * np.random.randn(K * D)\n",
    "        opt = minimize(obj_toppush, w0, args=(X_train, Y_train, self.C, self.r, self.weighting), \\\n",
    "                       method=opt_method, jac=True, options=options)\n",
    "        if opt.success is True:\n",
    "            self.W = np.reshape(opt.x, (K, D))\n",
    "            self.trained = True\n",
    "        else:\n",
    "            sys.stderr.write('Optimisation failed')\n",
    "            print(opt.items())\n",
    "            self.trained = False\n",
    "            \n",
    "            \n",
    "    def decision_function(self, X_test):\n",
    "        \"\"\"Make predictions (score is real number)\"\"\"\n",
    "        \n",
    "        assert self.trained is True, \"Can't make prediction before training\"\n",
    "        D = X_test.shape[1]\n",
    "        return np.dot(X_test, self.W.T)\n",
    "        \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return self.decision_function(X_test)\n",
    "    #    \"\"\"Make predictions (score is boolean)\"\"\"   \n",
    "    #    preds = sigmoid(self.decision_function(X_test))\n",
    "    #    #return (preds >= 0)\n",
    "    #    assert self.TH is not None\n",
    "    #    return preds >= self.TH        \n",
    "        \n",
    "    # inherit from BaseEstimator instead of re-implement\n",
    "    #\n",
    "    #def get_params(self, deep = True):\n",
    "    #def set_params(self, **params):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_results(predictor, X_train, Y_train, X_test, Y_test, fname):\n",
    "    \"\"\"\n",
    "        Compute and save performance results\n",
    "    \"\"\"\n",
    "    preds_train = predictor.decision_function(X_train)\n",
    "    preds_test  = predictor.decision_function(X_test)\n",
    "    \n",
    "    print('Training set:')\n",
    "    perf_dict_train = evaluatePrecision(Y_train, preds_train)\n",
    "    print()\n",
    "    print('Test set:')\n",
    "    perf_dict_test = evaluatePrecision(Y_test, preds_test)\n",
    "    \n",
    "    print()\n",
    "    print('Training set:')\n",
    "    perf_dict_train.update(evaluateRankingLoss(Y_train, preds_train))\n",
    "    print(label_ranking_loss(Y_train, preds_train))\n",
    "    print()\n",
    "    print('Test set:')\n",
    "    perf_dict_test.update(evaluateRankingLoss(Y_test, preds_test))\n",
    "    print(label_ranking_loss(Y_test, preds_test))\n",
    "    \n",
    "    # compute F1 score w.r.t. different thresholds\n",
    "    #TH1 = predictor.cv_results_['mean_test_TH'][clf.best_index_]\n",
    "    TH2 = np.mean(Y_train, axis=0)\n",
    "    TH3 = np.mean(TH2)\n",
    "    \n",
    "    preds_train_bin = sigmoid(preds_train)\n",
    "    preds_test_bin  = sigmoid(preds_test)\n",
    "    \n",
    "    #F1_train1 = f1_score_nowarn(Y_train, sigmoid(preds_train) >= TH1, average='samples')\n",
    "    #F1_test1  = f1_score_nowarn(Y_test, sigmoid(preds_test) >= TH1, average='samples')\n",
    "    #print('\\nTrain: %.4f, %f' % (F1_train1, f1_score(Y_train, sigmoid(preds_train) >= TH1, average='samples')))\n",
    "    #print('\\nTest : %.4f, %f' % (F1_test1, f1_score(Y_test, sigmoid(preds_test) >= TH1, average='samples')))\n",
    "    \n",
    "    F1_train2 = f1_score_nowarn(Y_train, (preds_train_bin - TH2) >= 0, average='samples')\n",
    "    F1_test2  = f1_score_nowarn(Y_test, (preds_test_bin - TH2) >= 0, average='samples')\n",
    "    print('\\nTrain: %.4f, %f' % (F1_train2, f1_score(Y_train, (preds_train_bin - TH2) >= 0, average='samples')))\n",
    "    print('\\nTest : %.4f, %f' % (F1_test2, f1_score(Y_test, (preds_test_bin - TH2) >= 0, average='samples')))\n",
    "    \n",
    "    F1_train3 = f1_score_nowarn(Y_train, preds_train_bin >= TH3, average='samples')\n",
    "    F1_test3  = f1_score_nowarn(Y_test, preds_test_bin >= TH3, average='samples')\n",
    "    print('\\nTrain: %.4f, %f' % (F1_train3, f1_score(Y_train, preds_train_bin >= TH3, average='samples')))\n",
    "    print('\\nTest : %.4f, %f' % (F1_test3, f1_score(Y_test, preds_test_bin >= TH3, average='samples')))\n",
    "    \n",
    "    #perf_dict_train.update({'F1': [(F1_train1,), (F1_train2,), (F1_train3,)]})\n",
    "    #perf_dict_test.update( {'F1': [(F1_test1,),  (F1_test2,),  (F1_test3,)]})\n",
    "    perf_dict_train.update({'F1': [(F1_train2,), (F1_train3,)]})\n",
    "    perf_dict_test.update( {'F1': [(F1_test2,),  (F1_test3,)]})\n",
    "    \n",
    "    perf_dict = {'Train': perf_dict_train, 'Test': perf_dict_test}\n",
    "    if os.path.exists(fname):\n",
    "        _dict = pkl.load(open(fname, 'rb'))\n",
    "        if dataset_name not in _dict:\n",
    "            _dict[dataset_name] = perf_dict\n",
    "    else:\n",
    "        _dict = {dataset_name: perf_dict}\n",
    "    pkl.dump(_dict, open(fname, 'wb'))\n",
    "    \n",
    "    print()\n",
    "    print(pkl.load(open(fname, 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_settings = np.seterr(all='ignore')  # seterr to known value\n",
    "np.seterr(all='raise')\n",
    "#np.seterr(all='ignore')\n",
    "#np.seterr(**old_settings)  # restore settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%memit model.fit(X_train[:30], Y_train[:30])\n",
    "#%mprun -f minimize model.fit(X_train[:100], Y_train[:100])\n",
    "#%mprun -f _minimize_slsqp model.fit(X_train[:10], Y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(fmodel_base):\n",
    "    clf = pkl.load(open(fmodel_base, 'rb'))\n",
    "else:\n",
    "    clf = clf = MLC_toppush()\n",
    "    clf.fit(X_train, Y_train)\n",
    "    pkl.dump(clf, open(fmodel_base, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_results(clf, X_train, Y_train, X_test, Y_test, fperf_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation w.r.t. average precision@K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = range(-6, 7)\n",
    "parameters = [{'C': sorted([10**(e) for e in ranges] + [3 * 10**(e) for e in ranges])}]\n",
    "scorer = {'Prec': make_scorer(avgPrecisionK)}#, 'TH': make_scorer(get_TH)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(fmodel_prec):\n",
    "    clf = GridSearchCV(MLC_toppush(), parameters, scoring=scorer, cv=10, n_jobs=1, refit='Prec')\n",
    "    clf.fit(X_train, Y_train)\n",
    "    pkl.dump(clf, open(fmodel_prec, 'wb'))\n",
    "else:\n",
    "    clf = pkl.load(open(fmodel_prec, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_results(clf, X_train, Y_train, X_test, Y_test, fperf_prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation w.r.t. average precision@K, without weighting positive labels in objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(fmodel_noclsw):\n",
    "    clf = GridSearchCV(MLC_toppush(weighting=False), parameters, scoring=scorer, cv=10, n_jobs=1, refit='Prec')\n",
    "    clf.fit(X_train, Y_train)\n",
    "    pkl.dump(clf, open(fmodel_noclsw, 'wb'))\n",
    "else:\n",
    "    clf = pkl.load(open(fmodel_noclsw, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_results(clf, X_train, Y_train, X_test, Y_test, fperf_noclsw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute threshold for F1:\n",
    "- approach I: grid search in range(0.01, 1) with step 0.01 during cross-validation.\n",
    "- approach II: for each label, let the threshold be the average probability (to be +1) in training set.\n",
    "- approach III: let the threshold be the average of all thresholds computed in approach II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "\n",
    "#TH1 = clf.cv_results_['mean_test_TH'][clf.best_index_]\n",
    "#print(TH1)\n",
    "\n",
    "TH2 = np.mean(Y_train, axis=0)\n",
    "TH3 = np.mean(TH2)\n",
    "\n",
    "#F1_train1 = f1_score_nowarn(Y_train, sigmoid(preds_train) >= TH1, average='samples')\n",
    "#F1_test1  = f1_score_nowarn(Y_test, sigmoid(preds_test) >= TH1, average='samples')\n",
    "#print('Train: %.4f \\n' % F1_train1, f1_score(Y_train, sigmoid(preds_train) >= TH1, average='samples'))\n",
    "#print('Test : %.4f \\n' % F1_test1, f1_score(Y_test, sigmoid(preds_test) >= TH1, average='samples'))\n",
    "\n",
    "F1_train2 = f1_score_nowarn(Y_train, (preds_train - TH2) >= 0, average='samples')\n",
    "F1_test2  = f1_score_nowarn(Y_test, (preds_test - TH2) >= 0, average='samples')\n",
    "print('Train: %.4f \\n' % F1_train2, f1_score(Y_train, (preds_train - TH2) >= 0, average='samples'))\n",
    "print('Test : %.4f \\n' % F1_test2, f1_score(Y_test, (preds_test - TH2) >= 0, average='samples'))\n",
    "\n",
    "F1_train3 = f1_score_nowarn(Y_train, preds_train >= TH3, average='samples')\n",
    "F1_test3  = f1_score_nowarn(Y_test, preds_test >= TH3, average='samples')\n",
    "print('Train: %.4f \\n' % F1_train3, f1_score(Y_train, preds_train >= TH3, average='samples'))\n",
    "print('Test : %.4f \\n' % F1_test3, f1_score(Y_test, preds_test >= TH3, average='samples'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histogram of the $K$-th highest prediction scores, where $K$ is the number of positive labels for a given example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "\n",
    "K_train = Y_train.sum(axis=1).astype(np.int)\n",
    "K_test = Y_test.sum(axis=1).astype(np.int)\n",
    "\n",
    "K_pred_train = [sorted(preds_train[n, :])[::-1][K_train[n]-1] for n in range(X_train.shape[0])]\n",
    "K_pred_test = [sorted(preds_test[n, :])[::-1][K_test[n]-1] for n in range(X_test.shape[0])]\n",
    "\n",
    "fig = plt.figure(figsize=[10, 3])\n",
    "plot_data = [K_pred_train, K_pred_test]\n",
    "plt.suptitle('Histogram on %s dataset' % dataset_name)\n",
    "for i in range(2):\n",
    "    ax = plt.subplot(1,2,i+1)\n",
    "    ax.hist(plot_data[i], bins=20)\n",
    "    ax.set_title('%s set' % ('Training' if i == 0 else 'Test'))\n",
    "    ax.set_xlabel('K-th highest prediction score')\n",
    "    ax.set_ylabel('Frequency')\n",
    "plt.savefig(dataset_name + '_kscore_hist.svg')\n",
    "\n",
    "#print(K_pred_train[0], K_train[0])\n",
    "#print(K_pred_test[0], K_test[0])\n",
    "\n",
    "#preds_train[0, :]\n",
    "#preds_test[0, :]\n",
    "\n",
    "precisions_train = [avgPrecision(Y_train, preds_train, k) for k in range(1, nLabels+1)]\n",
    "precisions_test  = [avgPrecision(Y_test,  preds_test,  k) for k in range(1, nLabels+1)]\n",
    "\n",
    "precisionK_train = avgPrecisionK(Y_train, preds_train)\n",
    "precisionK_test  = avgPrecisionK(Y_test,  preds_test)\n",
    "\n",
    "plt.figure(figsize=[10,5])\n",
    "plt.plot(precisions_train, ls='--', c='r', label='Train')\n",
    "plt.plot(precisions_test,  ls='-',  c='g', label='Test')\n",
    "plt.plot([precisionK_train for k in range(nLabels)], ls='-', c='r', label='Train, Precision@K')\n",
    "plt.plot([precisionK_test  for k in range(nLabels)], ls='-', c='g', label='Test, Precision@K')\n",
    "plt.xticks(np.arange(nLabels), np.arange(1,nLabels+1))\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Precision@k')\n",
    "plt.legend(loc='best')\n",
    "plt.title('MLC w. Top-push Loss on ' + dataset_name + ' dataset')\n",
    "#plt.savefig(dataset_name + '_tp.svg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
