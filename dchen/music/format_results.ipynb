{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('src')\n",
    "from datasets import dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = dataset_names\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_names = ['LR', 'LR-CV', 'TP', 'TP-CV', 'TP-NW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['Precision@3', 'Precision@5', 'Precision@K', 'RankingLoss', 'F1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pd.MultiIndex.from_product([dataset_names, algo_names], names=['Dataset', 'Method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train = pd.DataFrame(index=indices, columns=metrics)\n",
    "df_test  = pd.DataFrame(index=indices, columns=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fprec_tp0 = os.path.join(data_dir, 'perf-tp-base.pkl')\n",
    "fprec_tp1 = os.path.join(data_dir, 'perf-tp-prec.pkl')\n",
    "fprec_tp2 = os.path.join(data_dir, 'perf-tp-noclsw.pkl')\n",
    "fprec_lr0 = os.path.join(data_dir, 'perf-lr-base.pkl')\n",
    "fprec_lr1 = os.path.join(data_dir, 'perf-lr-prec.pkl')\n",
    "records_tp0 = pkl.load(open(fprec_tp0, 'rb'))\n",
    "records_tp1 = pkl.load(open(fprec_tp1, 'rb'))\n",
    "records_tp2 = pkl.load(open(fprec_tp2, 'rb'))\n",
    "records_lr0 = pkl.load(open(fprec_lr0, 'rb'))\n",
    "records_lr1 = pkl.load(open(fprec_lr1, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#records_lr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_str(records_dict, ds, split):\n",
    "    return ', '.join(['$%.4f$' % v[0] for v in records_dict[ds][split]['F1']])\n",
    "\n",
    "for metric in metrics:\n",
    "    for ds in datasets:\n",
    "        if metric == 'F1':\n",
    "            df_test.loc[(ds, 'TP'), metric] = f1_str(records_tp0, ds, 'Test')\n",
    "            df_test.loc[(ds, 'TP-CV'), metric] = f1_str(records_tp1, ds, 'Test')\n",
    "            df_test.loc[(ds, 'TP-NW'), metric] = f1_str(records_tp2, ds, 'Test')\n",
    "        else:\n",
    "            df_test.loc[(ds, 'TP'), metric] = records_tp0[ds]['Test' ][metric][0]\n",
    "            df_test.loc[(ds, 'TP-CV'), metric] = records_tp1[ds]['Test' ][metric][0]\n",
    "            df_test.loc[(ds, 'TP-NW'), metric] = records_tp2[ds]['Test' ][metric][0]\n",
    "        \n",
    "        df_test.loc[(ds, 'LR'), metric] = records_lr0[ds]['Test' ][metric][0]\n",
    "        df_test.loc[(ds, 'LR-CV'), metric] = records_lr1[ds]['Test' ][metric][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['\\\\textbf{%.4f}' % v if v else '' for v in is_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_test = df_test.to_latex(float_format=lambda x: '$%.4f$' % x, na_rep='-', multirow=True, escape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\\\begin{table}[!h]')\n",
    "print('\\centering')\n",
    "print('\\\\caption{Performance on test set}')\n",
    "print('\\\\label{tab:perf}')    \n",
    "print(tab_test)\n",
    "print('\\\\end{table}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['yeast', 'scene', 'bibtex', 'bookmarks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split = 'Train'\n",
    "split = 'Test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load perf records\n",
    "perf_files = [os.path.join(data_dir, 'perf-' + algo + '.pkl') for algo in algos]\n",
    "perf_dicts = [pkl.load(open(fname, 'rb')) for fname in perf_files]\n",
    "\n",
    "def perfstr(perftuple):\n",
    "    return '$%.4f \\\\pm  %.3f$' % (perftuple[0], perftuple[1])\n",
    "\n",
    "for metric in metrics:\n",
    "    tabstr = []\n",
    "    tabstr.append('\\\\begin{table}[!h]')\n",
    "    tabstr.append('\\centering')\n",
    "    tabstr.append('\\\\caption{Performance in terms of average %s (%s)}' % (metric, split))\n",
    "    tabstr.append('\\\\begin{tabular}{lcccc} \\\\\\\\ \\\\hline \\\\hline')\n",
    "    tabstr.append(' & ' + ' & '.join([ds for ds in datasets]) + ' \\\\\\\\')\n",
    "    for j in range(len(algos)):\n",
    "        tabstr.append(algo_names[j] + ' & '\n",
    "                      + ' & '.join([perfstr(perf_dicts[j][ds][split][metric]) \\\n",
    "                                    if ds in perf_dicts[j] else '-' for ds in datasets ])\n",
    "                      + ' \\\\\\\\')\n",
    "    tabstr.append('\\\\hline')\n",
    "    tabstr.append('\\\\end{tabular}')\n",
    "    tabstr.append('\\\\end{table}')\n",
    "    print('\\n'.join(tabstr))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
