{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines - new song recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os, sys, time, gzip\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix, csr_matrix, issparse\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tools import calc_RPrecision_HitRate\n",
    "from tools import calc_metrics, diversity, pairwise_distance_hamming, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(all='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPs = [5, 10, 20, 30, 50, 100, 200, 300, 500, 700, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['aotm2011', '30music']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aotm2011'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dix = 0\n",
    "dataset_name = datasets[dix]\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/%s/coldstart/setting1' % dataset_name\n",
    "X_trndev = pkl.load(gzip.open(os.path.join(data_dir, 'X_trndev.pkl.gz'), 'rb'))\n",
    "Y_trndev = pkl.load(gzip.open(os.path.join(data_dir, 'Y_trndev.pkl.gz'), 'rb'))\n",
    "X_test = pkl.load(gzip.open(os.path.join(data_dir, 'X_test.pkl.gz'), 'rb'))\n",
    "Y_test = pkl.load(gzip.open(os.path.join(data_dir, 'Y_test.pkl.gz'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs1 = pkl.load(gzip.open(os.path.join(data_dir, 'songs_train_dev_test_s1.pkl.gz'), 'rb'))\n",
    "train_songs = songs1['train_song_set']\n",
    "dev_songs = songs1['dev_song_set']\n",
    "test_songs = songs1['test_song_set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2index_trndev = {sid: ix for ix, (sid, _) in enumerate(train_songs + dev_songs)}\n",
    "song2index_test = {sid: ix for ix, (sid, _) in enumerate(test_songs)}\n",
    "index2song_test = {ix: sid for ix, (sid, _) in enumerate(test_songs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_song2artist = pkl.load(gzip.open('data/msd/song2artist.pkl.gz', 'rb'))\n",
    "song2artist = {sid: _song2artist[sid] for sid, _ in train_songs + dev_songs + test_songs if sid in _song2artist}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_playlists = pkl.load(gzip.open(os.path.join(data_dir, 'playlists_s1.pkl.gz'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist2pop = dict()\n",
    "test_songset = set(test_songs)\n",
    "\n",
    "for pl, _ in all_playlists:\n",
    "    for sid in [sid for sid in pl if sid not in test_songset]:\n",
    "        if sid in song2artist:\n",
    "            aid = song2artist[sid]\n",
    "            try:\n",
    "                artist2pop[aid] += 1\n",
    "            except KeyError:\n",
    "                artist2pop[aid] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2genre = pkl.load(gzip.open('data/msd/song2genre.pkl.gz', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_genre = set(song2genre.values())\n",
    "# all_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliques_all = pkl.load(gzip.open(os.path.join(data_dir, 'cliques_trndev.pkl.gz'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = len(cliques_all)\n",
    "pl2u = np.zeros(Y_test.shape[1], dtype=np.int32)\n",
    "for u in range(U):\n",
    "    clq = cliques_all[u]\n",
    "    pl2u[clq] = u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_user = np.zeros((Y_test.shape[0], U), dtype=np.int)\n",
    "# for u in range(U):\n",
    "#     clq = cliques_all[u]\n",
    "#     Y_user[:, u] = Y_test[:, clq].sum(axis=1).A.reshape(-1).astype(np.bool).astype(np.int)\n",
    "# Y_user = csr_matrix(Y_user)\n",
    "# print(Y_user.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2pop = pkl.load(gzip.open(os.path.join(data_dir, 'song2pop.pkl.gz'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 63)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 84646)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_test_csr = Y_test.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `p XOR q = ( p AND NOT q )  OR  ( NOT p AND q )` from [here](https://math.stackexchange.com/questions/38473/is-xor-a-combination-of-and-and-not-operators),\n",
    "let $\\mathbf{p}, \\mathbf{q} \\in \\{0, 1\\}^{n}$, then\n",
    "  \n",
    "$\n",
    "\\begin{aligned}\n",
    "& \\text{Hamming_distance}(\\mathbf{p}, \\mathbf{q})  \\\\\n",
    "& = \\frac{1}{n} \\sum_{i=1}^n p_i \\ \\text{XOR} \\ q_i \\\\\n",
    "& = \\frac{1}{n} \\sum_{i=1}^n \\left( p_i (1 - q_i) + (1 - p_i) q_i \\right) \\\\\n",
    "& = \\frac{1}{n} \\left( \\sum_{i=1}^n p_i (1 - q_i) + \\sum_{i=1}^n (1 - p_i) q_i \\right) \\\\\n",
    "& = \\frac{1}{n} \\left( \\mathbf{p}^\\top (\\mathbf{1} - \\mathbf{q}) + (\\mathbf{1} - \\mathbf{p})^\\top \\mathbf{q} \\right) \\\\\n",
    "& = \\frac{1}{n} \\left( \\text{sum}(\\mathbf{p}) + \\text{sum}(\\mathbf{q}) - 2 \\mathbf{p}^\\top \\mathbf{q} \\right)\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.977465792911703e-28\n",
      "4.977465792911703e-28\n"
     ]
    }
   ],
   "source": [
    "N, D = 1000, 200\n",
    "aa = np.zeros(N * D, dtype=np.int)\n",
    "idx = np.random.permutation(N * D)[:int(N * D * .3)]\n",
    "aa[idx] = 1\n",
    "aa = aa.reshape(N, D)\n",
    "d1 = pairwise_distances(aa, metric='hamming', n_jobs=2)\n",
    "d2 = (np.dot(aa, 1-aa.T) + np.dot(1-aa, aa.T)) / D\n",
    "sum_vec = aa.sum(axis=1, keepdims=True)\n",
    "d3 = (sum_vec + sum_vec.T - 2 * np.dot(aa, aa.T)) / D\n",
    "diff = (d1 - d2).ravel()\n",
    "print(np.dot(diff, diff))\n",
    "diff2 = (d1 - d3).ravel()\n",
    "print(np.dot(diff2, diff2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aa = Y_test_csr[:500, :].A\n",
    "# aa = Y_user[:10, :].A\n",
    "# aa_csr = csr_matrix(aa)\n",
    "# t0 = time.time()\n",
    "# d1 = pairwise_distances(aa, metric='hamming', n_jobs=2)\n",
    "# t1 = time.time()\n",
    "# d2 = pairwise_distance_hamming(aa_csr)\n",
    "# t2 = time.time()\n",
    "# diff = (d1 - d2.A).ravel()\n",
    "# print(np.sqrt(np.dot(diff, diff)))\n",
    "# print('%.3f sec, %.3f sec' % (t1 - t0, t2 - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def diversity(vec):\n",
    "#     assert vec.ndim == 1\n",
    "#     norm = len(vec) * (len(vec) - 1)\n",
    "#     sim_mat = vec[..., np.newaxis] == vec[np.newaxis, ...]  # pairwise comparison\n",
    "#     # dist_mat = 1 - sim_mat\n",
    "#     # return (dist_mat.sum() - dist_mat.trace()) / norm  # note that dist_mat.trace() = 0\n",
    "#     return (1 - sim_mat).sum() / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popularity (of artist) based recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[:, 3].A.reshape(-1).dtype == np.bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[:, 3].A.reshape(-1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[:, 3].A.reshape(-1)[(1 - Y_test[:, 3].A.reshape(-1))].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9998,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[:, 3].A.reshape(-1)[(1 - Y_test[:, 3].A.reshape(-1)).astype(np.bool)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84600 / 84646\n",
      "19504 / 84646\n"
     ]
    }
   ],
   "source": [
    "rps_pop = []\n",
    "hitrates_pop = {top: [] for top in TOPs}\n",
    "aucs_pop = []\n",
    "spreads_pop = []\n",
    "novelties_pop = {top: dict() for top in TOPs}\n",
    "# diversities_pop = []\n",
    "artist_diversities_pop = {top: [] for top in TOPs}\n",
    "genre_diversities_pop = {top: [] for top in TOPs}\n",
    "ptops_pop = []\n",
    "np.random.seed(0)\n",
    "\n",
    "y_pred = np.zeros(len(test_songs))\n",
    "for ix in range(len(test_songs)):\n",
    "    sid = index2song_test[ix]\n",
    "    if sid in song2artist:\n",
    "        aid = song2artist[sid]\n",
    "        if aid in artist2pop:\n",
    "            y_pred[ix] = np.log(artist2pop[aid])\n",
    "\n",
    "npos = Y_test.sum(axis=0).A.reshape(-1)\n",
    "assert Y_test.shape[0] == len(test_songs)\n",
    "for j in range(Y_test.shape[1]):\n",
    "    if (j+1) % 100 == 0:\n",
    "        sys.stdout.write('\\r%d / %d' % (j+1, Y_test.shape[1]))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    if npos[j] < 1:\n",
    "        continue\n",
    "        \n",
    "    y_true = Y_test[:, j].A.reshape(-1)\n",
    "\n",
    "    # rp, hr_dict = calc_RPrecision_HitRate(y_true, y_pred, tops=TOPs)\n",
    "    rp, hr_dict, auc = calc_metrics(y_true, y_pred, tops=TOPs)\n",
    "    rps_pop.append(rp)\n",
    "    for top in TOPs:\n",
    "        hitrates_pop[top].append(hr_dict[top])\n",
    "    aucs_pop.append(auc)\n",
    "    \n",
    "    # spread\n",
    "    y_pred_prob = softmax(y_pred)\n",
    "    spreads_pop.append(-np.dot(y_pred_prob, np.log(y_pred_prob)))\n",
    "\n",
    "    # novelty\n",
    "    sortix = np.argsort(-y_pred)\n",
    "    u = pl2u[j]\n",
    "    for top in TOPs:\n",
    "        nov = np.mean([-np.log2(song2pop[index2song_test[ix]]) for ix in sortix[:top]])\n",
    "        try:\n",
    "            novelties_pop[top][u].append(nov)\n",
    "        except KeyError:\n",
    "            novelties_pop[top][u] = [nov]\n",
    "    \n",
    "    # PTop: (#pos ranked above the top-ranked negative) / #pos\n",
    "    assert y_true.dtype == np.bool\n",
    "    negIx = (1 - y_true).astype(np.bool)\n",
    "    negMax = y_pred[negIx].max()\n",
    "    pt = (y_pred[y_true] > negMax).sum() / npos[j]\n",
    "    ptops_pop.append(pt)\n",
    "\n",
    "    # compute diversity@100\n",
    "    # sim = cosine_similarity(X_test[sortix[:100], :])\n",
    "    # sim = cosine_similarity(Y_user[sortix[:100], :])\n",
    "    # csd = 1. / cosine_similarity(X_test[sortix[:100], :])\n",
    "    # dist = pairwise_distances(Y_test_csr[sortix[:100], :].A, metric='hamming', n_jobs=4)\n",
    "    # dist = pairwise_distance_hamming(Y_test_csr[sortix[:100], :], normalise=True)\n",
    "    # dist = pairwise_distance_hamming(Y_user[sortix[:50], :], normalise=True)\n",
    "    # div = 100 * 99 / (sim.sum() - sim.trace())\n",
    "    # diversities_pop.append(div)\n",
    "    \n",
    "    # artist/genre diversity\n",
    "#     for top in TOPs:\n",
    "#         artist_vec = np.array([song2artist[index2song_test[ix]] for ix in sortix[:top]])\n",
    "#         genre_vec = np.array([song2genre[index2song_test[ix]] if index2song_test[ix] in song2genre \\\n",
    "#                               else str(np.random.rand()) for ix in sortix[:top]])\n",
    "#         artist_diversities_pop[top].append( diversity(artist_vec) )\n",
    "#         genre_diversities_pop[top].append( diversity(genre_vec) )\n",
    "    \n",
    "print('\\n%d / %d' % (len(rps_pop), Y_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=[20, 5])\n",
    "# ax1 = plt.subplot(131)\n",
    "# ax1.hist(rps_pop, bins=100)\n",
    "# ax1.set_yscale('log')\n",
    "# ax1.set_title('R-Precision')\n",
    "# #ax.set_xlim(0, xmax)\n",
    "# ax2 = plt.subplot(132)\n",
    "# ax2.hist(aucs_pop, bins=100)\n",
    "# ax2.set_yscale('log')\n",
    "# ax2.set_title('AUC')\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R-Precision': 0.00045846185932834824,\n",
       " 'Hit-Rate': {5: 0.0019463027743380492,\n",
       "  10: 0.008978570066680198,\n",
       "  20: 0.026329491607767857,\n",
       "  30: 0.034485467064040695,\n",
       "  50: 0.041645387039762755,\n",
       "  100: 0.0869623720184838,\n",
       "  200: 0.16780507689895133,\n",
       "  300: 0.21846529423276878,\n",
       "  500: 0.2880161152209812,\n",
       "  700: 0.37076227219712626,\n",
       "  1000: 0.44232044414107435},\n",
       " 'AUC': 0.7651405470977015,\n",
       " 'Spread': 7.837291792929633,\n",
       " 'Novelty': {5: -3.129941053094283,\n",
       "  10: -2.9389485875244667,\n",
       "  20: -2.846227428738747,\n",
       "  30: -2.649783184632382,\n",
       "  50: -2.6637723140352563,\n",
       "  100: -2.3080483444860573,\n",
       "  200: -2.390772397823536,\n",
       "  300: -2.1781126574158867,\n",
       "  500: -2.037780627134103,\n",
       "  700: -1.9853254227767099,\n",
       "  1000: -1.8056258495204032},\n",
       " 'PTop': 0.0}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_perf = {dataset_name: {'Test': {'R-Precision': np.mean(rps_pop), \n",
    "                                    'Hit-Rate': {top: np.mean(hitrates_pop[top]) for top in TOPs},\n",
    "                                    'AUC': np.mean(aucs_pop),\n",
    "                                    'Spread': np.mean(spreads_pop),\n",
    "                                    'Novelty': {t: np.mean([np.mean(novelties_pop[t][u]) for u in novelties_pop[t]]) \n",
    "                                                for t in TOPs},\n",
    "                                    'PTop': np.mean(ptops_pop),\n",
    "                                    #'Artist-Diversity': {top: np.mean(artist_diversities_pop[top]) for top in TOPs},\n",
    "                                    #'Genre-Diversity': {top: np.mean(genre_diversities_pop[top]) for top in TOPs}},\n",
    "                                    # 'Novelty': np.mean([np.mean(novelty_pop[u]) for u in novelty_pop]),\n",
    "                                    # 'Diveristy': np.mean(diversities_pop)},\n",
    "                                   },\n",
    "                           'Test_All': {'R-Precision': rps_pop,\n",
    "                                        'Hit-Rate': {top: hitrates_pop[top] for top in TOPs},\n",
    "                                        'AUC': aucs_pop,\n",
    "                                        'Spread': spreads_pop,\n",
    "                                        'Novelty': novelties_pop,\n",
    "                                        'PTop': ptops_pop,\n",
    "                                        #'Artist-Diversity': artist_diversities_pop,\n",
    "                                        #'Genre-Diversity': genre_diversities_pop},\n",
    "                                        # 'Novelty': novelty_pop,\n",
    "                                        # 'Diversity': diversities_pop},\n",
    "                          }}}\n",
    "pop_perf[dataset_name]['Test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/aotm2011/coldstart/setting1/perf-pop.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'R-Precision': 0.00045846185932834824,\n",
       " 'Hit-Rate': {5: 0.0019463027743380492,\n",
       "  10: 0.008978570066680198,\n",
       "  20: 0.026329491607767857,\n",
       "  30: 0.034485467064040695,\n",
       "  50: 0.041645387039762755,\n",
       "  100: 0.0869623720184838,\n",
       "  200: 0.16780507689895133,\n",
       "  300: 0.21846529423276878,\n",
       "  500: 0.2880161152209812,\n",
       "  700: 0.37076227219712626,\n",
       "  1000: 0.44232044414107435},\n",
       " 'AUC': 0.7651405470977015,\n",
       " 'Spread': 7.837291792929633,\n",
       " 'Novelty': {5: -3.129941053094283,\n",
       "  10: -2.9389485875244667,\n",
       "  20: -2.846227428738747,\n",
       "  30: -2.649783184632382,\n",
       "  50: -2.6637723140352563,\n",
       "  100: -2.3080483444860573,\n",
       "  200: -2.390772397823536,\n",
       "  300: -2.1781126574158867,\n",
       "  500: -2.037780627134103,\n",
       "  700: -1.9853254227767099,\n",
       "  1000: -1.8056258495204032},\n",
       " 'PTop': 0.0}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fperf_pop = os.path.join(data_dir, 'perf-pop.pkl')\n",
    "print(fperf_pop)\n",
    "pkl.dump(pop_perf, open(fperf_pop, 'wb'))\n",
    "pkl.load(open(fperf_pop, 'rb'))[dataset_name]['Test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same Artists - Greatest Hits (SAGH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommend according to the popularity of artists in listening history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84600 / 84646\n",
      "19504 / 84646\n"
     ]
    }
   ],
   "source": [
    "rps_sagh = []\n",
    "hitrates_sagh = {top: [] for top in TOPs}\n",
    "aucs_sagh = []\n",
    "spreads_sagh = []\n",
    "novelties_sagh = {top: dict() for top in TOPs}\n",
    "ptops_sagh = []\n",
    "# diversities_sagh = []\n",
    "# artist_diversities_sagh = {top: [] for top in TOPs}\n",
    "# genre_diversities_sagh = {top: [] for top in TOPs}\n",
    "np.random.seed(0)\n",
    "\n",
    "npos = Y_test.sum(axis=0).A.reshape(-1)\n",
    "assert Y_test.shape[0] == len(test_songs)\n",
    "for j in range(Y_test.shape[1]):\n",
    "    if (j+1) % 100 == 0:\n",
    "        sys.stdout.write('\\r%d / %d' % (j+1, Y_test.shape[1]))\n",
    "        sys.stdout.flush()\n",
    "    if npos[j] < 1:\n",
    "        continue\n",
    "\n",
    "    y_true = Y_test[:, j].A.reshape(-1)\n",
    "    y_pred = np.zeros(y_true.shape)\n",
    "    \n",
    "    pl = all_playlists[j][0]\n",
    "    artists = set([song2artist[sid] for sid in pl if (sid not in test_songset) and (sid in song2artist)])\n",
    "    assert len(artists) > 0\n",
    "    \n",
    "    for ix in range(Y_test.shape[0]):\n",
    "        sid = index2song_test[ix]\n",
    "        if sid in song2artist:\n",
    "            aid = song2artist[sid]\n",
    "            if aid in artists and aid in artist2pop:\n",
    "                y_pred[ix] = np.log(artist2pop[aid])\n",
    "    \n",
    "    # rp, hr_dict = calc_RPrecision_HitRate(y_true, y_pred, tops=TOPs)\n",
    "    rp, hr_dict, auc = calc_metrics(y_true, y_pred, tops=TOPs)\n",
    "    rps_sagh.append(rp)\n",
    "    for top in TOPs:\n",
    "        hitrates_sagh[top].append(hr_dict[top])\n",
    "    aucs_sagh.append(auc)\n",
    "    \n",
    "    # spread\n",
    "    y_pred_prob = softmax(y_pred)\n",
    "    spreads_sagh.append(-np.dot(y_pred_prob, np.log(y_pred_prob)))\n",
    "\n",
    "    # novelty\n",
    "    sortix = np.argsort(-y_pred)\n",
    "    u = pl2u[j]\n",
    "    for top in TOPs:\n",
    "        nov = np.mean([-np.log2(song2pop[index2song_test[ix]]) for ix in sortix[:top]])\n",
    "        try:\n",
    "            novelties_sagh[top][u].append(nov)\n",
    "        except KeyError:\n",
    "            novelties_sagh[top][u] = [nov]\n",
    "            \n",
    "    # PTop: (#pos ranked above the top-ranked negative) / #pos\n",
    "    assert y_true.dtype == np.bool\n",
    "    negIx = (1 - y_true).astype(np.bool)\n",
    "    negMax = y_pred[negIx].max()\n",
    "    pt = (y_pred[y_true] > negMax).sum() / npos[j]\n",
    "    ptops_sagh.append(pt)\n",
    "\n",
    "    # compute diversity@100\n",
    "    # csd = 1. / cosine_similarity(X_test[sortix[:100], :])\n",
    "    # dist = pairwise_distance_hamming(Y_test_csr[sortix[:100], :])\n",
    "    # diversities_sagh.append((dist.sum() - np.trace(dist)) / (100 * 99))\n",
    "    \n",
    "    # artist/genre diversity\n",
    "#     for top in TOPs:\n",
    "#         artist_vec = np.array([song2artist[index2song_test[ix]] if index2song_test[ix] in song2artist\n",
    "#                                else str(np.random.rand()) for ix in sortix[:top]])\n",
    "#         genre_vec = np.array([song2genre[index2song_test[ix]] if index2song_test[ix] in song2genre \\\n",
    "#                               else str(np.random.rand()) for ix in sortix[:top]])\n",
    "#         artist_diversities_sagh[top].append( diversity(artist_vec) )\n",
    "#         genre_diversities_sagh[top].append( diversity(genre_vec) )\n",
    "    \n",
    "print('\\n%d / %d' % (len(rps_sagh), Y_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=[20, 5])\n",
    "# ax1 = plt.subplot(131)\n",
    "# ax1.hist(rps_sagh, bins=100)\n",
    "# ax1.set_yscale('log')\n",
    "# ax1.set_title('R-Precision')\n",
    "# #ax.set_xlim(0, xmax)\n",
    "# ax2 = plt.subplot(132)\n",
    "# ax2.hist(aucs_sagh, bins=100)\n",
    "# ax2.set_yscale('log')\n",
    "# ax2.set_title('AUC')\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R-Precision': 0.007427217237866541,\n",
       " 'Hit-Rate': {5: 0.01811550290685792,\n",
       "  10: 0.02937167795898846,\n",
       "  20: 0.047835047518287065,\n",
       "  30: 0.060370186285889915,\n",
       "  50: 0.0713790240142713,\n",
       "  100: 0.07804852414600995,\n",
       "  200: 0.08528333158796113,\n",
       "  300: 0.09073722788110458,\n",
       "  500: 0.1092896683212962,\n",
       "  700: 0.12851089049949455,\n",
       "  1000: 0.15930748940090228},\n",
       " 'AUC': 0.535619728646388,\n",
       " 'Spread': 5.867802862020574,\n",
       " 'Novelty': {5: -2.038097015098608,\n",
       "  10: -1.899618058035108,\n",
       "  20: -1.7028521238730225,\n",
       "  30: -1.5721217031131645,\n",
       "  50: -1.3424711122102508,\n",
       "  100: -1.229427210513937,\n",
       "  200: -1.1178257167852619,\n",
       "  300: -1.0415400690234087,\n",
       "  500: -1.0433153171979328,\n",
       "  700: -1.0074659092459426,\n",
       "  1000: -0.9949820403776282},\n",
       " 'PTop': 0.0038752734481815694}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagh_perf = {dataset_name: {'Test': {'R-Precision': np.mean(rps_sagh), \n",
    "                                     'Hit-Rate': {top: np.mean(hitrates_sagh[top]) for top in TOPs},\n",
    "                                     'AUC': np.mean(aucs_sagh),\n",
    "                                     'Spread': np.mean(spreads_sagh),\n",
    "                                     'Novelty': {t: np.mean([np.mean(novelties_sagh[t][u]) \n",
    "                                                             for u in novelties_sagh[t]]) for t in TOPs},\n",
    "                                     'PTop': np.mean(ptops_sagh),\n",
    "                                     # 'Artist-Diversity': {t: np.mean(artist_diversities_sagh[t]) for t in TOPs},\n",
    "                                     # 'Genre-Diversity': {t: np.mean(genre_diversities_sagh[t]) for t in TOPs}},\n",
    "                                    },\n",
    "                            'Test_All': {'R-Precision': rps_sagh,\n",
    "                                        'Hit-Rate': {top: hitrates_sagh[top] for top in TOPs},\n",
    "                                        'AUC': aucs_sagh,\n",
    "                                        'Spread': spreads_sagh,\n",
    "                                        'Novelty': novelties_sagh,\n",
    "                                        'PTop': ptops_sagh, \n",
    "                                        # 'Artist-Diversity': artist_diversities_sagh,\n",
    "                                        # 'Genre-Diversity': genre_diversities_sagh},\n",
    "                           }}}\n",
    "sagh_perf[dataset_name]['Test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/aotm2011/coldstart/setting1/perf-sagh.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'R-Precision': 0.007427217237866541,\n",
       " 'Hit-Rate': {5: 0.01811550290685792,\n",
       "  10: 0.02937167795898846,\n",
       "  20: 0.047835047518287065,\n",
       "  30: 0.060370186285889915,\n",
       "  50: 0.0713790240142713,\n",
       "  100: 0.07804852414600995,\n",
       "  200: 0.08528333158796113,\n",
       "  300: 0.09073722788110458,\n",
       "  500: 0.1092896683212962,\n",
       "  700: 0.12851089049949455,\n",
       "  1000: 0.15930748940090228},\n",
       " 'AUC': 0.535619728646388,\n",
       " 'Spread': 5.867802862020574,\n",
       " 'Novelty': {5: -2.038097015098608,\n",
       "  10: -1.899618058035108,\n",
       "  20: -1.7028521238730225,\n",
       "  30: -1.5721217031131645,\n",
       "  50: -1.3424711122102508,\n",
       "  100: -1.229427210513937,\n",
       "  200: -1.1178257167852619,\n",
       "  300: -1.0415400690234087,\n",
       "  500: -1.0433153171979328,\n",
       "  700: -1.0074659092459426,\n",
       "  1000: -0.9949820403776282},\n",
       " 'PTop': 0.0038752734481815694}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fperf_sagh = os.path.join(data_dir, 'perf-sagh.pkl')\n",
    "print(fperf_sagh)\n",
    "pkl.dump(sagh_perf, open(fperf_sagh, 'wb'))\n",
    "pkl.load(open(fperf_sagh, 'rb'))[dataset_name]['Test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocated Artists - Greatest Hits (CAGH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the similarity of two artist $a_1$ and $a_2$ given a set of playlist $P$:   \n",
    "$$\n",
    "\\text{sim}(a_1, a_2) \n",
    "= \\frac{\\sum_{p \\in P} \\delta(a_1, p) \\times \\delta(a_2, p)}\n",
    "       {\\sqrt{\\sum_{p \\in P} \\delta(a_1, p) \\times \\sum_{p \\in P} \\delta(a_2, p)}}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\delta(a, p) \n",
    "= \\begin{cases}\n",
    "1, \\ \\text{at least one song in playlist $p$ is from artist $a$}, \\\\\n",
    "0, \\ \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommend according to the popularity of songs, but weighted by similarity of (`artist in user's listening history`, `artist of song`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_artist_trndev = sorted(set([song2artist[sid] for pl, _ in all_playlists for sid in pl \\\n",
    "                                if (sid not in test_songset) and (sid in song2artist)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist2index = {aid: ix for ix, aid in enumerate(all_artist_trndev)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Na = len(all_artist_trndev)\n",
    "Np = len(all_playlists)\n",
    "Delta = lil_matrix((Na, Np), dtype=np.float)\n",
    "for j in range(Np):\n",
    "    pl_artist = sorted(set([song2artist[sid] for sid in all_playlists[j][0] \\\n",
    "                            if (sid not in test_songset) and (sid in song2artist)]))\n",
    "    ix = [artist2index[aid] for aid in pl_artist]\n",
    "    Delta[ix, j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Delta = Delta.tocsr()\n",
    "Dsum = Delta.sum(axis=1).A.reshape(-1)\n",
    "ColloMat = Delta.dot(Delta.T).A\n",
    "\n",
    "assert np.all(np.isclose(ColloMat.diagonal(), Dsum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15698 15698\n"
     ]
    }
   ],
   "source": [
    "print(len(Dsum), len(all_artist_trndev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(ColloMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = 1. / np.sqrt(Dsum)\n",
    "NormMat = np.dot(T1.reshape(Na, 1), T1.reshape(1, Na))\n",
    "\n",
    "WeightMat = np.multiply(ColloMat, NormMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84640 / 84646\n",
      "19504 / 84646\n"
     ]
    }
   ],
   "source": [
    "rps_cagh = []\n",
    "hitrates_cagh = {top: [] for top in TOPs}\n",
    "aucs_cagh = []\n",
    "spreads_cagh = []\n",
    "novelties_cagh = {top: dict() for top in TOPs}\n",
    "ptops_cagh = []\n",
    "# diversities_cagh = []\n",
    "# artist_diversities_cagh = {top: [] for top in TOPs}\n",
    "# genre_diversities_cagh = {top: [] for top in TOPs}\n",
    "np.random.seed(0)\n",
    "\n",
    "npos = Y_test.sum(axis=0).A.reshape(-1)\n",
    "assert Y_test.shape[0] == len(test_songs)\n",
    "for j in range(Y_test.shape[1]):\n",
    "    if (j+1) % 10 == 0:\n",
    "        sys.stdout.write('\\r%d / %d' % (j+1, Y_test.shape[1]))\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    if npos[j] < 1:\n",
    "        continue\n",
    "    \n",
    "    y_true = Y_test[:, j].A.reshape(-1)\n",
    "    y_pred = np.zeros(y_true.shape)\n",
    "    \n",
    "    pl = all_playlists[j][0]\n",
    "    artists = set([song2artist[sid] for sid in pl if (sid not in test_songset) and (sid in song2artist)])\n",
    "    assert len(artists) > 0\n",
    "    artists_ix = [artist2index[aid] for aid in artists]\n",
    "    \n",
    "    for ix in range(Y_test.shape[0]):\n",
    "        sid = index2song_test[ix]\n",
    "        if sid in song2artist:\n",
    "            aid = song2artist[sid]\n",
    "            if aid in artist2pop:\n",
    "                aix = artist2index[aid]\n",
    "                y_pred[ix] = np.log(artist2pop[aid]) * WeightMat[aix, artists_ix].sum()\n",
    "\n",
    "    # rp, hr_dict = calc_RPrecision_HitRate(y_true, y_pred, tops=TOPs)\n",
    "    rp, hr_dict, auc = calc_metrics(y_true, y_pred, tops=TOPs)\n",
    "    rps_cagh.append(rp)\n",
    "    for top in TOPs:\n",
    "        hitrates_cagh[top].append(hr_dict[top])\n",
    "    aucs_cagh.append(auc)\n",
    "    \n",
    "    # spread\n",
    "    y_pred_prob = softmax(y_pred)\n",
    "    spreads_cagh.append(-np.dot(y_pred_prob, np.log(y_pred_prob)))\n",
    "\n",
    "    # novelty\n",
    "    sortix = np.argsort(-y_pred)\n",
    "    u = pl2u[j]\n",
    "    for top in TOPs:\n",
    "        nov = np.mean([-np.log2(song2pop[index2song_test[ix]]) for ix in sortix[:top]])\n",
    "        try:\n",
    "            novelties_cagh[top][u].append(nov)\n",
    "        except KeyError:\n",
    "            novelties_cagh[top][u] = [nov]\n",
    "            \n",
    "    # PTop: (#pos ranked above the top-ranked negative) / #pos\n",
    "    assert y_true.dtype == np.bool\n",
    "    negIx = (1 - y_true).astype(np.bool)\n",
    "    negMax = y_pred[negIx].max()\n",
    "    pt = (y_pred[y_true] > negMax).sum() / npos[j]\n",
    "    ptops_cagh.append(pt)\n",
    "    \n",
    "    # compute diversity@100\n",
    "    # csd = 1. / cosine_similarity(X_test[sortix[:100], :])\n",
    "    # dist = pairwise_distance_hamming(Y_test_csr[sortix[:100], :])\n",
    "    # diversities_cagh.append((dist.sum() - np.trace(dist)) / (100 * 99))\n",
    "    \n",
    "    # artist/genre diversity\n",
    "#     for top in TOPs:\n",
    "#         artist_vec = np.array([song2artist[index2song_test[ix]] if index2song_test[ix] in song2artist\n",
    "#                                else str(np.random.rand()) for ix in sortix[:top]])\n",
    "#         genre_vec = np.array([song2genre[index2song_test[ix]] if index2song_test[ix] in song2genre \\\n",
    "#                               else str(np.random.rand()) for ix in sortix[:top]])\n",
    "#         artist_diversities_cagh[top].append( diversity(artist_vec) )\n",
    "#         genre_diversities_cagh[top].append( diversity(genre_vec) )\n",
    "\n",
    "print('\\n%d / %d' % (len(rps_cagh), Y_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=[20, 5])\n",
    "# ax1 = plt.subplot(131)\n",
    "# ax1.hist(rps_cagh, bins=100)\n",
    "# ax1.set_yscale('log')\n",
    "# ax1.set_title('R-Precision')\n",
    "# #ax.set_xlim(0, xmax)\n",
    "# ax2 = plt.subplot(132)\n",
    "# ax2.hist(aucs_cagh, bins=100)\n",
    "# ax2.set_yscale('log')\n",
    "# ax2.set_title('AUC')\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R-Precision': 0.0076308614727239946,\n",
       " 'Hit-Rate': {5: 0.019745139414181663,\n",
       "  10: 0.032550809411417254,\n",
       "  20: 0.053402223775310655,\n",
       "  30: 0.07159239409045987,\n",
       "  50: 0.09603324985392517,\n",
       "  100: 0.14014146034829175,\n",
       "  200: 0.20379345596951262,\n",
       "  300: 0.25486085310600776,\n",
       "  500: 0.3351584031318428,\n",
       "  700: 0.3957267538151583,\n",
       "  1000: 0.46938556781504315},\n",
       " 'AUC': 0.7735410075089875,\n",
       " 'Spread': 4.639159240333002,\n",
       " 'Novelty': {5: -2.1168483464298298,\n",
       "  10: -2.050718107485238,\n",
       "  20: -2.0182549571285233,\n",
       "  30: -2.026902432072022,\n",
       "  50: -2.045289356105412,\n",
       "  100: -2.065246183575041,\n",
       "  200: -2.031182145195714,\n",
       "  300: -1.981334162693092,\n",
       "  500: -1.8882250789762673,\n",
       "  700: -1.815222713507643,\n",
       "  1000: -1.7221702459939532},\n",
       " 'PTop': 0.004011997538966366}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cagh_perf = {dataset_name: {'Test': {'R-Precision': np.mean(rps_cagh), \n",
    "                                     'Hit-Rate': {top: np.mean(hitrates_cagh[top]) for top in hitrates_cagh},\n",
    "                                     'AUC': np.mean(aucs_cagh),\n",
    "                                     'Spread': np.mean(spreads_cagh),\n",
    "                                     'Novelty': {t: np.mean([np.mean(novelties_cagh[t][u]) \n",
    "                                                             for u in novelties_cagh[t]]) for t in TOPs},\n",
    "                                     'PTop': np.mean(ptops_cagh),\n",
    "                                     # 'Artist-Diversity': {t: np.mean(artist_diversities_cagh[t]) for t in TOPs},\n",
    "                                     # 'Genre-Diversity': {t: np.mean(genre_diversities_cagh[t]) for t in TOPs}},\n",
    "                                    },\n",
    "                            'Test_All': {'R-Precision': rps_cagh,\n",
    "                                        'Hit-Rate': {top: hitrates_cagh[top] for top in TOPs},\n",
    "                                        'AUC': aucs_cagh,\n",
    "                                        'Spread': spreads_cagh,\n",
    "                                        'Novelty': novelties_cagh,\n",
    "                                        'PTop': ptops_cagh,\n",
    "                                        # 'Artist-Diversity': artist_diversities_cagh,\n",
    "                                        # 'Genre-Diversity': genre_diversities_cagh},\n",
    "                           }}}\n",
    "cagh_perf[dataset_name]['Test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/aotm2011/coldstart/setting1/perf-cagh.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'R-Precision': 0.0076308614727239946,\n",
       " 'Hit-Rate': {5: 0.019745139414181663,\n",
       "  10: 0.032550809411417254,\n",
       "  20: 0.053402223775310655,\n",
       "  30: 0.07159239409045987,\n",
       "  50: 0.09603324985392517,\n",
       "  100: 0.14014146034829175,\n",
       "  200: 0.20379345596951262,\n",
       "  300: 0.25486085310600776,\n",
       "  500: 0.3351584031318428,\n",
       "  700: 0.3957267538151583,\n",
       "  1000: 0.46938556781504315},\n",
       " 'AUC': 0.7735410075089875,\n",
       " 'Spread': 4.639159240333002,\n",
       " 'Novelty': {5: -2.1168483464298298,\n",
       "  10: -2.050718107485238,\n",
       "  20: -2.0182549571285233,\n",
       "  30: -2.026902432072022,\n",
       "  50: -2.045289356105412,\n",
       "  100: -2.065246183575041,\n",
       "  200: -2.031182145195714,\n",
       "  300: -1.981334162693092,\n",
       "  500: -1.8882250789762673,\n",
       "  700: -1.815222713507643,\n",
       "  1000: -1.7221702459939532},\n",
       " 'PTop': 0.004011997538966366}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fperf_cagh = os.path.join(data_dir, 'perf-cagh.pkl')\n",
    "print(fperf_cagh)\n",
    "pkl.dump(cagh_perf, open(fperf_cagh, 'wb'))\n",
    "pkl.load(open(fperf_cagh, 'rb'))[dataset_name]['Test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $S \\in \\mathbb{R}^{M \\times D}, P \\in \\mathbb{R}^{N \\times D}, Y \\in \\mathbb{R}^{M \\times N}$ be the latent factors of songs and playlists, respectively.\n",
    "\n",
    "The optimisation objective:\n",
    "$\n",
    "\\begin{aligned}\n",
    "J = \\sum_{m=1}^M \\sum_{n=1}^N \\left( y_{m,n} - \\mathbf{s}_m^\\top \\mathbf{p}_n \\right)^2 \n",
    "    + C \\left( \\sum_{m=1}^M \\mathbf{s}_m^\\top \\mathbf{s}_m + \\sum_{n=1}^N \\mathbf{p}_n^\\top \\mathbf{p}_n \\right)\n",
    "\\end{aligned}\n",
    "$  \n",
    "Use alternating least squares optimisation method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fix $S$, then let\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\mathbf{0}\n",
    "= \\frac{\\partial J}{\\partial \\mathbf{p}_n}\n",
    "= \\sum_{m=1}^M 2 \\left( y_{m,n} - \\mathbf{s}_m^\\top \\mathbf{p}_n \\right) (-\\mathbf{s}_m) + 2 C \\mathbf{p}_n\n",
    "\\end{aligned}\n",
    "$  \n",
    "in other words\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\sum_{m=1}^M y_{m,n} \\mathbf{s}_m \n",
    "= \\sum_{m=1}^M (\\mathbf{s}_m^\\top \\mathbf{p}_n^*) \\mathbf{s}_m + C \\mathbf{p}_n^*\n",
    "= \\sum_{m=1}^M \\mathbf{s}_m \\mathbf{s}_m^\\top \\mathbf{p}_n^* + C \\mathbf{p}_n^*\n",
    "= \\left( \\sum_{m=1}^M \\mathbf{s}_m \\mathbf{s}_m^\\top + C \\mathbf{I} \\right) \\mathbf{p}_n^*\n",
    "\\end{aligned}\n",
    "$  \n",
    "where $\\mathbf{I} \\in \\mathbb{R}^{D \\times D}$ diagonal matrix and the every element at diagonal is 1.  \n",
    "So \n",
    "$\n",
    "\\begin{aligned}\n",
    "\\mathbf{p}_n^* = \\left( \\sum_{m=1}^M \\mathbf{s}_m \\mathbf{s}_m^\\top + C \\mathbf{I} \\right)^{-1} \\sum_{m=1}^M y_{m,n} \\mathbf{s}_m\n",
    "\\end{aligned}\n",
    "$  \n",
    "or equivalently\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\mathbf{p}_n^* \n",
    "= \\left( S^\\top S + C \\mathbf{I} \\right)^{-1} \\left( \\mathbf{y}_{:n}^\\top S \\right)^\\top\n",
    "= \\left( S^\\top S + C \\mathbf{I} \\right)^{-1} S^\\top \\mathbf{y}_{:n}\n",
    "\\end{aligned}\n",
    "$  \n",
    "The matrix form is  \n",
    "$\n",
    "\\begin{aligned}\n",
    "P' \n",
    "= \\left( \\left( S^\\top S + C \\mathbf{I} \\right)^{-1} S^\\top Y \\right)^\\top\n",
    "= Y^\\top S \\left( \\left( S^\\top S + C \\mathbf{I} \\right)^{-1} \\right)^\\top\n",
    "\\end{aligned}\n",
    "$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fix $S$, then let\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\mathbf{0}\n",
    "= \\frac{\\partial J}{\\partial \\mathbf{s}_m}\n",
    "= \\sum_{n=1}^N 2 \\left( y_{m,n} - \\mathbf{s}_m^\\top \\mathbf{p}_n \\right) (-\\mathbf{p}_n) + 2 C \\mathbf{s}_m\n",
    "\\end{aligned}\n",
    "$  \n",
    "by symmetry, we have  \n",
    "$\n",
    "\\begin{aligned}\n",
    "\\mathbf{s}_m^* = \\left( \\sum_{n=1}^N \\mathbf{p}_n \\mathbf{p}_n^\\top + C \\mathbf{I} \\right)^{-1} \\sum_{n=1}^N y_{m,n} \\mathbf{p}_n\n",
    "\\end{aligned}\n",
    "$  \n",
    "The matrix form is  \n",
    "$\n",
    "\\begin{aligned}\n",
    "S' \n",
    "= \\left( \\left( P^\\top P + C \\mathbf{I} \\right)^{-1} (Y P)^\\top \\right)^\\top\n",
    "= Y P \\left( \\left( P^\\top P + C \\mathbf{I} \\right)^{-1} \\right)^\\top\n",
    "\\end{aligned}\n",
    "$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P diff: 2909.145644, S diff: 5961.617182\n",
      "P diff: 0.565362, S diff: 2860.630889\n",
      "P diff: 0.276561, S diff: 1418.117794\n",
      "P diff: 0.155529, S diff: 868.197189\n",
      "P diff: 0.102249, S diff: 600.279503\n",
      "P diff: 0.073733, S diff: 446.289770\n",
      "P diff: 0.056381, S diff: 348.081680\n",
      "P diff: 0.044906, S diff: 280.996554\n",
      "P diff: 0.036874, S diff: 232.873005\n",
      "P diff: 0.031009, S diff: 197.038423\n",
      "P diff: 0.026582, S diff: 169.552836\n",
      "P diff: 0.023150, S diff: 147.958115\n",
      "P diff: 0.020430, S diff: 130.650177\n",
      "P diff: 0.018235, S diff: 116.542437\n",
      "P diff: 0.016435, S diff: 104.875589\n",
      "P diff: 0.014939, S diff: 95.104943\n",
      "P diff: 0.013681, S diff: 86.830954\n",
      "P diff: 0.012612, S diff: 79.754914\n",
      "P diff: 0.011695, S diff: 73.649824\n",
      "P diff: 0.010902, S diff: 68.340710\n",
      "P diff: 0.010210, S diff: 63.690998\n",
      "P diff: 0.009604, S diff: 59.592853\n",
      "P diff: 0.009069, S diff: 55.960217\n",
      "P diff: 0.008594, S diff: 52.723700\n",
      "P diff: 0.008170, S diff: 49.826788\n",
      "P diff: 0.007791, S diff: 47.223001\n",
      "P diff: 0.007450, S diff: 44.873749\n",
      "P diff: 0.007142, S diff: 42.746703\n",
      "P diff: 0.006864, S diff: 40.814547\n",
      "P diff: 0.006611, S diff: 39.054027\n",
      "P diff: 0.006381, S diff: 37.445209\n",
      "P diff: 0.006171, S diff: 35.970903\n",
      "P diff: 0.005980, S diff: 34.616210\n",
      "P diff: 0.005804, S diff: 33.368163\n",
      "P diff: 0.005642, S diff: 32.215434\n",
      "P diff: 0.005493, S diff: 31.148099\n",
      "P diff: 0.005356, S diff: 30.157441\n",
      "P diff: 0.005228, S diff: 29.235788\n",
      "P diff: 0.005110, S diff: 28.376373\n",
      "P diff: 0.005000, S diff: 27.573218\n",
      "P diff: 0.004898, S diff: 26.821031\n",
      "P diff: 0.004803, S diff: 26.115120\n",
      "P diff: 0.004713, S diff: 25.451318\n",
      "P diff: 0.004629, S diff: 24.825916\n",
      "P diff: 0.004551, S diff: 24.235604\n",
      "P diff: 0.004476, S diff: 23.677429\n",
      "P diff: 0.004406, S diff: 23.148744\n",
      "P diff: 0.004340, S diff: 22.647173\n",
      "P diff: 0.004278, S diff: 22.170581\n",
      "P diff: 0.004218, S diff: 21.717045\n",
      "P diff: 0.004162, S diff: 21.284827\n",
      "P diff: 0.004109, S diff: 20.872355\n",
      "P diff: 0.004058, S diff: 20.478206\n",
      "P diff: 0.004009, S diff: 20.101085\n",
      "P diff: 0.003962, S diff: 19.739816\n",
      "P diff: 0.003918, S diff: 19.393329\n",
      "P diff: 0.003875, S diff: 19.060646\n",
      "P diff: 0.003834, S diff: 18.740876\n",
      "P diff: 0.003795, S diff: 18.433201\n",
      "P diff: 0.003757, S diff: 18.136877\n",
      "P diff: 0.003721, S diff: 17.851216\n",
      "P diff: 0.003686, S diff: 17.575592\n",
      "P diff: 0.003652, S diff: 17.309428\n",
      "P diff: 0.003620, S diff: 17.052191\n",
      "P diff: 0.003588, S diff: 16.803395\n",
      "P diff: 0.003558, S diff: 16.562587\n",
      "P diff: 0.003528, S diff: 16.329351\n",
      "P diff: 0.003500, S diff: 16.103304\n",
      "P diff: 0.003472, S diff: 15.884088\n",
      "P diff: 0.003446, S diff: 15.671374\n",
      "P diff: 0.003420, S diff: 15.464855\n",
      "P diff: 0.003394, S diff: 15.264244\n",
      "P diff: 0.003370, S diff: 15.069277\n",
      "P diff: 0.003346, S diff: 14.879705\n",
      "P diff: 0.003323, S diff: 14.695294\n",
      "P diff: 0.003300, S diff: 14.515829\n",
      "P diff: 0.003278, S diff: 14.341103\n",
      "P diff: 0.003257, S diff: 14.170925\n",
      "P diff: 0.003236, S diff: 14.005113\n",
      "P diff: 0.003216, S diff: 13.843497\n",
      "P diff: 0.003196, S diff: 13.685914\n",
      "P diff: 0.003177, S diff: 13.532212\n",
      "P diff: 0.003158, S diff: 13.382245\n",
      "P diff: 0.003140, S diff: 13.235875\n",
      "P diff: 0.003122, S diff: 13.092972\n",
      "P diff: 0.003104, S diff: 12.953409\n",
      "P diff: 0.003087, S diff: 12.817067\n",
      "P diff: 0.003071, S diff: 12.683834\n",
      "P diff: 0.003055, S diff: 12.553600\n",
      "P diff: 0.003039, S diff: 12.426262\n",
      "P diff: 0.003023, S diff: 12.301720\n",
      "P diff: 0.003008, S diff: 12.179878\n",
      "P diff: 0.002993, S diff: 12.060647\n",
      "P diff: 0.002979, S diff: 11.943937\n",
      "P diff: 0.002965, S diff: 11.829667\n",
      "P diff: 0.002951, S diff: 11.717754\n",
      "P diff: 0.002937, S diff: 11.608123\n",
      "P diff: 0.002924, S diff: 11.500698\n",
      "P diff: 0.002911, S diff: 11.395410\n",
      "P diff: 0.002898, S diff: 11.292189\n",
      "P diff: 0.002886, S diff: 11.190969\n",
      "P diff: 0.002874, S diff: 11.091689\n",
      "P diff: 0.002862, S diff: 10.994287\n",
      "P diff: 0.002850, S diff: 10.898705\n",
      "P diff: 0.002839, S diff: 10.804887\n",
      "P diff: 0.002827, S diff: 10.712780\n",
      "P diff: 0.002816, S diff: 10.622330\n",
      "P diff: 0.002806, S diff: 10.533490\n",
      "P diff: 0.002795, S diff: 10.446211\n",
      "P diff: 0.002785, S diff: 10.360446\n",
      "P diff: 0.002774, S diff: 10.276152\n",
      "P diff: 0.002764, S diff: 10.193286\n",
      "P diff: 0.002754, S diff: 10.111807\n",
      "P diff: 0.002745, S diff: 10.031677\n",
      "P diff: 0.002735, S diff: 9.952855\n",
      "P diff: 0.002726, S diff: 9.875307\n",
      "P diff: 0.002716, S diff: 9.798998\n",
      "P diff: 0.002707, S diff: 9.723892\n",
      "P diff: 0.002698, S diff: 9.649959\n",
      "P diff: 0.002689, S diff: 9.577166\n",
      "P diff: 0.002681, S diff: 9.505483\n",
      "P diff: 0.002672, S diff: 9.434881\n",
      "P diff: 0.002664, S diff: 9.365332\n",
      "P diff: 0.002655, S diff: 9.296809\n",
      "P diff: 0.002647, S diff: 9.229287\n",
      "P diff: 0.002639, S diff: 9.162739\n",
      "P diff: 0.002631, S diff: 9.097142\n",
      "P diff: 0.002623, S diff: 9.032473\n",
      "P diff: 0.002616, S diff: 8.968708\n",
      "P diff: 0.002608, S diff: 8.905827\n",
      "P diff: 0.002600, S diff: 8.843808\n",
      "P diff: 0.002593, S diff: 8.782630\n",
      "P diff: 0.002586, S diff: 8.722275\n",
      "P diff: 0.002578, S diff: 8.662724\n",
      "P diff: 0.002571, S diff: 8.603958\n",
      "P diff: 0.002564, S diff: 8.545959\n",
      "P diff: 0.002557, S diff: 8.488711\n",
      "P diff: 0.002550, S diff: 8.432197\n",
      "P diff: 0.002543, S diff: 8.376401\n",
      "P diff: 0.002537, S diff: 8.321308\n",
      "P diff: 0.002530, S diff: 8.266903\n",
      "P diff: 0.002523, S diff: 8.213171\n",
      "P diff: 0.002517, S diff: 8.160099\n",
      "P diff: 0.002510, S diff: 8.107672\n",
      "P diff: 0.002504, S diff: 8.055878\n",
      "P diff: 0.002497, S diff: 8.004704\n",
      "P diff: 0.002491, S diff: 7.954137\n",
      "P diff: 0.002485, S diff: 7.904167\n",
      "P diff: 0.002479, S diff: 7.854780\n",
      "P diff: 0.002473, S diff: 7.805966\n",
      "P diff: 0.002467, S diff: 7.757714\n",
      "P diff: 0.002461, S diff: 7.710014\n",
      "P diff: 0.002455, S diff: 7.662854\n",
      "P diff: 0.002449, S diff: 7.616226\n",
      "P diff: 0.002443, S diff: 7.570119\n",
      "P diff: 0.002437, S diff: 7.524524\n",
      "P diff: 0.002432, S diff: 7.479431\n",
      "P diff: 0.002426, S diff: 7.434832\n",
      "P diff: 0.002421, S diff: 7.390718\n",
      "P diff: 0.002415, S diff: 7.347081\n",
      "P diff: 0.002410, S diff: 7.303912\n",
      "P diff: 0.002404, S diff: 7.261204\n",
      "P diff: 0.002399, S diff: 7.218948\n",
      "P diff: 0.002393, S diff: 7.177137\n",
      "P diff: 0.002388, S diff: 7.135764\n",
      "P diff: 0.002383, S diff: 7.094821\n",
      "P diff: 0.002378, S diff: 7.054302\n",
      "P diff: 0.002372, S diff: 7.014198\n",
      "P diff: 0.002367, S diff: 6.974505\n",
      "P diff: 0.002362, S diff: 6.935215\n",
      "P diff: 0.002357, S diff: 6.896321\n",
      "P diff: 0.002352, S diff: 6.857819\n",
      "P diff: 0.002347, S diff: 6.819701\n",
      "P diff: 0.002342, S diff: 6.781961\n",
      "P diff: 0.002337, S diff: 6.744594\n",
      "P diff: 0.002332, S diff: 6.707595\n",
      "P diff: 0.002328, S diff: 6.670957\n",
      "P diff: 0.002323, S diff: 6.634675\n",
      "P diff: 0.002318, S diff: 6.598744\n",
      "P diff: 0.002313, S diff: 6.563159\n",
      "P diff: 0.002309, S diff: 6.527914\n",
      "P diff: 0.002304, S diff: 6.493005\n",
      "P diff: 0.002299, S diff: 6.458427\n",
      "P diff: 0.002295, S diff: 6.424176\n",
      "P diff: 0.002290, S diff: 6.390245\n",
      "P diff: 0.002286, S diff: 6.356632\n",
      "P diff: 0.002281, S diff: 6.323331\n",
      "P diff: 0.002277, S diff: 6.290338\n",
      "P diff: 0.002272, S diff: 6.257648\n",
      "P diff: 0.002268, S diff: 6.225259\n",
      "P diff: 0.002264, S diff: 6.193165\n",
      "P diff: 0.002259, S diff: 6.161362\n",
      "P diff: 0.002255, S diff: 6.129847\n",
      "P diff: 0.002251, S diff: 6.098616\n",
      "P diff: 0.002246, S diff: 6.067664\n",
      "P diff: 0.002242, S diff: 6.036988\n",
      "P diff: 0.002238, S diff: 6.006585\n",
      "P diff: 0.002234, S diff: 5.976451\n",
      "P diff: 0.002230, S diff: 5.946582\n",
      "P diff: 0.002226, S diff: 5.916975\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "D = 300\n",
    "C = 1e-5\n",
    "n_sweeps = 200\n",
    "M, N = Y_trndev.shape\n",
    "S = np.random.rand(M, D)\n",
    "P = np.random.rand(N, D)\n",
    "\n",
    "# alternating least squares\n",
    "for sweep in range(n_sweeps):\n",
    "    # fix S, optimise P\n",
    "    SS = np.dot(S.T, S)  # D by D\n",
    "    np.fill_diagonal(SS, C + SS.diagonal())\n",
    "    P_new = np.dot(Y_trndev.transpose().dot(S), np.linalg.inv(SS).T)  # N by D\n",
    "    pdiff = (P_new - P).ravel()\n",
    "    P = P_new\n",
    "    \n",
    "    # fix P, optimise S\n",
    "    PP = np.dot(P.T, P)  # D by D\n",
    "    np.fill_diagonal(PP, C + PP.diagonal())\n",
    "    S_new = np.dot(Y_trndev.dot(P), np.linalg.inv(PP).T)  # M by D\n",
    "    sdiff = (S_new - S).ravel()\n",
    "    S = S_new\n",
    "    print('P diff: {:8.6f}, S diff: {:8.6f}'.format(np.sqrt(pdiff.dot(pdiff)), np.sqrt(sdiff.dot(sdiff))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check, RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9316430175327752\n"
     ]
    }
   ],
   "source": [
    "loss = 0.\n",
    "Y_trndev_coo = Y_trndev.tocoo()\n",
    "for row, col in zip(Y_trndev_coo.row, Y_trndev_coo.col):\n",
    "    diff = S[row, :].dot(P[col, :]) - 1\n",
    "    loss += diff * diff\n",
    "loss /= Y_trndev_coo.nnz\n",
    "print('RMSE:', np.sqrt(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map song features to song latent factors\n",
    "Learn an MLP to map song features to song latent factors, adapted from [here](https://github.com/francarranza/deep-content-based-music-recommendation/blob/master/src/audioutils/audio_models.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "104428/104428 [==============================] - 2s 17us/step - loss: 0.5309\n",
      "Epoch 2/20\n",
      "104428/104428 [==============================] - 2s 15us/step - loss: 0.4571\n",
      "Epoch 3/20\n",
      "104428/104428 [==============================] - 2s 15us/step - loss: 0.4429\n",
      "Epoch 4/20\n",
      "104428/104428 [==============================] - 1s 14us/step - loss: 0.4382\n",
      "Epoch 5/20\n",
      "104428/104428 [==============================] - 1s 14us/step - loss: 0.4364\n",
      "Epoch 6/20\n",
      "104428/104428 [==============================] - 2s 15us/step - loss: 0.4356\n",
      "Epoch 7/20\n",
      "104428/104428 [==============================] - 1s 14us/step - loss: 0.4351\n",
      "Epoch 8/20\n",
      "104428/104428 [==============================] - 1s 14us/step - loss: 0.4348\n",
      "Epoch 9/20\n",
      "104428/104428 [==============================] - 1s 14us/step - loss: 0.4346\n",
      "Epoch 10/20\n",
      "104428/104428 [==============================] - 1s 14us/step - loss: 0.4345\n",
      "Epoch 11/20\n",
      "104428/104428 [==============================] - 2s 15us/step - loss: 0.4343\n",
      "Epoch 12/20\n",
      "104428/104428 [==============================] - 2s 15us/step - loss: 0.4342\n",
      "Epoch 13/20\n",
      "104428/104428 [==============================] - 2s 15us/step - loss: 0.4341\n",
      "Epoch 14/20\n",
      "104428/104428 [==============================] - 2s 15us/step - loss: 0.4340\n",
      "Epoch 15/20\n",
      "104428/104428 [==============================] - 2s 15us/step - loss: 0.4340\n",
      "Epoch 16/20\n",
      "104428/104428 [==============================] - 2s 15us/step - loss: 0.4339\n",
      "Epoch 17/20\n",
      "104428/104428 [==============================] - 2s 15us/step - loss: 0.4339\n",
      "Epoch 18/20\n",
      "104428/104428 [==============================] - 2s 15us/step - loss: 0.4338\n",
      "Epoch 19/20\n",
      "104428/104428 [==============================] - 2s 15us/step - loss: 0.4337\n",
      "Epoch 20/20\n",
      "104428/104428 [==============================] - 2s 15us/step - loss: 0.4337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f630dcf4128>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.set_random_seed(0)\n",
    "if dataset_name == 'aotm2011':\n",
    "    batch_size = 8192\n",
    "    n_hidden = 512\n",
    "    n_epochs = 20\n",
    "else:\n",
    "    batch_size = 1024\n",
    "    n_hidden = 512\n",
    "    n_epochs = 40\n",
    "    \n",
    "input_shape = (batch_size, X_trndev.shape[1])\n",
    "dimensions = D\n",
    "model = tf.keras.Sequential()\n",
    "model.add(\n",
    "    tf.layers.Dense(\n",
    "        units = n_hidden,\n",
    "        input_dim = X_trndev.shape[1],\n",
    "        activation = 'sigmoid'))\n",
    "model.add(tf.layers.Dense(units = dimensions))\n",
    "model.compile(loss=tf.keras.losses.MSE,\n",
    "              optimizer=tf.keras.optimizers.Adam())\n",
    "model.fit(X_trndev, S, epochs=n_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 300)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_factors = model.predict(X_test, batch_size=X_test.shape[0])\n",
    "X_test_factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84600 / 84646\n",
      "19504 / 84646\n"
     ]
    }
   ],
   "source": [
    "rps_mf = []\n",
    "hitrates_mf = {top: [] for top in TOPs}\n",
    "aucs_mf = []\n",
    "spreads_mf = []\n",
    "novelties_mf = {top: dict() for top in TOPs}\n",
    "ptops_mf = []\n",
    "# artist_diversities_mf = {top: [] for top in TOPs}\n",
    "# genre_diversities_mf = {top: [] for top in TOPs}\n",
    "np.random.seed(0)\n",
    "\n",
    "npos = Y_test.sum(axis=0).A.reshape(-1)\n",
    "assert Y_test.shape[0] == len(test_songs)\n",
    "for j in range(Y_test.shape[1]):\n",
    "    if (j+1) % 100 == 0:\n",
    "        sys.stdout.write('\\r%d / %d' % (j+1, Y_test.shape[1]))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    if npos[j] < 1:\n",
    "        continue\n",
    "        \n",
    "    y_true = Y_test[:, j].A.reshape(-1)\n",
    "    y_pred = np.dot(X_test_factors, P[j])\n",
    "\n",
    "    rp, hr_dict, auc = calc_metrics(y_true, y_pred, tops=TOPs)\n",
    "    rps_mf.append(rp)\n",
    "    for top in TOPs:\n",
    "        hitrates_mf[top].append(hr_dict[top])\n",
    "    aucs_mf.append(auc)\n",
    "    \n",
    "    # spread\n",
    "    y_pred_prob = softmax(y_pred)\n",
    "    spreads_mf.append(-np.dot(y_pred_prob, np.log(y_pred_prob)))\n",
    "\n",
    "    # novelty\n",
    "    sortix = np.argsort(-y_pred)\n",
    "    u = pl2u[j]\n",
    "    for top in TOPs:\n",
    "        nov = np.mean([-np.log2(song2pop[index2song_test[ix]]) for ix in sortix[:top]])\n",
    "        try:\n",
    "            novelties_mf[top][u].append(nov)\n",
    "        except KeyError:\n",
    "            novelties_mf[top][u] = [nov]\n",
    "    \n",
    "    # PTop: (#pos ranked above the top-ranked negative) / #pos\n",
    "    assert y_true.dtype == np.bool\n",
    "    negIx = (1 - y_true).astype(np.bool)\n",
    "    negMax = y_pred[negIx].max()\n",
    "    pt = (y_pred[y_true] > negMax).sum() / npos[j]\n",
    "    ptops_mf.append(pt)\n",
    "    \n",
    "    # artist/genre diversity\n",
    "#     for top in TOPs:\n",
    "#         artist_vec = np.array([song2artist[index2song_test[ix]] if index2song_test[ix] in song2artist\n",
    "#                                else str(np.random.rand()) for ix in sortix[:top]])\n",
    "#         genre_vec = np.array([song2genre[index2song_test[ix]] if index2song_test[ix] in song2genre \\\n",
    "#                               else str(np.random.rand()) for ix in sortix[:top]])\n",
    "#         artist_diversities_mf[top].append( diversity(artist_vec) )\n",
    "#         genre_diversities_mf[top].append( diversity(genre_vec) )\n",
    "    \n",
    "print('\\n%d / %d' % (len(rps_mf), Y_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R-Precision': 0.013819648598503978,\n",
       " 'Hit-Rate': {5: 0.0361792148443299,\n",
       "  10: 0.05733431831459949,\n",
       "  20: 0.08200529549877351,\n",
       "  30: 0.10185805694691621,\n",
       "  50: 0.1320624591211707,\n",
       "  100: 0.18827211540427982,\n",
       "  200: 0.263742497426317,\n",
       "  300: 0.3204958828854446,\n",
       "  500: 0.40568226883987335,\n",
       "  700: 0.4689563076650101,\n",
       "  1000: 0.5429730094039664},\n",
       " 'AUC': 0.8080679628408417,\n",
       " 'Spread': 9.210340347912547,\n",
       " 'Novelty': {5: -1.9418008878206876,\n",
       "  10: -1.9157068575306841,\n",
       "  20: -1.8612707900930598,\n",
       "  30: -1.8409308250506253,\n",
       "  50: -1.8083397728998925,\n",
       "  100: -1.7657656401675872,\n",
       "  200: -1.7227998347117146,\n",
       "  300: -1.6880696790025385,\n",
       "  500: -1.632253757631902,\n",
       "  700: -1.5855595512878395,\n",
       "  1000: -1.5238491811417227},\n",
       " 'PTop': 0.012083296710809016}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_mf = {dataset_name: {'Test': {'R-Precision': np.mean(rps_mf), \n",
    "                                  'Hit-Rate': {top: np.mean(hitrates_mf[top]) for top in TOPs},\n",
    "                                  'AUC': np.mean(aucs_mf),\n",
    "                                  'Spread': np.mean(spreads_mf),\n",
    "                                  'Novelty': {t: np.mean([np.mean(novelties_mf[t][u]) for u in novelties_mf[t]]) \n",
    "                                              for t in TOPs},\n",
    "                                  'PTop': np.mean(ptops_mf),\n",
    "                                  # 'Artist-Diversity': {top: np.mean(artist_diversities_mf[top]) for top in TOPs},\n",
    "                                  # 'Genre-Diversity': {top: np.mean(genre_diversities_mf[top]) for top in TOPs}},\n",
    "                                  },\n",
    "                          'Test_All': {'R-Precision': rps_mf,\n",
    "                                       'Hit-Rate': {top: hitrates_mf[top] for top in TOPs},\n",
    "                                       'AUC': aucs_mf,\n",
    "                                       'Spread': spreads_mf,\n",
    "                                       'Novelty': novelties_mf,\n",
    "                                       'PTop': ptops_mf,\n",
    "                                       # 'Artist-Diversity': artist_diversities_mf,\n",
    "                                       # 'Genre-Diversity': genre_diversities_mf}}}\n",
    "                                      }}}\n",
    "perf_mf[dataset_name]['Test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/aotm2011/coldstart/setting1/perf-mf.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'R-Precision': 0.013819648598503978,\n",
       " 'Hit-Rate': {5: 0.0361792148443299,\n",
       "  10: 0.05733431831459949,\n",
       "  20: 0.08200529549877351,\n",
       "  30: 0.10185805694691621,\n",
       "  50: 0.1320624591211707,\n",
       "  100: 0.18827211540427982,\n",
       "  200: 0.263742497426317,\n",
       "  300: 0.3204958828854446,\n",
       "  500: 0.40568226883987335,\n",
       "  700: 0.4689563076650101,\n",
       "  1000: 0.5429730094039664},\n",
       " 'AUC': 0.8080679628408417,\n",
       " 'Spread': 9.210340347912547,\n",
       " 'Novelty': {5: -1.9418008878206876,\n",
       "  10: -1.9157068575306841,\n",
       "  20: -1.8612707900930598,\n",
       "  30: -1.8409308250506253,\n",
       "  50: -1.8083397728998925,\n",
       "  100: -1.7657656401675872,\n",
       "  200: -1.7227998347117146,\n",
       "  300: -1.6880696790025385,\n",
       "  500: -1.632253757631902,\n",
       "  700: -1.5855595512878395,\n",
       "  1000: -1.5238491811417227},\n",
       " 'PTop': 0.012083296710809016}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fperf_mf = os.path.join(data_dir, 'perf-mf.pkl')\n",
    "print(fperf_mf)\n",
    "pkl.dump(perf_mf, open(fperf_mf, 'wb'))\n",
    "pkl.load(open(fperf_mf, 'rb'))[dataset_name]['Test']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
