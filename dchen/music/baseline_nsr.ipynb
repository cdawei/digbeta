{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines - new song recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os, sys, time, gzip\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix, csr_matrix, issparse\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tools import calc_RPrecision_HitRate\n",
    "from tools import calc_metrics, diversity, pairwise_distance_hamming, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(all='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPs = [5, 10, 20, 30, 50, 100, 200, 300, 500, 700, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['aotm2011', '30music']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'30music'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dix = 1\n",
    "dataset_name = datasets[dix]\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/%s/coldstart/setting1' % dataset_name\n",
    "X_trndev = pkl.load(gzip.open(os.path.join(data_dir, 'X_trndev.pkl.gz'), 'rb'))\n",
    "Y_trndev = pkl.load(gzip.open(os.path.join(data_dir, 'Y_trndev.pkl.gz'), 'rb'))\n",
    "X_test = pkl.load(gzip.open(os.path.join(data_dir, 'X_test.pkl.gz'), 'rb'))\n",
    "Y_test = pkl.load(gzip.open(os.path.join(data_dir, 'Y_test.pkl.gz'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs1 = pkl.load(gzip.open(os.path.join(data_dir, 'songs_train_dev_test_s1.pkl.gz'), 'rb'))\n",
    "train_songs = songs1['train_song_set']\n",
    "dev_songs = songs1['dev_song_set']\n",
    "test_songs = songs1['test_song_set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2index_trndev = {sid: ix for ix, (sid, _) in enumerate(train_songs + dev_songs)}\n",
    "song2index_test = {sid: ix for ix, (sid, _) in enumerate(test_songs)}\n",
    "index2song_test = {ix: sid for ix, (sid, _) in enumerate(test_songs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_song2artist = pkl.load(gzip.open('data/msd/song2artist.pkl.gz', 'rb'))\n",
    "song2artist = {sid: _song2artist[sid] for sid, _ in train_songs + dev_songs + test_songs if sid in _song2artist}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_playlists = pkl.load(gzip.open(os.path.join(data_dir, 'playlists_s1.pkl.gz'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist2pop = dict()\n",
    "test_songset = set(test_songs)\n",
    "\n",
    "for pl, _ in all_playlists:\n",
    "    for sid in [sid for sid in pl if sid not in test_songset]:\n",
    "        if sid in song2artist:\n",
    "            aid = song2artist[sid]\n",
    "            try:\n",
    "                artist2pop[aid] += 1\n",
    "            except KeyError:\n",
    "                artist2pop[aid] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2genre = pkl.load(gzip.open('data/msd/song2genre.pkl.gz', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_genre = set(song2genre.values())\n",
    "# all_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliques_all = pkl.load(gzip.open(os.path.join(data_dir, 'cliques_trndev.pkl.gz'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = len(cliques_all)\n",
    "pl2u = np.zeros(Y_test.shape[1], dtype=np.int32)\n",
    "for u in range(U):\n",
    "    clq = cliques_all[u]\n",
    "    pl2u[clq] = u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_user = np.zeros((Y_test.shape[0], U), dtype=np.int)\n",
    "# for u in range(U):\n",
    "#     clq = cliques_all[u]\n",
    "#     Y_user[:, u] = Y_test[:, clq].sum(axis=1).A.reshape(-1).astype(np.bool).astype(np.int)\n",
    "# Y_user = csr_matrix(Y_user)\n",
    "# print(Y_user.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2pop = pkl.load(gzip.open(os.path.join(data_dir, 'song2pop.pkl.gz'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 63)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 17342)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_test_csr = Y_test.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `p XOR q = ( p AND NOT q )  OR  ( NOT p AND q )` from [here](https://math.stackexchange.com/questions/38473/is-xor-a-combination-of-and-and-not-operators),\n",
    "let $\\mathbf{p}, \\mathbf{q} \\in \\{0, 1\\}^{n}$, then\n",
    "  \n",
    "$\n",
    "\\begin{aligned}\n",
    "& \\text{Hamming_distance}(\\mathbf{p}, \\mathbf{q})  \\\\\n",
    "& = \\frac{1}{n} \\sum_{i=1}^n p_i \\ \\text{XOR} \\ q_i \\\\\n",
    "& = \\frac{1}{n} \\sum_{i=1}^n \\left( p_i (1 - q_i) + (1 - p_i) q_i \\right) \\\\\n",
    "& = \\frac{1}{n} \\left( \\sum_{i=1}^n p_i (1 - q_i) + \\sum_{i=1}^n (1 - p_i) q_i \\right) \\\\\n",
    "& = \\frac{1}{n} \\left( \\mathbf{p}^\\top (\\mathbf{1} - \\mathbf{q}) + (\\mathbf{1} - \\mathbf{p})^\\top \\mathbf{q} \\right) \\\\\n",
    "& = \\frac{1}{n} \\left( \\text{sum}(\\mathbf{p}) + \\text{sum}(\\mathbf{q}) - 2 \\mathbf{p}^\\top \\mathbf{q} \\right)\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.981471727196028e-28\n",
      "4.981471727196028e-28\n"
     ]
    }
   ],
   "source": [
    "N, D = 1000, 200\n",
    "aa = np.zeros(N * D, dtype=np.int)\n",
    "idx = np.random.permutation(N * D)[:int(N * D * .3)]\n",
    "aa[idx] = 1\n",
    "aa = aa.reshape(N, D)\n",
    "d1 = pairwise_distances(aa, metric='hamming', n_jobs=2)\n",
    "d2 = (np.dot(aa, 1-aa.T) + np.dot(1-aa, aa.T)) / D\n",
    "sum_vec = aa.sum(axis=1, keepdims=True)\n",
    "d3 = (sum_vec + sum_vec.T - 2 * np.dot(aa, aa.T)) / D\n",
    "diff = (d1 - d2).ravel()\n",
    "print(np.dot(diff, diff))\n",
    "diff2 = (d1 - d3).ravel()\n",
    "print(np.dot(diff2, diff2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aa = Y_test_csr[:500, :].A\n",
    "# aa = Y_user[:10, :].A\n",
    "# aa_csr = csr_matrix(aa)\n",
    "# t0 = time.time()\n",
    "# d1 = pairwise_distances(aa, metric='hamming', n_jobs=2)\n",
    "# t1 = time.time()\n",
    "# d2 = pairwise_distance_hamming(aa_csr)\n",
    "# t2 = time.time()\n",
    "# diff = (d1 - d2.A).ravel()\n",
    "# print(np.sqrt(np.dot(diff, diff)))\n",
    "# print('%.3f sec, %.3f sec' % (t1 - t0, t2 - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def diversity(vec):\n",
    "#     assert vec.ndim == 1\n",
    "#     norm = len(vec) * (len(vec) - 1)\n",
    "#     sim_mat = vec[..., np.newaxis] == vec[np.newaxis, ...]  # pairwise comparison\n",
    "#     # dist_mat = 1 - sim_mat\n",
    "#     # return (dist_mat.sum() - dist_mat.trace()) / norm  # note that dist_mat.trace() = 0\n",
    "#     return (1 - sim_mat).sum() / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popularity (of artist) based recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[:, 3].A.reshape(-1).dtype == np.bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[:, 3].A.reshape(-1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[:, 3].A.reshape(-1)[(1 - Y_test[:, 3].A.reshape(-1))].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[:, 3].A.reshape(-1)[(1 - Y_test[:, 3].A.reshape(-1)).astype(np.bool)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17300 / 17342\n",
      "8215 / 17342\n"
     ]
    }
   ],
   "source": [
    "rps_pop = []\n",
    "hitrates_pop = {top: [] for top in TOPs}\n",
    "aucs_pop = []\n",
    "spreads_pop = []\n",
    "novelties_pop = {top: dict() for top in TOPs}\n",
    "# diversities_pop = []\n",
    "artist_diversities_pop = {top: [] for top in TOPs}\n",
    "genre_diversities_pop = {top: [] for top in TOPs}\n",
    "ptops_pop = []\n",
    "np.random.seed(0)\n",
    "\n",
    "y_pred = np.zeros(len(test_songs))\n",
    "for ix in range(len(test_songs)):\n",
    "    sid = index2song_test[ix]\n",
    "    if sid in song2artist:\n",
    "        aid = song2artist[sid]\n",
    "        if aid in artist2pop:\n",
    "            y_pred[ix] = np.log(artist2pop[aid])\n",
    "\n",
    "npos = Y_test.sum(axis=0).A.reshape(-1)\n",
    "assert Y_test.shape[0] == len(test_songs)\n",
    "for j in range(Y_test.shape[1]):\n",
    "    if (j+1) % 100 == 0:\n",
    "        sys.stdout.write('\\r%d / %d' % (j+1, Y_test.shape[1]))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    if npos[j] < 1:\n",
    "        continue\n",
    "        \n",
    "    y_true = Y_test[:, j].A.reshape(-1)\n",
    "\n",
    "    # rp, hr_dict = calc_RPrecision_HitRate(y_true, y_pred, tops=TOPs)\n",
    "    rp, hr_dict, auc = calc_metrics(y_true, y_pred, tops=TOPs)\n",
    "    rps_pop.append(rp)\n",
    "    for top in TOPs:\n",
    "        hitrates_pop[top].append(hr_dict[top])\n",
    "    aucs_pop.append(auc)\n",
    "    \n",
    "    # spread\n",
    "    y_pred_prob = softmax(y_pred)\n",
    "    spreads_pop.append(-np.dot(y_pred_prob, np.log(y_pred_prob)))\n",
    "\n",
    "    # novelty\n",
    "    sortix = np.argsort(-y_pred)\n",
    "    u = pl2u[j]\n",
    "    for top in TOPs:\n",
    "        nov = np.mean([-np.log2(song2pop[index2song_test[ix]]) for ix in sortix[:top]])\n",
    "        try:\n",
    "            novelties_pop[top][u].append(nov)\n",
    "        except KeyError:\n",
    "            novelties_pop[top][u] = [nov]\n",
    "    \n",
    "    # PTop: (#pos ranked above the top-ranked negative) / #pos\n",
    "    assert y_true.dtype == np.bool\n",
    "    negIx = (1 - y_true).astype(np.bool)\n",
    "    negMax = y_pred[negIx].max()\n",
    "    pt = (y_pred[y_true] > negMax).sum() / npos[j]\n",
    "    ptops_pop.append(pt)\n",
    "\n",
    "    # compute diversity@100\n",
    "    # sim = cosine_similarity(X_test[sortix[:100], :])\n",
    "    # sim = cosine_similarity(Y_user[sortix[:100], :])\n",
    "    # csd = 1. / cosine_similarity(X_test[sortix[:100], :])\n",
    "    # dist = pairwise_distances(Y_test_csr[sortix[:100], :].A, metric='hamming', n_jobs=4)\n",
    "    # dist = pairwise_distance_hamming(Y_test_csr[sortix[:100], :], normalise=True)\n",
    "    # dist = pairwise_distance_hamming(Y_user[sortix[:50], :], normalise=True)\n",
    "    # div = 100 * 99 / (sim.sum() - sim.trace())\n",
    "    # diversities_pop.append(div)\n",
    "    \n",
    "    # artist/genre diversity\n",
    "#     for top in TOPs:\n",
    "#         artist_vec = np.array([song2artist[index2song_test[ix]] for ix in sortix[:top]])\n",
    "#         genre_vec = np.array([song2genre[index2song_test[ix]] if index2song_test[ix] in song2genre \\\n",
    "#                               else str(np.random.rand()) for ix in sortix[:top]])\n",
    "#         artist_diversities_pop[top].append( diversity(artist_vec) )\n",
    "#         genre_diversities_pop[top].append( diversity(genre_vec) )\n",
    "    \n",
    "print('\\n%d / %d' % (len(rps_pop), Y_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=[20, 5])\n",
    "# ax1 = plt.subplot(131)\n",
    "# ax1.hist(rps_pop, bins=100)\n",
    "# ax1.set_yscale('log')\n",
    "# ax1.set_title('R-Precision')\n",
    "# #ax.set_xlim(0, xmax)\n",
    "# ax2 = plt.subplot(132)\n",
    "# ax2.hist(aucs_pop, bins=100)\n",
    "# ax2.set_yscale('log')\n",
    "# ax2.set_title('AUC')\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R-Precision': 0.005203867316768658,\n",
       " 'Hit-Rate': {5: 0.013328379539578565,\n",
       "  10: 0.01745343146743025,\n",
       "  20: 0.04239344314298478,\n",
       "  30: 0.046325904135701396,\n",
       "  50: 0.06587742420171959,\n",
       "  100: 0.12189305939747051,\n",
       "  200: 0.15982659365799431,\n",
       "  300: 0.23000178915292693,\n",
       "  500: 0.3464156702960598,\n",
       "  700: 0.41762750417701194,\n",
       "  1000: 0.508522684424406},\n",
       " 'AUC': 0.7093575433337442,\n",
       " 'Spread': 7.396529421141267,\n",
       " 'Novelty': {5: -4.631410163543202,\n",
       "  10: -3.8555794509654193,\n",
       "  20: -4.234888556521792,\n",
       "  30: -3.6032449824617605,\n",
       "  50: -3.6311567329369514,\n",
       "  100: -3.795928427450747,\n",
       "  200: -3.1438790996939834,\n",
       "  300: -3.147187786687166,\n",
       "  500: -3.026529370637619,\n",
       "  700: -2.8183193719559148,\n",
       "  1000: -2.5801843454877087},\n",
       " 'PTop': 0.0007100831811726518}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_perf = {dataset_name: {'Test': {'R-Precision': np.mean(rps_pop), \n",
    "                                    'Hit-Rate': {top: np.mean(hitrates_pop[top]) for top in TOPs},\n",
    "                                    'AUC': np.mean(aucs_pop),\n",
    "                                    'Spread': np.mean(spreads_pop),\n",
    "                                    'Novelty': {t: np.mean([np.mean(novelties_pop[t][u]) for u in novelties_pop[t]]) \n",
    "                                                for t in TOPs},\n",
    "                                    'PTop': np.mean(ptops_pop),\n",
    "                                    #'Artist-Diversity': {top: np.mean(artist_diversities_pop[top]) for top in TOPs},\n",
    "                                    #'Genre-Diversity': {top: np.mean(genre_diversities_pop[top]) for top in TOPs}},\n",
    "                                    # 'Novelty': np.mean([np.mean(novelty_pop[u]) for u in novelty_pop]),\n",
    "                                    # 'Diveristy': np.mean(diversities_pop)},\n",
    "                                   },\n",
    "                           'Test_All': {'R-Precision': rps_pop,\n",
    "                                        'Hit-Rate': {top: hitrates_pop[top] for top in TOPs},\n",
    "                                        'AUC': aucs_pop,\n",
    "                                        'Spread': spreads_pop,\n",
    "                                        'Novelty': novelties_pop,\n",
    "                                        'PTop': ptops_pop,\n",
    "                                        #'Artist-Diversity': artist_diversities_pop,\n",
    "                                        #'Genre-Diversity': genre_diversities_pop},\n",
    "                                        # 'Novelty': novelty_pop,\n",
    "                                        # 'Diversity': diversities_pop},\n",
    "                          }}}\n",
    "pop_perf[dataset_name]['Test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/30music/coldstart/setting1/perf-pop.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'R-Precision': 0.005203867316768658,\n",
       " 'Hit-Rate': {5: 0.013328379539578565,\n",
       "  10: 0.01745343146743025,\n",
       "  20: 0.04239344314298478,\n",
       "  30: 0.046325904135701396,\n",
       "  50: 0.06587742420171959,\n",
       "  100: 0.12189305939747051,\n",
       "  200: 0.15982659365799431,\n",
       "  300: 0.23000178915292693,\n",
       "  500: 0.3464156702960598,\n",
       "  700: 0.41762750417701194,\n",
       "  1000: 0.508522684424406},\n",
       " 'AUC': 0.7093575433337442,\n",
       " 'Spread': 7.396529421141267,\n",
       " 'Novelty': {5: -4.631410163543202,\n",
       "  10: -3.8555794509654193,\n",
       "  20: -4.234888556521792,\n",
       "  30: -3.6032449824617605,\n",
       "  50: -3.6311567329369514,\n",
       "  100: -3.795928427450747,\n",
       "  200: -3.1438790996939834,\n",
       "  300: -3.147187786687166,\n",
       "  500: -3.026529370637619,\n",
       "  700: -2.8183193719559148,\n",
       "  1000: -2.5801843454877087},\n",
       " 'PTop': 0.0007100831811726518}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fperf_pop = os.path.join(data_dir, 'perf-pop.pkl')\n",
    "print(fperf_pop)\n",
    "pkl.dump(pop_perf, open(fperf_pop, 'wb'))\n",
    "pkl.load(open(fperf_pop, 'rb'))[dataset_name]['Test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same Artists - Greatest Hits (SAGH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommend according to the popularity of artists in listening history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17300 / 17342\n",
      "8215 / 17342\n"
     ]
    }
   ],
   "source": [
    "rps_sagh = []\n",
    "hitrates_sagh = {top: [] for top in TOPs}\n",
    "aucs_sagh = []\n",
    "spreads_sagh = []\n",
    "novelties_sagh = {top: dict() for top in TOPs}\n",
    "ptops_sagh = []\n",
    "# diversities_sagh = []\n",
    "# artist_diversities_sagh = {top: [] for top in TOPs}\n",
    "# genre_diversities_sagh = {top: [] for top in TOPs}\n",
    "np.random.seed(0)\n",
    "\n",
    "npos = Y_test.sum(axis=0).A.reshape(-1)\n",
    "assert Y_test.shape[0] == len(test_songs)\n",
    "for j in range(Y_test.shape[1]):\n",
    "    if (j+1) % 100 == 0:\n",
    "        sys.stdout.write('\\r%d / %d' % (j+1, Y_test.shape[1]))\n",
    "        sys.stdout.flush()\n",
    "    if npos[j] < 1:\n",
    "        continue\n",
    "\n",
    "    y_true = Y_test[:, j].A.reshape(-1)\n",
    "    y_pred = np.zeros(y_true.shape)\n",
    "    \n",
    "    pl = all_playlists[j][0]\n",
    "    artists = set([song2artist[sid] for sid in pl if (sid not in test_songset) and (sid in song2artist)])\n",
    "    assert len(artists) > 0\n",
    "    \n",
    "    for ix in range(Y_test.shape[0]):\n",
    "        sid = index2song_test[ix]\n",
    "        if sid in song2artist:\n",
    "            aid = song2artist[sid]\n",
    "            if aid in artists and aid in artist2pop:\n",
    "                y_pred[ix] = np.log(artist2pop[aid])\n",
    "    \n",
    "    # rp, hr_dict = calc_RPrecision_HitRate(y_true, y_pred, tops=TOPs)\n",
    "    rp, hr_dict, auc = calc_metrics(y_true, y_pred, tops=TOPs)\n",
    "    rps_sagh.append(rp)\n",
    "    for top in TOPs:\n",
    "        hitrates_sagh[top].append(hr_dict[top])\n",
    "    aucs_sagh.append(auc)\n",
    "    \n",
    "    # spread\n",
    "    y_pred_prob = softmax(y_pred)\n",
    "    spreads_sagh.append(-np.dot(y_pred_prob, np.log(y_pred_prob)))\n",
    "\n",
    "    # novelty\n",
    "    sortix = np.argsort(-y_pred)\n",
    "    u = pl2u[j]\n",
    "    for top in TOPs:\n",
    "        nov = np.mean([-np.log2(song2pop[index2song_test[ix]]) for ix in sortix[:top]])\n",
    "        try:\n",
    "            novelties_sagh[top][u].append(nov)\n",
    "        except KeyError:\n",
    "            novelties_sagh[top][u] = [nov]\n",
    "            \n",
    "    # PTop: (#pos ranked above the top-ranked negative) / #pos\n",
    "    assert y_true.dtype == np.bool\n",
    "    negIx = (1 - y_true).astype(np.bool)\n",
    "    negMax = y_pred[negIx].max()\n",
    "    pt = (y_pred[y_true] > negMax).sum() / npos[j]\n",
    "    ptops_sagh.append(pt)\n",
    "\n",
    "    # compute diversity@100\n",
    "    # csd = 1. / cosine_similarity(X_test[sortix[:100], :])\n",
    "    # dist = pairwise_distance_hamming(Y_test_csr[sortix[:100], :])\n",
    "    # diversities_sagh.append((dist.sum() - np.trace(dist)) / (100 * 99))\n",
    "    \n",
    "    # artist/genre diversity\n",
    "#     for top in TOPs:\n",
    "#         artist_vec = np.array([song2artist[index2song_test[ix]] if index2song_test[ix] in song2artist\n",
    "#                                else str(np.random.rand()) for ix in sortix[:top]])\n",
    "#         genre_vec = np.array([song2genre[index2song_test[ix]] if index2song_test[ix] in song2genre \\\n",
    "#                               else str(np.random.rand()) for ix in sortix[:top]])\n",
    "#         artist_diversities_sagh[top].append( diversity(artist_vec) )\n",
    "#         genre_diversities_sagh[top].append( diversity(genre_vec) )\n",
    "    \n",
    "print('\\n%d / %d' % (len(rps_sagh), Y_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=[20, 5])\n",
    "# ax1 = plt.subplot(131)\n",
    "# ax1.hist(rps_sagh, bins=100)\n",
    "# ax1.set_yscale('log')\n",
    "# ax1.set_title('R-Precision')\n",
    "# #ax.set_xlim(0, xmax)\n",
    "# ax2 = plt.subplot(132)\n",
    "# ax2.hist(aucs_sagh, bins=100)\n",
    "# ax2.set_yscale('log')\n",
    "# ax2.set_title('AUC')\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R-Precision': 0.0058906181071328645,\n",
       " 'Hit-Rate': {5: 0.010129273461055353,\n",
       "  10: 0.01668065892592404,\n",
       "  20: 0.025824710286042635,\n",
       "  30: 0.030388153299415827,\n",
       "  50: 0.03643224096499933,\n",
       "  100: 0.047171460440967976,\n",
       "  200: 0.07414110401376134,\n",
       "  300: 0.10445369660991596,\n",
       "  500: 0.1485900338030378,\n",
       "  700: 0.20310122840124942,\n",
       "  1000: 0.24831610839509155},\n",
       " 'AUC': 0.5154321118343206,\n",
       " 'Spread': 6.475404266745977,\n",
       " 'Novelty': {5: -2.4882684631410146,\n",
       "  10: -2.315565551503914,\n",
       "  20: -1.9936531438312832,\n",
       "  30: -1.7277073859076204,\n",
       "  50: -1.4974086163795883,\n",
       "  100: -1.3835290379979113,\n",
       "  200: -1.3804877846139376,\n",
       "  300: -1.4328591844597078,\n",
       "  500: -1.3908542931429568,\n",
       "  700: -1.3900595827713906,\n",
       "  1000: -1.3518510108122386},\n",
       " 'PTop': 0.0013694461351186854}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagh_perf = {dataset_name: {'Test': {'R-Precision': np.mean(rps_sagh), \n",
    "                                     'Hit-Rate': {top: np.mean(hitrates_sagh[top]) for top in TOPs},\n",
    "                                     'AUC': np.mean(aucs_sagh),\n",
    "                                     'Spread': np.mean(spreads_sagh),\n",
    "                                     'Novelty': {t: np.mean([np.mean(novelties_sagh[t][u]) \n",
    "                                                             for u in novelties_sagh[t]]) for t in TOPs},\n",
    "                                     'PTop': np.mean(ptops_sagh),\n",
    "                                     # 'Artist-Diversity': {t: np.mean(artist_diversities_sagh[t]) for t in TOPs},\n",
    "                                     # 'Genre-Diversity': {t: np.mean(genre_diversities_sagh[t]) for t in TOPs}},\n",
    "                                    },\n",
    "                            'Test_All': {'R-Precision': rps_sagh,\n",
    "                                        'Hit-Rate': {top: hitrates_sagh[top] for top in TOPs},\n",
    "                                        'AUC': aucs_sagh,\n",
    "                                        'Spread': spreads_sagh,\n",
    "                                        'Novelty': novelties_sagh,\n",
    "                                        'PTop': ptops_sagh, \n",
    "                                        # 'Artist-Diversity': artist_diversities_sagh,\n",
    "                                        # 'Genre-Diversity': genre_diversities_sagh},\n",
    "                           }}}\n",
    "sagh_perf[dataset_name]['Test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/30music/coldstart/setting1/perf-sagh.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'R-Precision': 0.0058906181071328645,\n",
       " 'Hit-Rate': {5: 0.010129273461055353,\n",
       "  10: 0.01668065892592404,\n",
       "  20: 0.025824710286042635,\n",
       "  30: 0.030388153299415827,\n",
       "  50: 0.03643224096499933,\n",
       "  100: 0.047171460440967976,\n",
       "  200: 0.07414110401376134,\n",
       "  300: 0.10445369660991596,\n",
       "  500: 0.1485900338030378,\n",
       "  700: 0.20310122840124942,\n",
       "  1000: 0.24831610839509155},\n",
       " 'AUC': 0.5154321118343206,\n",
       " 'Spread': 6.475404266745977,\n",
       " 'Novelty': {5: -2.4882684631410146,\n",
       "  10: -2.315565551503914,\n",
       "  20: -1.9936531438312832,\n",
       "  30: -1.7277073859076204,\n",
       "  50: -1.4974086163795883,\n",
       "  100: -1.3835290379979113,\n",
       "  200: -1.3804877846139376,\n",
       "  300: -1.4328591844597078,\n",
       "  500: -1.3908542931429568,\n",
       "  700: -1.3900595827713906,\n",
       "  1000: -1.3518510108122386},\n",
       " 'PTop': 0.0013694461351186854}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fperf_sagh = os.path.join(data_dir, 'perf-sagh.pkl')\n",
    "print(fperf_sagh)\n",
    "pkl.dump(sagh_perf, open(fperf_sagh, 'wb'))\n",
    "pkl.load(open(fperf_sagh, 'rb'))[dataset_name]['Test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocated Artists - Greatest Hits (CAGH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the similarity of two artist $a_1$ and $a_2$ given a set of playlist $P$:   \n",
    "$$\n",
    "\\text{sim}(a_1, a_2) \n",
    "= \\frac{\\sum_{p \\in P} \\delta(a_1, p) \\times \\delta(a_2, p)}\n",
    "       {\\sqrt{\\sum_{p \\in P} \\delta(a_1, p) \\times \\sum_{p \\in P} \\delta(a_2, p)}}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\delta(a, p) \n",
    "= \\begin{cases}\n",
    "1, \\ \\text{at least one song in playlist $p$ is from artist $a$}, \\\\\n",
    "0, \\ \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommend according to the popularity of songs, but weighted by similarity of (`artist in user's listening history`, `artist of song`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_artist_trndev = sorted(set([song2artist[sid] for pl, _ in all_playlists for sid in pl \\\n",
    "                                if (sid not in test_songset) and (sid in song2artist)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist2index = {aid: ix for ix, aid in enumerate(all_artist_trndev)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Na = len(all_artist_trndev)\n",
    "Np = len(all_playlists)\n",
    "Delta = lil_matrix((Na, Np), dtype=np.float)\n",
    "for j in range(Np):\n",
    "    pl_artist = sorted(set([song2artist[sid] for sid in all_playlists[j][0] \\\n",
    "                            if (sid not in test_songset) and (sid in song2artist)]))\n",
    "    ix = [artist2index[aid] for aid in pl_artist]\n",
    "    Delta[ix, j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Delta = Delta.tocsr()\n",
    "Dsum = Delta.sum(axis=1).A.reshape(-1)\n",
    "ColloMat = Delta.dot(Delta.T).A\n",
    "\n",
    "assert np.all(np.isclose(ColloMat.diagonal(), Dsum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9981 9981\n"
     ]
    }
   ],
   "source": [
    "print(len(Dsum), len(all_artist_trndev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(ColloMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = 1. / np.sqrt(Dsum)\n",
    "NormMat = np.dot(T1.reshape(Na, 1), T1.reshape(1, Na))\n",
    "\n",
    "WeightMat = np.multiply(ColloMat, NormMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17340 / 17342\n",
      "8215 / 17342\n"
     ]
    }
   ],
   "source": [
    "rps_cagh = []\n",
    "hitrates_cagh = {top: [] for top in TOPs}\n",
    "aucs_cagh = []\n",
    "spreads_cagh = []\n",
    "novelties_cagh = {top: dict() for top in TOPs}\n",
    "ptops_cagh = []\n",
    "# diversities_cagh = []\n",
    "# artist_diversities_cagh = {top: [] for top in TOPs}\n",
    "# genre_diversities_cagh = {top: [] for top in TOPs}\n",
    "np.random.seed(0)\n",
    "\n",
    "npos = Y_test.sum(axis=0).A.reshape(-1)\n",
    "assert Y_test.shape[0] == len(test_songs)\n",
    "for j in range(Y_test.shape[1]):\n",
    "    if (j+1) % 10 == 0:\n",
    "        sys.stdout.write('\\r%d / %d' % (j+1, Y_test.shape[1]))\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    if npos[j] < 1:\n",
    "        continue\n",
    "    \n",
    "    y_true = Y_test[:, j].A.reshape(-1)\n",
    "    y_pred = np.zeros(y_true.shape)\n",
    "    \n",
    "    pl = all_playlists[j][0]\n",
    "    artists = set([song2artist[sid] for sid in pl if (sid not in test_songset) and (sid in song2artist)])\n",
    "    assert len(artists) > 0\n",
    "    artists_ix = [artist2index[aid] for aid in artists]\n",
    "    \n",
    "    for ix in range(Y_test.shape[0]):\n",
    "        sid = index2song_test[ix]\n",
    "        if sid in song2artist:\n",
    "            aid = song2artist[sid]\n",
    "            if aid in artist2pop:\n",
    "                aix = artist2index[aid]\n",
    "                y_pred[ix] = np.log(artist2pop[aid]) * WeightMat[aix, artists_ix].sum()\n",
    "\n",
    "    # rp, hr_dict = calc_RPrecision_HitRate(y_true, y_pred, tops=TOPs)\n",
    "    rp, hr_dict, auc = calc_metrics(y_true, y_pred, tops=TOPs)\n",
    "    rps_cagh.append(rp)\n",
    "    for top in TOPs:\n",
    "        hitrates_cagh[top].append(hr_dict[top])\n",
    "    aucs_cagh.append(auc)\n",
    "    \n",
    "    # spread\n",
    "    y_pred_prob = softmax(y_pred)\n",
    "    spreads_cagh.append(-np.dot(y_pred_prob, np.log(y_pred_prob)))\n",
    "\n",
    "    # novelty\n",
    "    sortix = np.argsort(-y_pred)\n",
    "    u = pl2u[j]\n",
    "    for top in TOPs:\n",
    "        nov = np.mean([-np.log2(song2pop[index2song_test[ix]]) for ix in sortix[:top]])\n",
    "        try:\n",
    "            novelties_cagh[top][u].append(nov)\n",
    "        except KeyError:\n",
    "            novelties_cagh[top][u] = [nov]\n",
    "            \n",
    "    # PTop: (#pos ranked above the top-ranked negative) / #pos\n",
    "    assert y_true.dtype == np.bool\n",
    "    negIx = (1 - y_true).astype(np.bool)\n",
    "    negMax = y_pred[negIx].max()\n",
    "    pt = (y_pred[y_true] > negMax).sum() / npos[j]\n",
    "    ptops_cagh.append(pt)\n",
    "    \n",
    "    # compute diversity@100\n",
    "    # csd = 1. / cosine_similarity(X_test[sortix[:100], :])\n",
    "    # dist = pairwise_distance_hamming(Y_test_csr[sortix[:100], :])\n",
    "    # diversities_cagh.append((dist.sum() - np.trace(dist)) / (100 * 99))\n",
    "    \n",
    "    # artist/genre diversity\n",
    "#     for top in TOPs:\n",
    "#         artist_vec = np.array([song2artist[index2song_test[ix]] if index2song_test[ix] in song2artist\n",
    "#                                else str(np.random.rand()) for ix in sortix[:top]])\n",
    "#         genre_vec = np.array([song2genre[index2song_test[ix]] if index2song_test[ix] in song2genre \\\n",
    "#                               else str(np.random.rand()) for ix in sortix[:top]])\n",
    "#         artist_diversities_cagh[top].append( diversity(artist_vec) )\n",
    "#         genre_diversities_cagh[top].append( diversity(genre_vec) )\n",
    "\n",
    "print('\\n%d / %d' % (len(rps_cagh), Y_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=[20, 5])\n",
    "# ax1 = plt.subplot(131)\n",
    "# ax1.hist(rps_cagh, bins=100)\n",
    "# ax1.set_yscale('log')\n",
    "# ax1.set_title('R-Precision')\n",
    "# #ax.set_xlim(0, xmax)\n",
    "# ax2 = plt.subplot(132)\n",
    "# ax2.hist(aucs_cagh, bins=100)\n",
    "# ax2.set_yscale('log')\n",
    "# ax2.set_title('AUC')\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R-Precision': 0.005038845342843542,\n",
       " 'Hit-Rate': {5: 0.00999448010806358,\n",
       "  10: 0.017353031135799384,\n",
       "  20: 0.03031135139228203,\n",
       "  30: 0.04221813023157314,\n",
       "  50: 0.06307996553359844,\n",
       "  100: 0.10042079340913203,\n",
       "  200: 0.15669076805366502,\n",
       "  300: 0.20865694906983764,\n",
       "  500: 0.29832722141527607,\n",
       "  700: 0.3710588823840103,\n",
       "  1000: 0.4659364063758232},\n",
       " 'AUC': 0.6796715502309781,\n",
       " 'Spread': 4.327457616401731,\n",
       " 'Novelty': {5: -2.7667853477798365,\n",
       "  10: -2.74375096212388,\n",
       "  20: -2.7515989542796238,\n",
       "  30: -2.7684845693182765,\n",
       "  50: -2.820895598757543,\n",
       "  100: -2.7984903896907594,\n",
       "  200: -2.708008390820775,\n",
       "  300: -2.647672906157982,\n",
       "  500: -2.543548576791839,\n",
       "  700: -2.446922137748246,\n",
       "  1000: -2.310867507663371},\n",
       " 'PTop': 0.0013694461351186854}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cagh_perf = {dataset_name: {'Test': {'R-Precision': np.mean(rps_cagh), \n",
    "                                     'Hit-Rate': {top: np.mean(hitrates_cagh[top]) for top in hitrates_cagh},\n",
    "                                     'AUC': np.mean(aucs_cagh),\n",
    "                                     'Spread': np.mean(spreads_cagh),\n",
    "                                     'Novelty': {t: np.mean([np.mean(novelties_cagh[t][u]) \n",
    "                                                             for u in novelties_cagh[t]]) for t in TOPs},\n",
    "                                     'PTop': np.mean(ptops_cagh),\n",
    "                                     # 'Artist-Diversity': {t: np.mean(artist_diversities_cagh[t]) for t in TOPs},\n",
    "                                     # 'Genre-Diversity': {t: np.mean(genre_diversities_cagh[t]) for t in TOPs}},\n",
    "                                    },\n",
    "                            'Test_All': {'R-Precision': rps_cagh,\n",
    "                                        'Hit-Rate': {top: hitrates_cagh[top] for top in TOPs},\n",
    "                                        'AUC': aucs_cagh,\n",
    "                                        'Spread': spreads_cagh,\n",
    "                                        'Novelty': novelties_cagh,\n",
    "                                        'PTop': ptops_cagh,\n",
    "                                        # 'Artist-Diversity': artist_diversities_cagh,\n",
    "                                        # 'Genre-Diversity': genre_diversities_cagh},\n",
    "                           }}}\n",
    "cagh_perf[dataset_name]['Test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/30music/coldstart/setting1/perf-cagh.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'R-Precision': 0.005038845342843542,\n",
       " 'Hit-Rate': {5: 0.00999448010806358,\n",
       "  10: 0.017353031135799384,\n",
       "  20: 0.03031135139228203,\n",
       "  30: 0.04221813023157314,\n",
       "  50: 0.06307996553359844,\n",
       "  100: 0.10042079340913203,\n",
       "  200: 0.15669076805366502,\n",
       "  300: 0.20865694906983764,\n",
       "  500: 0.29832722141527607,\n",
       "  700: 0.3710588823840103,\n",
       "  1000: 0.4659364063758232},\n",
       " 'AUC': 0.6796715502309781,\n",
       " 'Spread': 4.327457616401731,\n",
       " 'Novelty': {5: -2.7667853477798365,\n",
       "  10: -2.74375096212388,\n",
       "  20: -2.7515989542796238,\n",
       "  30: -2.7684845693182765,\n",
       "  50: -2.820895598757543,\n",
       "  100: -2.7984903896907594,\n",
       "  200: -2.708008390820775,\n",
       "  300: -2.647672906157982,\n",
       "  500: -2.543548576791839,\n",
       "  700: -2.446922137748246,\n",
       "  1000: -2.310867507663371},\n",
       " 'PTop': 0.0013694461351186854}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fperf_cagh = os.path.join(data_dir, 'perf-cagh.pkl')\n",
    "print(fperf_cagh)\n",
    "pkl.dump(cagh_perf, open(fperf_cagh, 'wb'))\n",
    "pkl.load(open(fperf_cagh, 'rb'))[dataset_name]['Test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $S \\in \\mathbb{R}^{M \\times D}, P \\in \\mathbb{R}^{N \\times D}, Y \\in \\mathbb{R}^{M \\times N}$ be the latent factors of songs and playlists, respectively.\n",
    "\n",
    "The optimisation objective:\n",
    "$\n",
    "\\begin{aligned}\n",
    "J = \\sum_{m=1}^M \\sum_{n=1}^N \\left( y_{m,n} - \\mathbf{s}_m^\\top \\mathbf{p}_n \\right)^2 \n",
    "    + C \\left( \\sum_{m=1}^M \\mathbf{s}_m^\\top \\mathbf{s}_m + \\sum_{n=1}^N \\mathbf{p}_n^\\top \\mathbf{p}_n \\right)\n",
    "\\end{aligned}\n",
    "$  \n",
    "Use alternating least squares optimisation method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fix $S$, then let\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\mathbf{0}\n",
    "= \\frac{\\partial J}{\\partial \\mathbf{p}_n}\n",
    "= \\sum_{m=1}^M 2 \\left( y_{m,n} - \\mathbf{s}_m^\\top \\mathbf{p}_n \\right) (-\\mathbf{s}_m) + 2 C \\mathbf{p}_n\n",
    "\\end{aligned}\n",
    "$  \n",
    "in other words\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\sum_{m=1}^M y_{m,n} \\mathbf{s}_m \n",
    "= \\sum_{m=1}^M (\\mathbf{s}_m^\\top \\mathbf{p}_n^*) \\mathbf{s}_m + C \\mathbf{p}_n^*\n",
    "= \\sum_{m=1}^M \\mathbf{s}_m \\mathbf{s}_m^\\top \\mathbf{p}_n^* + C \\mathbf{p}_n^*\n",
    "= \\left( \\sum_{m=1}^M \\mathbf{s}_m \\mathbf{s}_m^\\top + C \\mathbf{I} \\right) \\mathbf{p}_n^*\n",
    "\\end{aligned}\n",
    "$  \n",
    "where $\\mathbf{I} \\in \\mathbb{R}^{D \\times D}$ diagonal matrix and the every element at diagonal is 1.  \n",
    "So \n",
    "$\n",
    "\\begin{aligned}\n",
    "\\mathbf{p}_n^* = \\left( \\sum_{m=1}^M \\mathbf{s}_m \\mathbf{s}_m^\\top + C \\mathbf{I} \\right)^{-1} \\sum_{m=1}^M y_{m,n} \\mathbf{s}_m\n",
    "\\end{aligned}\n",
    "$  \n",
    "or equivalently\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\mathbf{p}_n^* \n",
    "= \\left( S^\\top S + C \\mathbf{I} \\right)^{-1} \\left( \\mathbf{y}_{:n}^\\top S \\right)^\\top\n",
    "= \\left( S^\\top S + C \\mathbf{I} \\right)^{-1} S^\\top \\mathbf{y}_{:n}\n",
    "\\end{aligned}\n",
    "$  \n",
    "The matrix form is  \n",
    "$\n",
    "\\begin{aligned}\n",
    "P' \n",
    "= \\left( \\left( S^\\top S + C \\mathbf{I} \\right)^{-1} S^\\top Y \\right)^\\top\n",
    "= Y^\\top S \\left( \\left( S^\\top S + C \\mathbf{I} \\right)^{-1} \\right)^\\top\n",
    "\\end{aligned}\n",
    "$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fix $S$, then let\n",
    "$\n",
    "\\begin{aligned}\n",
    "\\mathbf{0}\n",
    "= \\frac{\\partial J}{\\partial \\mathbf{s}_m}\n",
    "= \\sum_{n=1}^N 2 \\left( y_{m,n} - \\mathbf{s}_m^\\top \\mathbf{p}_n \\right) (-\\mathbf{p}_n) + 2 C \\mathbf{s}_m\n",
    "\\end{aligned}\n",
    "$  \n",
    "by symmetry, we have  \n",
    "$\n",
    "\\begin{aligned}\n",
    "\\mathbf{s}_m^* = \\left( \\sum_{n=1}^N \\mathbf{p}_n \\mathbf{p}_n^\\top + C \\mathbf{I} \\right)^{-1} \\sum_{n=1}^N y_{m,n} \\mathbf{p}_n\n",
    "\\end{aligned}\n",
    "$  \n",
    "The matrix form is  \n",
    "$\n",
    "\\begin{aligned}\n",
    "S' \n",
    "= \\left( \\left( P^\\top P + C \\mathbf{I} \\right)^{-1} (Y P)^\\top \\right)^\\top\n",
    "= Y P \\left( \\left( P^\\top P + C \\mathbf{I} \\right)^{-1} \\right)^\\top\n",
    "\\end{aligned}\n",
    "$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P diff: 1316.574078, S diff: 3912.395199\n",
      "P diff: 0.651017, S diff: 1621.246868\n",
      "P diff: 0.309908, S diff: 816.477397\n",
      "P diff: 0.175767, S diff: 503.835563\n",
      "P diff: 0.115235, S diff: 349.528758\n",
      "P diff: 0.082677, S diff: 261.020856\n",
      "P diff: 0.062997, S diff: 204.774430\n",
      "P diff: 0.050064, S diff: 166.307690\n",
      "P diff: 0.041027, S diff: 138.587439\n",
      "P diff: 0.034423, S diff: 117.834307\n",
      "P diff: 0.029433, S diff: 101.835169\n",
      "P diff: 0.025562, S diff: 89.205114\n",
      "P diff: 0.022496, S diff: 79.034587\n",
      "P diff: 0.020022, S diff: 70.704425\n",
      "P diff: 0.017995, S diff: 63.780892\n",
      "P diff: 0.016310, S diff: 57.952586\n",
      "P diff: 0.014892, S diff: 52.991014\n",
      "P diff: 0.013686, S diff: 48.725282\n",
      "P diff: 0.012648, S diff: 45.025419\n",
      "P diff: 0.011749, S diff: 41.791143\n",
      "P diff: 0.010962, S diff: 38.944087\n",
      "P diff: 0.010269, S diff: 36.422305\n",
      "P diff: 0.009655, S diff: 34.176295\n",
      "P diff: 0.009108, S diff: 32.166077\n",
      "P diff: 0.008619, S diff: 30.359023\n",
      "P diff: 0.008179, S diff: 28.728223\n",
      "P diff: 0.007782, S diff: 27.251254\n",
      "P diff: 0.007424, S diff: 25.909258\n",
      "P diff: 0.007098, S diff: 24.686232\n",
      "P diff: 0.006802, S diff: 23.568489\n",
      "P diff: 0.006532, S diff: 22.544247\n",
      "P diff: 0.006285, S diff: 21.603307\n",
      "P diff: 0.006059, S diff: 20.736795\n",
      "P diff: 0.005851, S diff: 19.936962\n",
      "P diff: 0.005660, S diff: 19.197018\n",
      "P diff: 0.005484, S diff: 18.510996\n",
      "P diff: 0.005321, S diff: 17.873637\n",
      "P diff: 0.005171, S diff: 17.280294\n",
      "P diff: 0.005031, S diff: 16.726855\n",
      "P diff: 0.004901, S diff: 16.209668\n",
      "P diff: 0.004780, S diff: 15.725485\n",
      "P diff: 0.004668, S diff: 15.271413\n",
      "P diff: 0.004562, S diff: 14.844864\n",
      "P diff: 0.004463, S diff: 14.443526\n",
      "P diff: 0.004371, S diff: 14.065323\n",
      "P diff: 0.004284, S diff: 13.708389\n",
      "P diff: 0.004202, S diff: 13.371047\n",
      "P diff: 0.004125, S diff: 13.051783\n",
      "P diff: 0.004052, S diff: 12.749231\n",
      "P diff: 0.003983, S diff: 12.462152\n",
      "P diff: 0.003918, S diff: 12.189426\n",
      "P diff: 0.003856, S diff: 11.930035\n",
      "P diff: 0.003798, S diff: 11.683053\n",
      "P diff: 0.003742, S diff: 11.447635\n",
      "P diff: 0.003689, S diff: 11.223012\n",
      "P diff: 0.003639, S diff: 11.008479\n",
      "P diff: 0.003591, S diff: 10.803390\n",
      "P diff: 0.003545, S diff: 10.607152\n",
      "P diff: 0.003501, S diff: 10.419220\n",
      "P diff: 0.003460, S diff: 10.239091\n",
      "P diff: 0.003420, S diff: 10.066301\n",
      "P diff: 0.003381, S diff: 9.900421\n",
      "P diff: 0.003345, S diff: 9.741051\n",
      "P diff: 0.003310, S diff: 9.587824\n",
      "P diff: 0.003276, S diff: 9.440394\n",
      "P diff: 0.003244, S diff: 9.298443\n",
      "P diff: 0.003213, S diff: 9.161671\n",
      "P diff: 0.003183, S diff: 9.029800\n",
      "P diff: 0.003155, S diff: 8.902570\n",
      "P diff: 0.003127, S diff: 8.779736\n",
      "P diff: 0.003101, S diff: 8.661070\n",
      "P diff: 0.003075, S diff: 8.546358\n",
      "P diff: 0.003051, S diff: 8.435398\n",
      "P diff: 0.003027, S diff: 8.328003\n",
      "P diff: 0.003004, S diff: 8.223995\n",
      "P diff: 0.002982, S diff: 8.123208\n",
      "P diff: 0.002961, S diff: 8.025484\n",
      "P diff: 0.002940, S diff: 7.930677\n",
      "P diff: 0.002920, S diff: 7.838648\n",
      "P diff: 0.002901, S diff: 7.749268\n",
      "P diff: 0.002882, S diff: 7.662412\n",
      "P diff: 0.002864, S diff: 7.577965\n",
      "P diff: 0.002846, S diff: 7.495820\n",
      "P diff: 0.002829, S diff: 7.415872\n",
      "P diff: 0.002812, S diff: 7.338027\n",
      "P diff: 0.002796, S diff: 7.262192\n",
      "P diff: 0.002781, S diff: 7.188282\n",
      "P diff: 0.002765, S diff: 7.116216\n",
      "P diff: 0.002750, S diff: 7.045918\n",
      "P diff: 0.002736, S diff: 6.977315\n",
      "P diff: 0.002722, S diff: 6.910340\n",
      "P diff: 0.002708, S diff: 6.844929\n",
      "P diff: 0.002695, S diff: 6.781020\n",
      "P diff: 0.002682, S diff: 6.718557\n",
      "P diff: 0.002669, S diff: 6.657486\n",
      "P diff: 0.002656, S diff: 6.597754\n",
      "P diff: 0.002644, S diff: 6.539313\n",
      "P diff: 0.002632, S diff: 6.482117\n",
      "P diff: 0.002621, S diff: 6.426123\n",
      "P diff: 0.002609, S diff: 6.371288\n",
      "P diff: 0.002598, S diff: 6.317574\n",
      "P diff: 0.002587, S diff: 6.264943\n",
      "P diff: 0.002576, S diff: 6.213359\n",
      "P diff: 0.002566, S diff: 6.162788\n",
      "P diff: 0.002556, S diff: 6.113197\n",
      "P diff: 0.002546, S diff: 6.064557\n",
      "P diff: 0.002536, S diff: 6.016837\n",
      "P diff: 0.002526, S diff: 5.970009\n",
      "P diff: 0.002517, S diff: 5.924047\n",
      "P diff: 0.002508, S diff: 5.878924\n",
      "P diff: 0.002499, S diff: 5.834616\n",
      "P diff: 0.002490, S diff: 5.791098\n",
      "P diff: 0.002481, S diff: 5.748349\n",
      "P diff: 0.002472, S diff: 5.706347\n",
      "P diff: 0.002464, S diff: 5.665070\n",
      "P diff: 0.002456, S diff: 5.624498\n",
      "P diff: 0.002448, S diff: 5.584612\n",
      "P diff: 0.002440, S diff: 5.545394\n",
      "P diff: 0.002432, S diff: 5.506825\n",
      "P diff: 0.002424, S diff: 5.468887\n",
      "P diff: 0.002417, S diff: 5.431565\n",
      "P diff: 0.002409, S diff: 5.394842\n",
      "P diff: 0.002402, S diff: 5.358702\n",
      "P diff: 0.002395, S diff: 5.323130\n",
      "P diff: 0.002388, S diff: 5.288112\n",
      "P diff: 0.002381, S diff: 5.253634\n",
      "P diff: 0.002374, S diff: 5.219682\n",
      "P diff: 0.002368, S diff: 5.186242\n",
      "P diff: 0.002361, S diff: 5.153302\n",
      "P diff: 0.002355, S diff: 5.120850\n",
      "P diff: 0.002348, S diff: 5.088874\n",
      "P diff: 0.002342, S diff: 5.057362\n",
      "P diff: 0.002336, S diff: 5.026302\n",
      "P diff: 0.002330, S diff: 4.995684\n",
      "P diff: 0.002324, S diff: 4.965497\n",
      "P diff: 0.002318, S diff: 4.935731\n",
      "P diff: 0.002312, S diff: 4.906376\n",
      "P diff: 0.002307, S diff: 4.877422\n",
      "P diff: 0.002301, S diff: 4.848860\n",
      "P diff: 0.002296, S diff: 4.820680\n",
      "P diff: 0.002290, S diff: 4.792874\n",
      "P diff: 0.002285, S diff: 4.765433\n",
      "P diff: 0.002280, S diff: 4.738349\n",
      "P diff: 0.002275, S diff: 4.711613\n",
      "P diff: 0.002269, S diff: 4.685218\n",
      "P diff: 0.002264, S diff: 4.659156\n",
      "P diff: 0.002259, S diff: 4.633419\n",
      "P diff: 0.002255, S diff: 4.608000\n",
      "P diff: 0.002250, S diff: 4.582892\n",
      "P diff: 0.002245, S diff: 4.558088\n",
      "P diff: 0.002240, S diff: 4.533581\n",
      "P diff: 0.002236, S diff: 4.509366\n",
      "P diff: 0.002231, S diff: 4.485434\n",
      "P diff: 0.002227, S diff: 4.461781\n",
      "P diff: 0.002222, S diff: 4.438400\n",
      "P diff: 0.002218, S diff: 4.415286\n",
      "P diff: 0.002213, S diff: 4.392432\n",
      "P diff: 0.002209, S diff: 4.369833\n",
      "P diff: 0.002205, S diff: 4.347485\n",
      "P diff: 0.002201, S diff: 4.325380\n",
      "P diff: 0.002196, S diff: 4.303515\n",
      "P diff: 0.002192, S diff: 4.281884\n",
      "P diff: 0.002188, S diff: 4.260482\n",
      "P diff: 0.002184, S diff: 4.239306\n",
      "P diff: 0.002180, S diff: 4.218349\n",
      "P diff: 0.002176, S diff: 4.197608\n",
      "P diff: 0.002172, S diff: 4.177078\n",
      "P diff: 0.002168, S diff: 4.156755\n",
      "P diff: 0.002165, S diff: 4.136635\n",
      "P diff: 0.002161, S diff: 4.116713\n",
      "P diff: 0.002157, S diff: 4.096987\n",
      "P diff: 0.002153, S diff: 4.077451\n",
      "P diff: 0.002150, S diff: 4.058103\n",
      "P diff: 0.002146, S diff: 4.038939\n",
      "P diff: 0.002142, S diff: 4.019954\n",
      "P diff: 0.002139, S diff: 4.001146\n",
      "P diff: 0.002135, S diff: 3.982512\n",
      "P diff: 0.002132, S diff: 3.964047\n",
      "P diff: 0.002128, S diff: 3.945749\n",
      "P diff: 0.002125, S diff: 3.927615\n",
      "P diff: 0.002121, S diff: 3.909642\n",
      "P diff: 0.002118, S diff: 3.891827\n",
      "P diff: 0.002114, S diff: 3.874166\n",
      "P diff: 0.002111, S diff: 3.856658\n",
      "P diff: 0.002107, S diff: 3.839299\n",
      "P diff: 0.002104, S diff: 3.822087\n",
      "P diff: 0.002101, S diff: 3.805019\n",
      "P diff: 0.002097, S diff: 3.788093\n",
      "P diff: 0.002094, S diff: 3.771307\n",
      "P diff: 0.002091, S diff: 3.754657\n",
      "P diff: 0.002088, S diff: 3.738143\n",
      "P diff: 0.002084, S diff: 3.721760\n",
      "P diff: 0.002081, S diff: 3.705508\n",
      "P diff: 0.002078, S diff: 3.689385\n",
      "P diff: 0.002075, S diff: 3.673387\n",
      "P diff: 0.002072, S diff: 3.657514\n",
      "P diff: 0.002068, S diff: 3.641763\n",
      "P diff: 0.002065, S diff: 3.626132\n",
      "P diff: 0.002062, S diff: 3.610620\n",
      "P diff: 0.002059, S diff: 3.595224\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "D = 300\n",
    "C = 1e-5\n",
    "n_sweeps = 200\n",
    "M, N = Y_trndev.shape\n",
    "S = np.random.rand(M, D)\n",
    "P = np.random.rand(N, D)\n",
    "\n",
    "# alternating least squares\n",
    "for sweep in range(n_sweeps):\n",
    "    # fix S, optimise P\n",
    "    SS = np.dot(S.T, S)  # D by D\n",
    "    np.fill_diagonal(SS, C + SS.diagonal())\n",
    "    P_new = np.dot(Y_trndev.transpose().dot(S), np.linalg.inv(SS).T)  # N by D\n",
    "    pdiff = (P_new - P).ravel()\n",
    "    P = P_new\n",
    "    \n",
    "    # fix P, optimise S\n",
    "    PP = np.dot(P.T, P)  # D by D\n",
    "    np.fill_diagonal(PP, C + PP.diagonal())\n",
    "    S_new = np.dot(Y_trndev.dot(P), np.linalg.inv(PP).T)  # M by D\n",
    "    sdiff = (S_new - S).ravel()\n",
    "    S = S_new\n",
    "    print('P diff: {:8.6f}, S diff: {:8.6f}'.format(np.sqrt(pdiff.dot(pdiff)), np.sqrt(sdiff.dot(sdiff))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check, RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8348427075588326\n"
     ]
    }
   ],
   "source": [
    "loss = 0.\n",
    "Y_trndev_coo = Y_trndev.tocoo()\n",
    "for row, col in zip(Y_trndev_coo.row, Y_trndev_coo.col):\n",
    "    diff = S[row, :].dot(P[col, :]) - 1\n",
    "    loss += diff * diff\n",
    "loss /= Y_trndev_coo.nnz\n",
    "print('RMSE:', np.sqrt(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map song features to song latent factors\n",
    "Learn an MLP to map song features to song latent factors, adapted from [here](https://github.com/francarranza/deep-content-based-music-recommendation/blob/master/src/audioutils/audio_models.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "40468/40468 [==============================] - 1s 19us/step - loss: 0.6266\n",
      "Epoch 2/40\n",
      "40468/40468 [==============================] - 1s 14us/step - loss: 0.5827\n",
      "Epoch 3/40\n",
      "40468/40468 [==============================] - 1s 14us/step - loss: 0.5780\n",
      "Epoch 4/40\n",
      "40468/40468 [==============================] - 1s 15us/step - loss: 0.5756\n",
      "Epoch 5/40\n",
      "40468/40468 [==============================] - 1s 15us/step - loss: 0.5739\n",
      "Epoch 6/40\n",
      "40468/40468 [==============================] - 1s 14us/step - loss: 0.5725\n",
      "Epoch 7/40\n",
      "40468/40468 [==============================] - 1s 15us/step - loss: 0.5716\n",
      "Epoch 8/40\n",
      "40468/40468 [==============================] - 1s 14us/step - loss: 0.5706\n",
      "Epoch 9/40\n",
      "40468/40468 [==============================] - 1s 15us/step - loss: 0.5701\n",
      "Epoch 10/40\n",
      "40468/40468 [==============================] - 1s 14us/step - loss: 0.5694\n",
      "Epoch 11/40\n",
      "40468/40468 [==============================] - 1s 15us/step - loss: 0.5689\n",
      "Epoch 12/40\n",
      "40468/40468 [==============================] - 1s 16us/step - loss: 0.5687\n",
      "Epoch 13/40\n",
      "40468/40468 [==============================] - 1s 16us/step - loss: 0.5682\n",
      "Epoch 14/40\n",
      "40468/40468 [==============================] - 1s 16us/step - loss: 0.5679\n",
      "Epoch 15/40\n",
      "40468/40468 [==============================] - 1s 15us/step - loss: 0.5676\n",
      "Epoch 16/40\n",
      "40468/40468 [==============================] - 1s 15us/step - loss: 0.5673\n",
      "Epoch 17/40\n",
      "40468/40468 [==============================] - 1s 15us/step - loss: 0.5671\n",
      "Epoch 18/40\n",
      "40468/40468 [==============================] - 1s 14us/step - loss: 0.5670\n",
      "Epoch 19/40\n",
      "40468/40468 [==============================] - 1s 15us/step - loss: 0.5667\n",
      "Epoch 20/40\n",
      "40468/40468 [==============================] - 1s 16us/step - loss: 0.5664\n",
      "Epoch 21/40\n",
      "40468/40468 [==============================] - 1s 16us/step - loss: 0.5663\n",
      "Epoch 22/40\n",
      "40468/40468 [==============================] - 1s 15us/step - loss: 0.5661\n",
      "Epoch 23/40\n",
      "40468/40468 [==============================] - 1s 15us/step - loss: 0.5660\n",
      "Epoch 24/40\n",
      "40468/40468 [==============================] - 1s 15us/step - loss: 0.5658\n",
      "Epoch 25/40\n",
      "40468/40468 [==============================] - 1s 14us/step - loss: 0.5655\n",
      "Epoch 26/40\n",
      "40468/40468 [==============================] - 1s 15us/step - loss: 0.5651\n",
      "Epoch 27/40\n",
      "40468/40468 [==============================] - 1s 15us/step - loss: 0.5650\n",
      "Epoch 28/40\n",
      "40468/40468 [==============================] - 1s 15us/step - loss: 0.5645\n",
      "Epoch 29/40\n",
      "40468/40468 [==============================] - 1s 16us/step - loss: 0.5643\n",
      "Epoch 30/40\n",
      "40468/40468 [==============================] - 1s 16us/step - loss: 0.5640\n",
      "Epoch 31/40\n",
      "40468/40468 [==============================] - 1s 15us/step - loss: 0.5635\n",
      "Epoch 32/40\n",
      "40468/40468 [==============================] - 1s 15us/step - loss: 0.5632\n",
      "Epoch 33/40\n",
      "40468/40468 [==============================] - 1s 15us/step - loss: 0.5628\n",
      "Epoch 34/40\n",
      "40468/40468 [==============================] - 1s 15us/step - loss: 0.5623\n",
      "Epoch 35/40\n",
      "40468/40468 [==============================] - 1s 16us/step - loss: 0.5618\n",
      "Epoch 36/40\n",
      "40468/40468 [==============================] - 1s 16us/step - loss: 0.5611\n",
      "Epoch 37/40\n",
      "40468/40468 [==============================] - 1s 15us/step - loss: 0.5607\n",
      "Epoch 38/40\n",
      "40468/40468 [==============================] - 1s 16us/step - loss: 0.5602\n",
      "Epoch 39/40\n",
      "40468/40468 [==============================] - 1s 16us/step - loss: 0.5595\n",
      "Epoch 40/40\n",
      "40468/40468 [==============================] - 1s 14us/step - loss: 0.5587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6c8304a6d8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.set_random_seed(0)\n",
    "if dataset_name == 'aotm2011':\n",
    "    batch_size = 8192\n",
    "    n_hidden = 512\n",
    "    n_epochs = 20\n",
    "else:\n",
    "    batch_size = 1024\n",
    "    n_hidden = 512\n",
    "    n_epochs = 40\n",
    "    \n",
    "input_shape = (batch_size, X_trndev.shape[1])\n",
    "dimensions = D\n",
    "model = tf.keras.Sequential()\n",
    "model.add(\n",
    "    tf.layers.Dense(\n",
    "        units = n_hidden,\n",
    "        input_dim = X_trndev.shape[1],\n",
    "        activation = 'sigmoid'))\n",
    "model.add(tf.layers.Dense(units = dimensions))\n",
    "model.compile(loss=tf.keras.losses.MSE,\n",
    "              optimizer=tf.keras.optimizers.Adam())\n",
    "model.fit(X_trndev, S, epochs=n_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 300)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_factors = model.predict(X_test, batch_size=X_test.shape[0])\n",
    "X_test_factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17300 / 17342\n",
      "8215 / 17342\n"
     ]
    }
   ],
   "source": [
    "rps_mf = []\n",
    "hitrates_mf = {top: [] for top in TOPs}\n",
    "aucs_mf = []\n",
    "spreads_mf = []\n",
    "novelties_mf = {top: dict() for top in TOPs}\n",
    "ptops_mf = []\n",
    "# artist_diversities_mf = {top: [] for top in TOPs}\n",
    "# genre_diversities_mf = {top: [] for top in TOPs}\n",
    "np.random.seed(0)\n",
    "\n",
    "npos = Y_test.sum(axis=0).A.reshape(-1)\n",
    "assert Y_test.shape[0] == len(test_songs)\n",
    "for j in range(Y_test.shape[1]):\n",
    "    if (j+1) % 100 == 0:\n",
    "        sys.stdout.write('\\r%d / %d' % (j+1, Y_test.shape[1]))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    if npos[j] < 1:\n",
    "        continue\n",
    "        \n",
    "    y_true = Y_test[:, j].A.reshape(-1)\n",
    "    y_pred = np.dot(X_test_factors, P[j])\n",
    "\n",
    "    rp, hr_dict, auc = calc_metrics(y_true, y_pred, tops=TOPs)\n",
    "    rps_mf.append(rp)\n",
    "    for top in TOPs:\n",
    "        hitrates_mf[top].append(hr_dict[top])\n",
    "    aucs_mf.append(auc)\n",
    "    \n",
    "    # spread\n",
    "    y_pred_prob = softmax(y_pred)\n",
    "    spreads_mf.append(-np.dot(y_pred_prob, np.log(y_pred_prob)))\n",
    "\n",
    "    # novelty\n",
    "    sortix = np.argsort(-y_pred)\n",
    "    u = pl2u[j]\n",
    "    for top in TOPs:\n",
    "        nov = np.mean([-np.log2(song2pop[index2song_test[ix]]) for ix in sortix[:top]])\n",
    "        try:\n",
    "            novelties_mf[top][u].append(nov)\n",
    "        except KeyError:\n",
    "            novelties_mf[top][u] = [nov]\n",
    "    \n",
    "    # PTop: (#pos ranked above the top-ranked negative) / #pos\n",
    "    assert y_true.dtype == np.bool\n",
    "    negIx = (1 - y_true).astype(np.bool)\n",
    "    negMax = y_pred[negIx].max()\n",
    "    pt = (y_pred[y_true] > negMax).sum() / npos[j]\n",
    "    ptops_mf.append(pt)\n",
    "    \n",
    "    # artist/genre diversity\n",
    "#     for top in TOPs:\n",
    "#         artist_vec = np.array([song2artist[index2song_test[ix]] if index2song_test[ix] in song2artist\n",
    "#                                else str(np.random.rand()) for ix in sortix[:top]])\n",
    "#         genre_vec = np.array([song2genre[index2song_test[ix]] if index2song_test[ix] in song2genre \\\n",
    "#                               else str(np.random.rand()) for ix in sortix[:top]])\n",
    "#         artist_diversities_mf[top].append( diversity(artist_vec) )\n",
    "#         genre_diversities_mf[top].append( diversity(genre_vec) )\n",
    "    \n",
    "print('\\n%d / %d' % (len(rps_mf), Y_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R-Precision': 0.038166174531040105,\n",
       " 'Hit-Rate': {5: 0.05960214133651911,\n",
       "  10: 0.08612297379386138,\n",
       "  20: 0.1225691088068307,\n",
       "  30: 0.15236368778496293,\n",
       "  50: 0.19903768753803333,\n",
       "  100: 0.2785863033085597,\n",
       "  200: 0.3830391347053173,\n",
       "  300: 0.4538939740834353,\n",
       "  500: 0.5594879010085065,\n",
       "  700: 0.63103020036896,\n",
       "  1000: 0.7077604622772226},\n",
       " 'AUC': 0.8140231867027435,\n",
       " 'Spread': 8.517191673556496,\n",
       " 'Novelty': {5: -3.1346224230630986,\n",
       "  10: -3.049614741081545,\n",
       "  20: -2.97092830320734,\n",
       "  30: -2.9210127681875586,\n",
       "  50: -2.847655412643849,\n",
       "  100: -2.728113108586504,\n",
       "  200: -2.5778971099149577,\n",
       "  300: -2.472568233532807,\n",
       "  500: -2.3123196370735415,\n",
       "  700: -2.1843251177716825,\n",
       "  1000: -2.0267584321002774},\n",
       " 'PTop': 0.023228090136761654}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_mf = {dataset_name: {'Test': {'R-Precision': np.mean(rps_mf), \n",
    "                                  'Hit-Rate': {top: np.mean(hitrates_mf[top]) for top in TOPs},\n",
    "                                  'AUC': np.mean(aucs_mf),\n",
    "                                  'Spread': np.mean(spreads_mf),\n",
    "                                  'Novelty': {t: np.mean([np.mean(novelties_mf[t][u]) for u in novelties_mf[t]]) \n",
    "                                              for t in TOPs},\n",
    "                                  'PTop': np.mean(ptops_mf),\n",
    "                                  # 'Artist-Diversity': {top: np.mean(artist_diversities_mf[top]) for top in TOPs},\n",
    "                                  # 'Genre-Diversity': {top: np.mean(genre_diversities_mf[top]) for top in TOPs}},\n",
    "                                  },\n",
    "                          'Test_All': {'R-Precision': rps_mf,\n",
    "                                       'Hit-Rate': {top: hitrates_mf[top] for top in TOPs},\n",
    "                                       'AUC': aucs_mf,\n",
    "                                       'Spread': spreads_mf,\n",
    "                                       'Novelty': novelties_mf,\n",
    "                                       'PTop': ptops_mf,\n",
    "                                       # 'Artist-Diversity': artist_diversities_mf,\n",
    "                                       # 'Genre-Diversity': genre_diversities_mf}}}\n",
    "                                      }}}\n",
    "perf_mf[dataset_name]['Test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/30music/coldstart/setting1/perf-mf.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'R-Precision': 0.038166174531040105,\n",
       " 'Hit-Rate': {5: 0.05960214133651911,\n",
       "  10: 0.08612297379386138,\n",
       "  20: 0.1225691088068307,\n",
       "  30: 0.15236368778496293,\n",
       "  50: 0.19903768753803333,\n",
       "  100: 0.2785863033085597,\n",
       "  200: 0.3830391347053173,\n",
       "  300: 0.4538939740834353,\n",
       "  500: 0.5594879010085065,\n",
       "  700: 0.63103020036896,\n",
       "  1000: 0.7077604622772226},\n",
       " 'AUC': 0.8140231867027435,\n",
       " 'Spread': 8.517191673556496,\n",
       " 'Novelty': {5: -3.1346224230630986,\n",
       "  10: -3.049614741081545,\n",
       "  20: -2.97092830320734,\n",
       "  30: -2.9210127681875586,\n",
       "  50: -2.847655412643849,\n",
       "  100: -2.728113108586504,\n",
       "  200: -2.5778971099149577,\n",
       "  300: -2.472568233532807,\n",
       "  500: -2.3123196370735415,\n",
       "  700: -2.1843251177716825,\n",
       "  1000: -2.0267584321002774},\n",
       " 'PTop': 0.023228090136761654}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fperf_mf = os.path.join(data_dir, 'perf-mf.pkl')\n",
    "print(fperf_mf)\n",
    "pkl.dump(perf_mf, open(fperf_mf, 'wb'))\n",
    "pkl.load(open(fperf_mf, 'rb'))[dataset_name]['Test']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
