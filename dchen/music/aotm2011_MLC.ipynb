{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple example of generating playlist by multilable learning (toppush)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, time\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer, label_ranking_loss\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('src')\n",
    "from TopPushMLC import TopPushMLC\n",
    "from evaluate import evaluatePrecision, evalPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "faotm = os.path.join(data_dir, 'aotm-2011/aotm-2011-subset.pkl')\n",
    "fmap  = os.path.join(data_dir, 'aotm-2011/songID2TrackID.pkl')\n",
    "ftag  = os.path.join(data_dir, 'msd/msd_tagtraum_cd2c.cls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load playlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists = pkl.load(open(faotm, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#Playlists: %d' % len(playlists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('#Songs: %d' % len({songID for p in playlists for songID in p['filtered_lists'][0]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lengths = [len(p['filtered_lists'][0]) for p in playlists]\n",
    "lengths = [len(sl) for sl in playlists]\n",
    "plt.hist(lengths, bins=20)\n",
    "print('Average playlist length: %.1f' % np.mean(lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `song_id` --> `track_id` mapping: a song may correspond to multiple tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2TrackID = pkl.load(open(fmap, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{ k : song2TrackID[k] for k in list(song2TrackID.keys())[:10] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load song tags, build `track_id` --> `tag` mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track2Tags = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ftag) as f:\n",
    "    for line in f:\n",
    "        if line[0] == '#': continue\n",
    "        tid, tag = line.strip().split('\\t')\n",
    "        #print(tid, tag)\n",
    "        track2Tags[tid] = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#(Track, Tag): %d' % len(track2Tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{ k : track2Tags[k] for k in list(track2Tags.keys())[:10] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the subset of playlist such that the first song (i.e. the *seed* song) in each playlist has tag(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_ix = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seedSong2Tag = { }\n",
    "for ix in range(len(playlists)):\n",
    "    # the list of song IDs in the playlist\n",
    "    #songIDs = playlists[ix]['filtered_lists'][0]\n",
    "    songIDs = playlists[ix]\n",
    "\n",
    "    # seed song\n",
    "    seedSongID   = songIDs[0]\n",
    "    seedTrackIDs = song2TrackID[seedSongID]\n",
    "    \n",
    "    # a song can have multiple tracks, make sure that at least one track for a song has a tag\n",
    "    flag = [ (trackID in track2Tags) for trackID in seedTrackIDs]\n",
    "    if not np.any(flag):\n",
    "        continue\n",
    "\n",
    "    #seedSong2Tag[playlists[ix]['mix_id']] = [track2Tags[seedTrackIDs[i]] for i in range(len(flag)) if flag[i] is True]\n",
    "    seedSong2Tag[playlists[ix][0]] = [track2Tags[seedTrackIDs[i]] for i in range(len(flag)) if flag[i] is True]\n",
    "\n",
    "    subset_ix.append(ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seedSong2Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists_subset = [playlists[ix] for ix in subset_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#Playlists used: %d' % len(subset_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists_subset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of unique songs, **in multilabel learning, we have a label for each song in this set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_set = sorted({songID for p in playlists_subset for songID in p})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#Songs used: %d' % len(song_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(song_set[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most part, playlists contain less than 10 songs. The most common playlist length is 2 songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_lengths = [len(p) for p in playlists_subset]\n",
    "plt.hist(playlist_lengths, bins=20)\n",
    "print('Average playlist length: %.1f' % np.mean(playlist_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot tag encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicator of tags: `tag` --> `index` mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the set of unique tags\n",
    "tag_set = sorted(set(track2Tags.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#Tags: %d' % len(tag_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_indicator = { tag: ix for ix, tag in enumerate(tag_set) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build features (1-hot encoding of tag) for a song given its `song_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_features(song_id, song2TrackID = song2TrackID, tag_indicator = tag_indicator):\n",
    "    \"\"\"\n",
    "        Generate one-hot feature vector for a given song ID\n",
    "    \"\"\"\n",
    "\n",
    "    features = np.zeros(len(tag_set), dtype = np.float)\n",
    "    trackIDs = song2TrackID[song_id]\n",
    "\n",
    "    cnt = 0\n",
    "    for trackID in trackIDs:\n",
    "        if trackID in track2Tags:\n",
    "            cnt += 1\n",
    "            tag = track2Tags[trackID]\n",
    "            tag_ix = tag_indicator[tag]\n",
    "            features[tag_ix] = 1\n",
    "\n",
    "    # must have at least one tag for the song, else useless\n",
    "    assert(cnt >= 1)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_feature_map(song_id, seed):\n",
    "    \"\"\"\n",
    "        Generate feature mapping for a given (label, query) pair\n",
    "    \"\"\"\n",
    "    \n",
    "    #return gen_features(song_id) - gen_features(seed)  # feature map\n",
    "    return gen_features(seed)  # a trivial feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_training_set(playlists = playlists_subset, song_set = song_set):\n",
    "    \"\"\"\n",
    "        Create the labelled dataset for a given song index\n",
    "        \n",
    "        Input:\n",
    "            - playlists: which playlists to create features for\n",
    "            \n",
    "        Output:\n",
    "            - (Feature, Label) pair (X, Y), with # num playlists rows\n",
    "              X comprises the features for each seed song and the given song\n",
    "              Y comprises the indicators of whether the given song is present in the respective playlist\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(playlists)\n",
    "    D = len(tag_set)\n",
    "    K = len(song_set)\n",
    "\n",
    "    X = np.zeros((N, D), dtype = np.float)\n",
    "    #Y = np.zeros((N, K), dtype = np.int)\n",
    "    Y = coo_matrix(([0], ([0],[0])), shape=(N, K), dtype=np.int8).tolil()\n",
    "    \n",
    "    for i in range(len(playlists)):\n",
    "        playlist = playlists[i]\n",
    "        seed     = playlist[0]\n",
    "\n",
    "        X[i, :] = gen_feature_map(None, seed)\n",
    "        Y[i, :] = [int(sid in playlist) for sid in song_set]\n",
    "\n",
    "    return X, Y.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_feature_map(song_set[100], playlists_subset[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a logistic regression model for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = gen_training_set()\n",
    "# by fixing random seed, the same playlists will be in the test set each time\n",
    "X_train, X_test, Y_train, Y_test = sk.model_selection.train_test_split(X, Y, test_size = 0.33, random_state = 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = OneVsRestClassifier(LogisticRegression(verbose=1))\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(X_train, open(os.path.join(data_dir, 'aotm-2011/XTrain_tag.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(Y_train, open(os.path.join(data_dir, 'aotm-2011/YTrain.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(X_test, open(os.path.join(data_dir, 'aotm-2011/XTest_tag.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(Y_test, open(os.path.join(data_dir, 'aotm-2011/YTest.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(clf, open(os.path.join(data_dir, 'aotm-2011/br-base.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(predictor, X_train, Y_train, X_test, Y_test):\n",
    "    \"\"\"\n",
    "        Compute and save performance results\n",
    "    \"\"\"\n",
    "    p3_train = []\n",
    "    p5_train = []\n",
    "    pk_train = []\n",
    "    p3_test = []\n",
    "    p5_test = []\n",
    "    pk_test = []\n",
    "    rankloss_train = []\n",
    "    rankloss_test = []\n",
    "    \n",
    "    N_train = X_train.shape[0]\n",
    "    batch_size = 200\n",
    "    N_batch_train = int((N_train-1) / batch_size) + 1\n",
    "    for i in range(N_batch_train):\n",
    "        ix0 = i * batch_size\n",
    "        ix1 = min((i+1) * batch_size, N_train)\n",
    "        preds = predictor.decision_function(X_train[ix0:ix1])\n",
    "        evaldict = evaluatePrecision(Y_train[ix0:ix1].toarray(), preds, verbose=-1)\n",
    "        p3_train.append(evaldict['Precision@3'][0])\n",
    "        p5_train.append(evaldict['Precision@5'][0])\n",
    "        pk_train.append(evaldict['Precision@K'][0])\n",
    "        #rankloss_train.append(evalPred1(Y_train[i].toarray()[0], pred, metricType='Ranking'))\n",
    "        sys.stdout.write('\\r%d / %d' % (i+1, N_batch_train)); sys.stdout.flush()\n",
    "    print()\n",
    "    \n",
    "    N_test = X_test.shape[0]\n",
    "    N_batch_test = int((N_test-1) / batch_size) + 1\n",
    "    for i in range(N_batch_test):\n",
    "        ix0 = i * batch_size\n",
    "        ix1 = min((i+1) * batch_size, N_test)\n",
    "        preds = predictor.decision_function(X_test[ix0:ix1])\n",
    "        evaldict = evaluatePrecision(Y_test[ix0:ix1].toarray(), preds, verbose=-1)\n",
    "        p3_test.append(evaldict['Precision@3'][0])\n",
    "        p5_test.append(evaldict['Precision@5'][0])\n",
    "        pk_test.append(evaldict['Precision@K'][0])\n",
    "        #rankloss_test.append(evalPred1(Y_test[i].toarray()[0], pred, metricType='Ranking'))\n",
    "        sys.stdout.write('\\r%d / %d' % (i+1, N_batch_test)); sys.stdout.flush()\n",
    "    print()\n",
    "    \n",
    "    print('Training set:')\n",
    "    print('Precision@3: %.4f' % np.mean(p3_train))\n",
    "    print('Precision@5: %.4f' % np.mean(p5_train))\n",
    "    print('Precision@k: %.4f' % np.mean(pk_train))\n",
    "    print()\n",
    "    print('Test set:')\n",
    "    print('Precision@3: %.4f' % np.mean(p3_test))\n",
    "    print('Precision@5: %.4f' % np.mean(p5_test))\n",
    "    print('Precision@k: %.4f' % np.mean(pk_test))\n",
    "    \n",
    "    #print()\n",
    "    #print('Training set:')\n",
    "    #print('RankingLoss: %.1f, %.1f' % (np.mean(rankloss_train), np.std(rankloss_train) / N_train))\n",
    "    #print()\n",
    "    #print('Test set:')\n",
    "    #print('RankingLoss: %.1f, %.1f' % (np.mean(rankloss_test), np.std(rankloss_test) / N_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(clf, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TopPushMLC(C=3000)\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(clf, X_train, Y_train, X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
