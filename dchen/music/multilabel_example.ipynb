{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple example of  multilabel learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os, sys, time\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import cython\n",
    "import itertools\n",
    "\n",
    "from scipy.io import arff\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import check_grad\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "#yeast_ftrain = os.path.join(data_dir, 'yeast/yeast-train.arff')\n",
    "#yeast_ftest  = os.path.join(data_dir, 'yeast/yeast-test.arff')\n",
    "#bibtex_ftrain = os.path.join(data_dir, 'bibtex/bibtex-train.arff')\n",
    "#bibtex_ftest  = os.path.join(data_dir, 'bibtex/bibtex-test.arff')\n",
    "#fbookmarks = os.path.join(data_dir, 'bookmarks/bookmarks.arff')\n",
    "mm_ftrain = os.path.join(data_dir, 'mediamill/mediamill-train.arff')\n",
    "mm_ftest  = os.path.join(data_dir, 'mediamill/mediamill-test.arff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEED = 123456789"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load yeast dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_train, meta_train = arff.loadarff(yeast_ftrain)\n",
    "#data_train, meta_train = arff.loadarff(open(bibtex_ftrain))\n",
    "#data_bookmarks = arff.loadarff(open(fbookmarks))\n",
    "data_train, meta_train = arff.loadarff(mm_ftrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_test, meta_test = arff.loadarff(yeast_ftest)\n",
    "#data_test, meta_test = arff.loadarff(bibtex_ftest)\n",
    "data_test, meta_test = arff.loadarff(mm_ftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(list(data_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(list(data_train[0])[120:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(list(data_train[0])[:120])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nFeatures = np.array(list(data_train[0])[:-14], dtype=np.float).shape[0]\n",
    "nFeatures = np.array(list(data_train[0])[:120], dtype=np.float).shape[0]\n",
    "print('#features:', nFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.array(list(data_train[0])[:-14], dtype=np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nLabels = np.array(list(data_train[0])[-14:], dtype=np.int).shape[0]\n",
    "nLabels = np.array(list(data_train[0])[120:], dtype=np.int).shape[0]\n",
    "print('#labels:', nLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.array(list(data_train[0])[-14:], dtype=np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#training examples:', len(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#test examples:', len(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of #positive labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nPositives = [np.sum(np.array(list(data_train[ix])[-14:], dtype=np.int)) for ix in range(len(data_train))]\n",
    "nPositives = [np.sum(np.array(list(data_train[ix])[120:], dtype=np.int)) for ix in range(len(data_train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(nPositives).hist(bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset(label_ix, data):\n",
    "    \"\"\"\n",
    "        Create the labelled dataset for a given label index\n",
    "        \n",
    "        Input:\n",
    "            - label_ix: label index, number in { 0, ..., # labels }\n",
    "            - data: original data with features + labels\n",
    "            \n",
    "        Output:\n",
    "            - (Feature, Label) pair (X, y)\n",
    "              X comprises the features for each example\n",
    "              y comprises the labels of the corresponding example\n",
    "    \"\"\"\n",
    "\n",
    "    assert(label_ix >= 0)\n",
    "    assert(label_ix < nLabels)\n",
    "\n",
    "    N = len(data)\n",
    "    d = nFeatures\n",
    "    \n",
    "    #magic = -14\n",
    "    magic = 120\n",
    "\n",
    "    X = np.zeros((N, d), dtype = np.float)\n",
    "    y = np.zeros(N, dtype = np.int)\n",
    "       \n",
    "    for i in range(N):\n",
    "        X[i, :] = list(data[i])[:magic]\n",
    "        y[i]    = list(data[i])[magic:][label_ix]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset_v2(data):\n",
    "    \"\"\"\n",
    "        Create the labelled dataset for a given label index\n",
    "        \n",
    "        Input:\n",
    "            - data: original data with features + labels\n",
    "            \n",
    "        Output:\n",
    "            - (Feature, Label) pair (X, y)\n",
    "              X comprises the features for each example\n",
    "              Y comprises the labels of the corresponding example\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(data)\n",
    "    D = nFeatures\n",
    "    L = nLabels\n",
    "\n",
    "    #magic = -14\n",
    "    magic = 120\n",
    "\n",
    "    X = np.zeros((N, D), dtype = np.float)\n",
    "    Y = np.zeros((N, L), dtype = np.int)\n",
    "       \n",
    "    for i in range(N):\n",
    "        X[i, :] = list(data[i])[:magic]\n",
    "        Y[i, :] = list(data[i])[magic:]\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss between a ground truth and a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalPred(truth, pred, lossType = 'Hamming'):\n",
    "    \"\"\"\n",
    "        Compute loss given ground truth and prediction\n",
    "        \n",
    "        Input:\n",
    "            - truth:    binary array of true labels\n",
    "            - pred:     real-valued array of predictions\n",
    "            - lossType: can be subset 0-1, Hamming, ranking, and Precision@K where K = # positive labels.\n",
    "    \"\"\"\n",
    "\n",
    "    assert(len(truth) == len(pred))\n",
    "    L = len(truth)\n",
    "    nPos = np.sum(truth)\n",
    "    \n",
    "    predBin = np.array((pred > 0), dtype=np.int)\n",
    "    \n",
    "    if lossType == 'Subset01':\n",
    "        return 1 - int(np.all(truth == predBin))\n",
    "    \n",
    "    elif lossType == 'Hamming':\n",
    "        return np.sum(truth != predBin) / L\n",
    "    \n",
    "    elif lossType == 'Ranking':\n",
    "        loss = 0\n",
    "        for i in range(L-1):\n",
    "            for j in range(i+1, L):\n",
    "                if truth[i] > truth[j]:\n",
    "                    if pred[i] < pred[j]: \n",
    "                        loss += 1\n",
    "                    if pred[i] == pred[j]:\n",
    "                        loss += 0.5\n",
    "        #return loss / (nPos * (L-nPos))\n",
    "        return loss\n",
    "        \n",
    "    elif lossType == 'Precision@K':\n",
    "        # sorted indices of the labels most likely to be +'ve\n",
    "        idx = np.argsort(pred)[::-1]\n",
    "        \n",
    "        # true labels according to the sorted order\n",
    "        y = truth[idx]\n",
    "        \n",
    "        # fraction of +'ves in the top K predictions\n",
    "        return np.mean(y[:nPos])if nPos > 0 else 0\n",
    "    \n",
    "    elif lossType == 'Precision@3':\n",
    "        # sorted indices of the labels most likely to be +'ve\n",
    "        idx = np.argsort(pred)[::-1]\n",
    "        \n",
    "        # true labels according to the sorted order\n",
    "        y = truth[idx]\n",
    "        \n",
    "        # fraction of +'ves in the top K predictions\n",
    "        return np.mean(y[:3])\n",
    "    \n",
    "    elif lossType == 'Precision@5':\n",
    "        # sorted indices of the labels most likely to be +'ve\n",
    "        idx = np.argsort(pred)[::-1]\n",
    "        \n",
    "        # true labels according to the sorted order\n",
    "        y = truth[idx]\n",
    "        \n",
    "        # fraction of +'ves in the top K predictions\n",
    "        return np.mean(y[:5])\n",
    "    \n",
    "    else:\n",
    "        assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avgPrecisionK(allTruths, allPreds):\n",
    "    losses = []\n",
    "    lossType = 'Precision@K'\n",
    "    for i in range(allPreds.shape[0]):\n",
    "        pred  = allPreds[i, :]\n",
    "        truth = allTruths[i, :]\n",
    "        losses.append(evalPred(truth, pred, lossType))\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printEvaluation(allTruths, allPreds):\n",
    "    \n",
    "    N = allTruths.shape[0]\n",
    "    print(N)\n",
    "\n",
    "    for lossType in ['Precision@K']: \n",
    "        # ['Subset01', 'Hamming', 'Ranking', 'Precision@K', 'Precision@3', 'Precision@5']:\n",
    "        losses = [ ]\n",
    "        for i in range(allPreds.shape[0]):\n",
    "            pred  = allPreds[i, :]\n",
    "            truth = allTruths[i, :]\n",
    "            losses.append(evalPred(truth, pred, lossType))\n",
    "\n",
    "            #print(allPreds[i])\n",
    "            #print(pred)\n",
    "            #print(truth)\n",
    "            #break\n",
    "\n",
    "        #print('%24s: %1.4f' % ('Average %s Loss' % lossType, np.mean(losses)))\n",
    "        print('%s: %1.4f, %.3f' % ('Average %s' % lossType, np.mean(losses), np.std(losses) / np.sqrt(N)))\n",
    "        #plt.hist(aucs, bins = 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary relevance baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a logistic regression model for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers = [ LogisticRegression(class_weight = 'balanced', C = 10**0) for i in range(nLabels) ]\n",
    "#classifiers = [ LogisticRegression(class_weight = 'balanced', C = 10) for i in range(nLabels) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allPreds_train  = [ ]\n",
    "allPreds_test  = [ ]\n",
    "allTruths_train = [ ]\n",
    "allTruths_test = [ ]\n",
    "coefMat = [ ]\n",
    "labelIndices = [ ]\n",
    "\n",
    "for label_ix in range(nLabels):\n",
    "    print('Training for Label %d' % (label_ix+1))\n",
    "    X_train, y_train = create_dataset(label_ix, data = data_train)\n",
    "    X_test, y_test   = create_dataset(label_ix, data = data_test)\n",
    "    \n",
    "    allTruths_train.append(y_train)\n",
    "    allTruths_test.append(y_test) \n",
    "    \n",
    "    assert( (not np.all(y_train == 0)) and (not np.all(y_train == 1)) )\n",
    "\n",
    "    classifiers[label_ix].fit(X_train, y_train)\n",
    "    allPreds_train.append(classifiers[label_ix].decision_function(X_train))\n",
    "    allPreds_test.append(classifiers[label_ix].decision_function(X_test))\n",
    "\n",
    "    coefMat.append(classifiers[label_ix].coef_.reshape(-1))\n",
    "    #labelIndices.append(label_ix)\n",
    "    #print(classifiers[label_ix].coef_)\n",
    "    #print(classifiers[label_ix].intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTruths_train = np.array(allTruths_train).T\n",
    "allTruths_test = np.array(allTruths_test).T\n",
    "\n",
    "allPreds_train  = np.array(allPreds_train).T\n",
    "allPreds_test  = np.array(allPreds_test).T\n",
    "\n",
    "\n",
    "print(allPreds_test.shape)\n",
    "print(allTruths_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#allPreds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set:')\n",
    "printEvaluation(allTruths_train, allPreds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test set:')\n",
    "printEvaluation(allTruths_test, allPreds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coefficient matrix `(#Genres, #Songs)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefMat = np.array(coefMat).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefMat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sns.heatmap(coefMat[:, :30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary relevance with exponential loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a regression model with exponential loss for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obj_exp(w, X, y, C):\n",
    "    \"\"\"\n",
    "        Objective with L2 regularisation and exponential loss\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector\n",
    "            - X: feature matrix, N x D\n",
    "            - y: label vector,   N x 1\n",
    "            - C: regularisation constant\n",
    "    \"\"\"\n",
    "    assert(len(y) == X.shape[0])\n",
    "    assert(len(w) == X.shape[1])\n",
    "    assert(C >= 0)\n",
    "    \n",
    "    N, D = X.shape\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    g = np.zeros_like(w)  # gradient\n",
    "    \n",
    "    for n in range(N):\n",
    "        x = X[n, :]\n",
    "        prod = np.dot(w, x)\n",
    "        \n",
    "        # negative label\n",
    "        if y[n] == 0:\n",
    "            t1 = np.exp(prod)\n",
    "            J += t1\n",
    "            g = g + t1 * x\n",
    "        \n",
    "        # positive label\n",
    "        else:\n",
    "            t2 = np.exp(-prod)\n",
    "            J += t2\n",
    "            g = g - t2 * x\n",
    "    \n",
    "    J = 0.5 * C * np.dot(w, w) + J / N\n",
    "    g = C * w + g / N\n",
    "    \n",
    "    return (J, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%script false\n",
    "X_train_, y_train_ = create_dataset(3, data = data_train)\n",
    "w0 = np.random.rand(X_train_.shape[1])\n",
    "C = 1\n",
    "check_grad(lambda w: obj_exp(w, X_train_, y_train_, C)[0], \\\n",
    "           lambda w: obj_exp(w, X_train_, y_train_, C)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params    = [ ]\n",
    "allPreds_train  = [ ]\n",
    "allPreds_test  = [ ]\n",
    "allTruths_train = [ ]\n",
    "allTruths_test = [ ]\n",
    "np.random.seed(SEED)\n",
    "C = 1\n",
    "\n",
    "for label_ix in range(nLabels):\n",
    "    #sys.stdout.write('\\r%d / %d' % (label_ix + 1, nLabels))\n",
    "    #sys.stdout.flush()\n",
    "    print('\\r%d / %d ' % (label_ix + 1, nLabels))\n",
    "    \n",
    "    X_train, y_train = create_dataset(label_ix, data = data_train)\n",
    "    X_test, y_test   = create_dataset(label_ix, data = data_test)\n",
    "    \n",
    "    allTruths_train.append(y_train)\n",
    "    allTruths_test.append(y_test)\n",
    "    \n",
    "    assert( (not np.all(y_train == 0)) and (not np.all(y_train == 1)) )\n",
    "        \n",
    "    opt_method = 'BFGS' #'Newton-CG' \n",
    "    #opt_method = 'nelder-mead'\n",
    "    options = {'disp': True}\n",
    "    \n",
    "    w0 = np.random.rand(X_train.shape[1])  # initial guess\n",
    "    opt = minimize(obj_exp, w0, args=(X_train, y_train, C), method=opt_method, jac=True, options=options)\n",
    "    \n",
    "    if opt.success == True:\n",
    "        w = opt.x\n",
    "        params.append(w)\n",
    "        #allPreds.append(sigmoid(np.dot(X_test, w)))\n",
    "        allPreds_train.append(np.dot(X_train, w))\n",
    "        allPreds_test.append(np.dot(X_test, w))\n",
    "    else:\n",
    "        sys.stderr.write('Optimisation failed, label_ix=%d\\n' % label_ix)\n",
    "        w = np.zeros(X_train.shape[1])\n",
    "        params.append(w)\n",
    "        #allPreds_test.append(np.dot(X_test, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allTruths_train = np.array(allTruths_train).T\n",
    "allTruths_test = np.array(allTruths_test).T\n",
    "\n",
    "allPreds_train  = np.array(allPreds_train).T\n",
    "allPreds_test  = np.array(allPreds_test).T\n",
    "\n",
    "print(allPreds_test.shape)\n",
    "print(allTruths_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#allPreds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Training set:')\n",
    "printEvaluation(allTruths_train, allPreds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Test set:')\n",
    "printEvaluation(allTruths_test, allPreds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary relevance with bipartite ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a bipartite ranking model for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%cython -a\n",
    "\n",
    "import numpy as np\n",
    "#cimport numpy as np\n",
    "\n",
    "#cpdef obj_biranking(w, X, y):\n",
    "\n",
    "def obj_biranking(w, X, y, C):\n",
    "    \"\"\"\n",
    "        Objective with L2 regularisation and bipartite ranking loss\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector\n",
    "            - X: feature matrix, N x D\n",
    "            - y: label vector,   N x 1\n",
    "            - C: regularisation constant\n",
    "    \"\"\"\n",
    "    assert(len(y) == X.shape[0])\n",
    "    assert(len(w) == X.shape[1])\n",
    "    assert(C >= 0)\n",
    "\n",
    "    #cdef int nPos, nNeg, i, j\n",
    "    #cdef double J, term, denom\n",
    "    nPos = np.sum(y)      # num of positive examples\n",
    "    nNeg = len(y) - nPos  # num of negative examples\n",
    "    \n",
    "    ixPos = np.nonzero(y)[0].tolist()                    # indices positive examples\n",
    "    ixNeg = list(set(np.arange(len(y))) - set(ixPos))    # indices negative examples\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    g = np.zeros_like(w)  # gradient    \n",
    "\n",
    "    scorePos = X[ixPos, :].dot(w)[:,np.newaxis] # nPos x 1\n",
    "    scoreNeg = X[ixNeg, :].dot(w)[:,np.newaxis] # nNeg x 1\n",
    "    scoreDif = scorePos - scoreNeg.T            # nPos x nNeg\n",
    "    #J = np.mean(np.log(1 + np.exp(-scoreDif)))\n",
    "    J = 0.5 * C * np.dot(w, w) + np.mean(np.log1p(np.exp(-scoreDif)))\n",
    "    \n",
    "    A = -1/(1 + np.exp(scoreDif))\n",
    "\n",
    "    T1 = X[ixPos, :].T.dot(A.sum(axis = 1))\n",
    "    T2 = X[ixNeg, :].T.dot(A.sum(axis = 0))\n",
    "    g  = C * w + 1/(nPos * nNeg) * (T1 - T2)\n",
    "    \n",
    "    return (J, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_, y_train_ = create_dataset(6, data = data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%script false\n",
    "w0 = w = np.random.rand(X_train_.shape[1])\n",
    "C = 1\n",
    "check_grad(lambda w: obj_biranking(w, X_train_, y_train_, C)[0], \\\n",
    "           lambda w: obj_biranking(w, X_train_, y_train_, C)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.1331503772158218e-06 * np.sqrt(nLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params    = [ ]\n",
    "allPreds_train  = [ ]\n",
    "allPreds_test  = [ ]\n",
    "allTruths_train = [ ]\n",
    "allTruths_test = [ ]\n",
    "np.random.seed(SEED)\n",
    "C = 1\n",
    "\n",
    "for label_ix in range(nLabels):\n",
    "    #sys.stdout.write('\\r%d / %d' % (label_ix + 1, nLabels))\n",
    "    #sys.stdout.flush()\n",
    "    print('\\r%d / %d ' % (label_ix + 1, nLabels))\n",
    "    \n",
    "    X_train, y_train = create_dataset(label_ix, data = data_train)\n",
    "    X_test, y_test   = create_dataset(label_ix, data = data_test)\n",
    "    \n",
    "    allTruths_train.append(y_train)\n",
    "    allTruths_test.append(y_test)\n",
    "    \n",
    "    assert( (not np.all(y_train == 0)) and (not np.all(y_train == 1)) )\n",
    "        \n",
    "    opt_method = 'BFGS' #'Newton-CG' \n",
    "    #opt_method = 'nelder-mead'\n",
    "    options = {'disp': True}\n",
    "    \n",
    "    w0 = np.random.rand(X_train.shape[1])  # initial guess\n",
    "    opt = minimize(obj_biranking, w0, args=(X_train, y_train, C), method=opt_method, jac=True, options=options)\n",
    "    \n",
    "    if opt.success == True:\n",
    "        w = opt.x\n",
    "        params.append(w)\n",
    "        #allPreds.append(sigmoid(np.dot(X_test, w)))\n",
    "        allPreds_train.append(np.dot(X_train, w))\n",
    "        allPreds_test.append(np.dot(X_test, w))\n",
    "    else:\n",
    "        sys.stderr.write('Optimisation failed, label_ix=%d\\n' % label_ix)\n",
    "        w = np.zeros(X_train.shape[1])\n",
    "        params.append(w)\n",
    "        allPreds_test.append(np.dot(X_test, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allTruths_train = np.array(allTruths_train).T\n",
    "allTruths_test = np.array(allTruths_test).T\n",
    "\n",
    "allPreds_train  = np.array(allPreds_train).T\n",
    "allPreds_test  = np.array(allPreds_test).T\n",
    "\n",
    "print(allPreds_test.shape)\n",
    "print(allTruths_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#allPreds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Training set:')\n",
    "printEvaluation(allTruths_train, allPreds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Test set:')\n",
    "printEvaluation(allTruths_test, allPreds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-label learning with ranking loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obj_ranking_loop(w, X, Y, C):\n",
    "    \"\"\"\n",
    "        Objective with L2 regularisation and ranking loss\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x L\n",
    "            - C: regularisation constant\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    L = Y.shape[1]\n",
    "    assert(w.shape[0] == L * D)\n",
    "    \n",
    "    W = w.reshape(L, D)  # reshape weight matrix    \n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    for n in range(N):\n",
    "        Jn = 0.0\n",
    "        Gn = np.zeros_like(W)\n",
    "        x = X[n, :]\n",
    "        y = Y[n, :]\n",
    "        nPos = np.sum(y)   # num of positive examples\n",
    "        nNeg = L - nPos    # num of negative examples\n",
    "        denom = nPos * nNeg\n",
    "        \n",
    "        ixPos = np.nonzero(y)[0].tolist()               # indices positive examples\n",
    "        ixNeg = list(set(np.arange(L)) - set(ixPos))    # indices negative examples\n",
    "        \n",
    "        for i in ixPos:\n",
    "            for j in ixNeg:\n",
    "                wDiff = W[i, :] - W[j, :]\n",
    "                sDiff = np.dot(wDiff, x)\n",
    "                term = np.exp(sDiff)\n",
    "                Jn += np.log1p(1.0 / term)\n",
    "                Gn[i, :] = Gn[i, :] - x / (1 + term)        \n",
    "        #for j in ixNeg:\n",
    "        #    for i in ixPos:\n",
    "        #        wDiff = W[i, :] - W[j, :]\n",
    "        #        sDiff = np.dot(wDiff, x)\n",
    "        #        term = np.exp(sDiff)\n",
    "                Gn[j, :] = Gn[j, :] + x / (1 + term)\n",
    "                \n",
    "        J += Jn / denom\n",
    "        G = G + Gn / denom\n",
    "        \n",
    "    J = 0.5 * C * np.dot(w, w) + J / N\n",
    "    G = C * W + G / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.tile([1,2,3], (3,1)) * np.array([0.1, 0.2, 0.3])[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.tile([1,2,3], (3,1)) / np.array([0.1, 0.2, 0.3])[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.tile([1,2,3], (3,1)) * np.array([0.1, 0.2, 0.3])[:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.tile([1,2,3], (3,1)) / np.array([0.1, 0.2, 0.3])[:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obj_ranking(w, X, Y, C):\n",
    "    \"\"\"\n",
    "        Objective with L2 regularisation and ranking loss\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x L\n",
    "            - C: regularisation constant\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    L = Y.shape[1]\n",
    "    assert(w.shape[0] == L * D)\n",
    "    \n",
    "    W = w.reshape(L, D)  # reshape weight matrix    \n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    for n in range(N):\n",
    "        Jn = 0.0\n",
    "        Gn = np.zeros_like(W)\n",
    "        x = X[n, :]\n",
    "        y = Y[n, :]\n",
    "        nPos = np.sum(y)   # num of positive examples\n",
    "        nNeg = L - nPos    # num of negative examples\n",
    "        denom = nPos * nNeg\n",
    "        \n",
    "        ixPos = np.nonzero(y)[0].tolist()               # indices positive examples\n",
    "        ixNeg = list(set(np.arange(L)) - set(ixPos))    # indices negative examples\n",
    "        \n",
    "        ixmat = np.array(list(itertools.product(ixPos, ixNeg)))  # shape: ixPos*ixNeg by 2\n",
    "        dW = W[ixmat[:, 0], :] - W[ixmat[:, 1], :]\n",
    "        sVec = np.dot(dW, x)\n",
    "        Jn = np.sum(np.log1p(np.exp(-sVec)))\n",
    "        \n",
    "        coeffVec = np.divide(1, 1 + np.exp(sVec))\n",
    "        coeffPos = pd.DataFrame(coeffVec)\n",
    "        coeffPos['gid'] = ixmat[:, 0]\n",
    "        coeffPos = coeffPos.groupby('gid', sort=False).sum()\n",
    "        coeffNeg = pd.DataFrame(coeffVec)\n",
    "        coeffNeg['gid'] = ixmat[:, 1]\n",
    "        coeffNeg = coeffNeg.groupby('gid', sort=False).sum()\n",
    "        \n",
    "        #print(coeffPos)\n",
    "        #print(coeffNeg)\n",
    "        \n",
    "        coeffs = np.ones(L)\n",
    "        coeffs[ixPos] = -coeffPos.loc[ixPos].values.squeeze()\n",
    "        coeffs[ixNeg] = coeffNeg.loc[ixNeg].values.squeeze()\n",
    "        \n",
    "        #print(coeffs)\n",
    "        Gn = np.tile(x, (L, 1)) * coeffs[:, None]\n",
    "                        \n",
    "        J += Jn / denom\n",
    "        G = G + Gn / denom\n",
    "        \n",
    "    J = 0.5 * C * np.dot(w, w) + J / N\n",
    "    G = C * W + G / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = create_dataset_v2(data = data_train)\n",
    "X_test,  Y_test  = create_dataset_v2(data = data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%script false\n",
    "C = 1\n",
    "w0 = np.random.rand(nFeatures * nLabels)\n",
    "check_grad(lambda w: obj_ranking(w, X_train[:10], Y_train[:10], C)[0], \\\n",
    "           lambda w: obj_ranking(w, X_train[:10], Y_train[:10], C)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allTruths_train = Y_train\n",
    "allTruths_test = Y_test\n",
    "allPreds_train = None\n",
    "allPreds_test  = None\n",
    "np.random.seed(SEED)\n",
    "\n",
    "opt_method = 'BFGS' #'Newton-CG' \n",
    "#opt_method = 'nelder-mead'\n",
    "options = {'disp': True}\n",
    "\n",
    "C = 1\n",
    "w0 = np.random.rand(nFeatures * nLabels)  # initial guess\n",
    "opt = minimize(obj_ranking, w0, args=(X_train, Y_train, C), method=opt_method, jac=True, options=options)\n",
    "\n",
    "if opt.success == True:\n",
    "    w = opt.x\n",
    "    #allPreds = sigmoid(np.dot(X_test, w.reshape(nLabels, nFeatures).T))\n",
    "    allPreds_train = np.dot(X_train, w.reshape(nLabels, nFeatures).T)\n",
    "    allPreds_test = np.dot(X_test, w.reshape(nLabels, nFeatures).T)\n",
    "else:\n",
    "    sys.stderr.write('Optimisation failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(allPreds_test.shape)\n",
    "print(allTruths_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Training set:')\n",
    "printEvaluation(allTruths_train, allPreds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Test set:')\n",
    "printEvaluation(allTruths_test, allPreds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p-classification loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-label learning with p-norm push loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obj_pnorm_push_loop(w, X, Y, p, C):\n",
    "    \"\"\"\n",
    "        Objective with L2 regularisation and p-norm push loss\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x L\n",
    "            - p: constant for p-norm push loss\n",
    "            - C: regularisation constant\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    L = Y.shape[1]\n",
    "    assert(w.shape[0] == L * D)\n",
    "    assert(p >= 1)\n",
    "    assert(C >= 0)\n",
    "    \n",
    "    W = w.reshape(L, D)  # reshape weight matrix\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    for n in range(N):\n",
    "        Gn = np.zeros_like(W)\n",
    "        x = X[n, :]\n",
    "        y = Y[n, :]\n",
    "        nPos = np.sum(y)   # num of positive examples\n",
    "        nNeg = L - nPos    # num of negative examples\n",
    "        \n",
    "        for k in range(L):\n",
    "            wk = W[k, :]\n",
    "            term = np.dot(wk, x)\n",
    "            if y[k] == 1:\n",
    "                term2 = np.exp(-term) / nPos\n",
    "                J += term2\n",
    "                Gn[k, :] = -x * term2\n",
    "            else:\n",
    "                term2 = np.exp(p * term) / nNeg\n",
    "                J += term2 / p\n",
    "                Gn[k, :] = x * term2\n",
    "        G = G + Gn\n",
    "        \n",
    "    J = 0.5 * C * np.dot(w, w) + J / N\n",
    "    G = C * W + G / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obj_pnorm_push_loopn(w, X, Y, p, C):\n",
    "    \"\"\"\n",
    "        Objective with L2 regularisation and p-norm push loss\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x L\n",
    "            - p: constant for p-norm push loss\n",
    "            - C: regularisation constant\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    L = Y.shape[1]\n",
    "    assert(w.shape[0] == L * D)\n",
    "    assert(p >= 1)\n",
    "    assert(C >= 0)\n",
    "    \n",
    "    W = w.reshape(L, D)  # reshape weight matrix\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    for n in range(N):\n",
    "        Gn = np.zeros_like(W)\n",
    "        x = X[n, :]\n",
    "        y = Y[n, :]\n",
    "        nPos = np.sum(y)   # num of positive examples\n",
    "        nNeg = L - nPos    # num of negative examples\n",
    "        \n",
    "        ixPos = np.nonzero(y)[0].tolist()               # indices positive examples\n",
    "        ixNeg = list(set(np.arange(L)) - set(ixPos))    # indices negative examples\n",
    "        \n",
    "        scalingPos = np.exp(   -np.dot(W[ixPos, :], x)) / nPos\n",
    "        scalingNeg = np.exp(p * np.dot(W[ixNeg, :], x)) / nNeg\n",
    "        \n",
    "        Gn[ixPos, :] = np.tile(-x, (nPos,1)) * scalingPos[:, None]  # scaling each row of a matrix\n",
    "        Gn[ixNeg, :] = np.tile( x, (nNeg,1)) * scalingNeg[:, None]  #         with a different scalar\n",
    "        \n",
    "        J += np.sum(scalingPos) + np.sum(scalingNeg) / p\n",
    "        G = G + Gn\n",
    "        \n",
    "    J = 0.5 * C * np.dot(w, w) + J / N\n",
    "    G = C * W + G / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obj_pnorm_push(w, X, Y, p, C):\n",
    "    \"\"\"\n",
    "        Objective with L2 regularisation and p-norm push loss\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x L\n",
    "            - p: constant for p-norm push loss\n",
    "            - C: regularisation constant\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    L = Y.shape[1]\n",
    "    assert(w.shape[0] == L * D)\n",
    "    assert(p >= 1)\n",
    "    assert(C >= 0)\n",
    "    \n",
    "    W = w.reshape(L, D)  # reshape weight matrix\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    for k in range(nLabels):\n",
    "        wk = W[k, :]\n",
    "        Yk = Y[:, k]\n",
    "        sPosVec = np.dot(X[Yk == 1, :], wk)      # Nk+ by 1\n",
    "        sNegVec = np.dot(X[Yk == 0, :], wk)      # NK- by 1\n",
    "        #nPosVec = np.sum(Y[Yk == 1, :], axis=1)  # Nk+ by 1\n",
    "        #nNegVec = np.sum(Y[Yk == 0, :], axis=1)  # NK- by 1\n",
    "        nPosVec = np.sum(Y[Yk == 1, :], axis=1) + 0.1 # Nk+ by 1 with smoothing\n",
    "        nNegVec = np.sum(Y[Yk == 0, :], axis=1) + 0.1 # NK- by 1 with smoothing\n",
    "        \n",
    "        #nPosVec = np.ones_like(sPosVec) * N\n",
    "        #nNegVec = np.ones_like(sNegVec) * N\n",
    "        lossPos = np.divide(np.exp(-sPosVec), nPosVec)     # NK+ by 1\n",
    "        lossNeg = np.divide(np.exp(p * sNegVec), nNegVec)  # NK- by 1\n",
    "        \n",
    "        J += np.sum(lossPos) + np.sum(lossNeg / p)\n",
    "        \n",
    "        GradPos = -X[Yk == 1, :] * lossPos[:, None]\n",
    "        GradNeg =  X[Yk == 0, :] * lossNeg[:, None]\n",
    "        \n",
    "        G[k, :] = np.sum(GradPos, axis=0) + np.sum(GradNeg, axis=0)\n",
    "                \n",
    "    J = 0.5 * C * np.dot(w, w) + J / N\n",
    "    G = C * W + G / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = create_dataset_v2(data = data_train)\n",
    "X_test,  Y_test  = create_dataset_v2(data = data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "p = 1\n",
    "C = 1\n",
    "w0 = np.random.rand(nFeatures * nLabels)\n",
    "check_grad(lambda w: obj_pnorm_push(w, X_train, Y_train, p, C)[0], \\\n",
    "           lambda w: obj_pnorm_push(w, X_train, Y_train, p, C)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allTruths_train = Y_train\n",
    "allTruths_test = Y_test\n",
    "allPreds_train = None\n",
    "allPreds_test  = None\n",
    "np.random.seed(SEED)\n",
    "\n",
    "p = 1  # [1, 10]\n",
    "C = 1  # [0, 1]\n",
    "opt_method = 'BFGS' #'Newton-CG' \n",
    "#opt_method = 'nelder-mead'\n",
    "options = {'disp': True}\n",
    "\n",
    "w0 = np.random.rand(nFeatures * nLabels)  # initial guess\n",
    "opt = minimize(obj_pnorm_push, w0, args=(X_train, Y_train, p, C), method=opt_method, jac=True, options=options)\n",
    "\n",
    "if opt.success == True:\n",
    "    w = opt.x\n",
    "    allPreds_train = np.dot(X_train, w.reshape(nLabels, nFeatures).T)\n",
    "    allPreds_test = np.dot(X_test, w.reshape(nLabels, nFeatures).T)\n",
    "else:\n",
    "    sys.stderr.write('Optimisation failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(allPreds_test.shape)\n",
    "print(allTruths_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Training set:')\n",
    "printEvaluation(allTruths_train, allPreds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Test set:')\n",
    "printEvaluation(allTruths_test, allPreds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for different hyper-parameter configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make_pipeline(preprocessing.StandardScaler(), svm.SVC(C=1))\n",
    "#cross_val_score(clf, iris.data, iris.target, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%script false\n",
    "\n",
    "precisions_train = dict()\n",
    "precisions_test = dict()\n",
    "\n",
    "allTruths_test  = Y_test\n",
    "allTruths_train = Y_train\n",
    "\n",
    "p_set = [1, 3, 10, 30]\n",
    "C_set = [0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000, 3000]\n",
    "opt_method = 'BFGS' #'Newton-CG' \n",
    "\n",
    "for p in p_set:\n",
    "    for C in C_set:\n",
    "        print('-------------------------------------')\n",
    "        print('p in loss: {}, C for regularisation: {}'.format(p, C))\n",
    "        allPreds       = None\n",
    "        allPreds_train = None\n",
    "\n",
    "        w0 = np.random.rand(nFeatures * nLabels)  # initial guess\n",
    "        opt = minimize(obj_pnorm_push, w0, args=(X_train, Y_train, p, C), method=opt_method, jac=True)\n",
    "\n",
    "        if opt.success == True:\n",
    "            w = opt.x\n",
    "            allPreds_test  = np.dot(X_test,  w.reshape(nLabels, nFeatures).T)\n",
    "            allPreds_train = np.dot(X_train, w.reshape(nLabels, nFeatures).T)\n",
    "            precisions_train[(p,C)] = avgPrecisionK(allTruths_train, allPreds_train)\n",
    "            precisions_test[(p,C)]  = avgPrecisionK(allTruths_test,  allPreds_test)\n",
    "        else:\n",
    "            sys.stderr.write('Optimisation failed')\n",
    "            precisions_train[(p,C)] = 0\n",
    "            precisions_test[(p,C)]  = 0\n",
    "\n",
    "        print('%20s %.4f' % ('Average Precision@K on training set: ', precisions_train[(p,C)]))\n",
    "        print('%20s %.4f\\n' % ('Average Precision@K on test set: ', precisions_test[(p,C)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%script false\n",
    "fig = plt.figure(figsize=[10, 8])\n",
    "ax = plt.subplot(1,1,1)\n",
    "#colors = itertools.cycle(['r', 'g'])\n",
    "styles = itertools.cycle(['-', '--', ':', '-.'])\n",
    "for p in p_set:\n",
    "    ls = styles.__next__()\n",
    "    plt.plot(np.arange(len(C_set)), [precisions_train[(p,C)] for C in C_set], \\\n",
    "             ls=ls, c='r', label='p=%d'%p + ', train')\n",
    "    plt.plot(np.arange(len(C_set)), [precisions_test[(p,C)] for C in C_set], \\\n",
    "             ls=ls, c='g',  label='p=%d'%p + ', test')\n",
    "plt.plot(np.arange(len(C_set)), [0.5149 for C in C_set], ls='-', c='b', label='Logistic Regression, test')\n",
    "plt.legend(loc='best')\n",
    "plt.xticks(np.arange(len(C_set)), C_set, fontsize=10, rotation=0, horizontalalignment='center')\n",
    "plt.xlabel('Regularisation Constant')\n",
    "plt.ylabel('Average Precision@K')\n",
    "plt.title('Performance on Yeast dataset, multi-label learning with p-norm push loss', fontsize=15)\n",
    "fig.savefig('pnorm.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top push loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods to compute $v = (\\sum_{n=1}^N A_{N \\times K})^\\top (\\sum_{n=1}^N A_{N \\times K})$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Let $\\mathbf{x} = (\\sum_{n=1}^N A_{N \\times K})$, then $v = \\mathbf{x}^\\top \\mathbf{x}$\n",
    "1. Let $C_{N \\times N} = A_{N \\times K} A_{N \\times K}^\\top$, then $v = \\sum_{n=1}^N \\sum_{m=1}^N C_{n,m}$,\n",
    "1. or $v = \\mathbf{1}_N^\\top C_{N \\times N} \\mathbf{1}_N$.\n",
    "\n",
    "**NOTE**: if $D_{K \\times K} = A_{N \\times K}^\\top A_{N \\times K}$, then $v \\ne \\sum_{k=1}^K \\sum_{l=1}^L D_{K \\times K}$, this can be checked with a simple $A_{2x2} = [A_{11}, A_{12}; A_{21}, A_{22}]$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = np.random.rand(15).reshape(3,5)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.sum(A, axis=0)\n",
    "#x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = np.dot(A, A.T)\n",
    "one = np.ones(A.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.dot(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sum(np.sum(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.dot(np.dot(one, C), one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = np.dot(A.T, A)\n",
    "np.sum(np.sum(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speed test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = np.random.rand(20000000).reshape(10000,2000)\n",
    "#A = np.random.rand(10000000).reshape(2000,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "x = np.sum(A, axis=0)\n",
    "np.dot(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "C = np.dot(A, A.T)\n",
    "np.sum(np.sum(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "C = np.dot(A, A.T)\n",
    "one = np.ones(A.shape[0])\n",
    "np.dot(np.dot(one, C), one)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
