{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple example of  multilable learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os, sys, time\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import cython\n",
    "from scipy.io import arff\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "yeast_ftrain = os.path.join(data_dir, 'yeast/yeast-train.arff')\n",
    "yeast_ftest  = os.path.join(data_dir, 'yeast/yeast-test.arff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load yeast dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train, meta_train = arff.loadarff(yeast_ftrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test, meta_test = arff.loadarff(yeast_ftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(data_train[0])[:-14], dtype=np.float).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(data_train[0])[-14:], dtype=np.int).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nFeatures = 103\n",
    "nLabels = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "917"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a logistic regression model for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_training_set(label_ix, data):\n",
    "    \"\"\"\n",
    "        Create the labelled dataset for a given label index\n",
    "        \n",
    "        Input:\n",
    "            - label_ix: label index, number in { 0, ..., # labels }\n",
    "            - data: original data with features + labels\n",
    "            \n",
    "        Output:\n",
    "            - (Feature, Label) pair (X, y)\n",
    "              X comprises the features for each example\n",
    "              y comprises the labels of the corresponding example\n",
    "    \"\"\"\n",
    "\n",
    "    assert(label_ix >= 0)\n",
    "    assert(label_ix < nLabels)\n",
    "\n",
    "    N = len(data)\n",
    "    d = nFeatures\n",
    "\n",
    "    X = np.zeros((N, d), dtype = np.float)\n",
    "    y = np.zeros(N, dtype = np.int)\n",
    "       \n",
    "    for i in range(N):\n",
    "        X[i, :] = list(data[i])[:-14]\n",
    "        y[i]    = list(data[i])[-14:][label_ix]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers = [LogisticRegression(class_weight='balanced') for i in range(nLabels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allPreds  = [ ]\n",
    "allTruths = [ ]\n",
    "coefMat = [ ]\n",
    "labelIndices = [ ]\n",
    "\n",
    "for label_ix in range(nLabels):\n",
    "    X_train, y_train = gen_training_set(label_ix, data = data_train)\n",
    "    X_test, y_test   = gen_training_set(label_ix, data = data_test)\n",
    "        \n",
    "    # by fixing random seed, the same playlists will be in the test set each time\n",
    "    #X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(X, y, \\\n",
    "    #                                                                       test_size = 0.33, \\\n",
    "    #                                                                       random_state = 31)    \n",
    "    \n",
    "    allTruths.append(y_test) \n",
    "    \n",
    "    assert( (not np.all(y_train == 0)) or (not np.all(y_train == 1)) )\n",
    "    \n",
    "    #if np.all(y_train == 0) or np.all(y_train == 1): continue\n",
    "    \n",
    "    #if np.all(y_train == 0):\n",
    "    #    allPreds.append(np.zeros(X_test.shape[0], dtype=np.int))\n",
    "    #    continue\n",
    "    #elif np.all(y_train == 1):\n",
    "    #    allPreds.append(np.ones(X_test.shape[0], dtype=np.int))\n",
    "    #    continue\n",
    "\n",
    "    classifiers[label_ix].fit(X_train, y_train)\n",
    "    allPreds.append(classifiers[label_ix].decision_function(X_test))\n",
    "\n",
    "    coefMat.append(classifiers[label_ix].coef_.reshape(-1))\n",
    "    #labelIndices.append(label_ix)\n",
    "    #print(classifiers[label_ix].coef_)\n",
    "    #print(classifiers[label_ix].intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(917, 14)\n",
      "(917, 14)\n"
     ]
    }
   ],
   "source": [
    "allPreds = np.array(allPreds).T\n",
    "allTruths = np.array(allTruths).T\n",
    "\n",
    "print(allPreds.shape)\n",
    "print(allTruths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.15997494, -0.51477752, -1.18553419,  0.21972233,  0.96896183,\n",
       "        0.00853919,  0.32721911,  0.37575265, -0.80121226, -2.4663948 ,\n",
       "       -1.79701745,  0.92752986,  0.89316058, -2.04288556])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allPreds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss between a ground truth and a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(truth, pred, lossType='Hamming'):\n",
    "    \"\"\"\n",
    "    compute a few losses given ground truth and prediction:\n",
    "    subset 0-1, Hamming, ranking, and Precision@K where K = # positive labels.\n",
    "    \"\"\"\n",
    "    assert(len(truth) == len(pred))\n",
    "    L = len(truth)\n",
    "    nPos = np.sum(truth)\n",
    "    \n",
    "    if lossType == 'Subset01':\n",
    "        return 1 - int(np.all(truth == pred))\n",
    "    \n",
    "    elif lossType == 'Hamming':\n",
    "        return np.sum(truth != pred) / L\n",
    "    \n",
    "    elif lossType == 'Ranking':\n",
    "        loss = 0\n",
    "        for i in range(L-1):\n",
    "            for j in range(i+1, L):\n",
    "                if truth[i] > truth[j]:\n",
    "                    if pred[i] < pred[j]: \n",
    "                        loss += 1\n",
    "                    if pred[i] == pred[j]:\n",
    "                        loss += 0.5\n",
    "        #return loss / (nPos * (L-nPos))\n",
    "        return loss\n",
    "        \n",
    "    elif lossType == 'Precision@K':\n",
    "        loss = 0\n",
    "        for i in range(L):\n",
    "            if truth[i] == 1 and pred[i] == 0:\n",
    "                loss += 1\n",
    "        return loss / nPos\n",
    "    \n",
    "    else:\n",
    "        assert(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Subset01 Loss: 0.9182\n",
      "Average Hamming Loss: 0.2874\n",
      "Average Ranking Loss: 6.9967\n",
      "Average Precision@K Loss: 0.5956\n"
     ]
    }
   ],
   "source": [
    "for lossType in ['Subset01', 'Hamming', 'Ranking', 'Precision@K']:\n",
    "    losses = [ ]\n",
    "    for i in range(allPreds.shape[0]):\n",
    "        pred  = np.array((allPreds[i, :] > 0.5), dtype=np.int)\n",
    "        truth = allTruths[i, :]\n",
    "        losses.append(loss(truth, pred, lossType))\n",
    "        \n",
    "        #print(allPreds[i])\n",
    "        #print(pred)\n",
    "        #print(truth)\n",
    "        #break\n",
    "    \n",
    "    print('Average %s Loss: %1.4f' % (lossType, np.mean(losses)))\n",
    "    #plt.hist(aucs, bins = 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute average precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coefficient matrix `(#Genres, #Songs)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefMat = np.array(coefMat).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 14)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefMat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sns.heatmap(coefMat[:, :30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "cpdef obj_rank(w, X, y):\n",
    "    assert(len(y) == X.shape[0])\n",
    "    assert(len(w) == X.shape[1])\n",
    "\n",
    "    cdef int nPos, nNeg, i, j\n",
    "    cdef double J, term, denom\n",
    "    nPos = np.sum(y)      # num of positive examples\n",
    "    nNeg = len(y) - nPos  # num of negative examples\n",
    "    \n",
    "    ixPos = np.nonzero(y)[0].tolist()                    # indices positive examples\n",
    "    ixNeg = sorted(set(np.arange(len(y))) - set(ixPos))  # indices negative examples\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    g = np.zeros_like(w)  # gradient\n",
    "    \n",
    "    for i in ixPos:\n",
    "        for j in ixNeg:\n",
    "            dx = X[i, :] - X[j, :]\n",
    "            term = np.dot(w, dx)\n",
    "            J += np.log1p(np.exp(term))\n",
    "            g = g + dx / (np.expm1(-term) + 2)    \n",
    "    \n",
    "    denom = nPos * nNeg\n",
    "    J = 0.5 * np.dot(w, w) + J / denom\n",
    "    g = w + g / denom\n",
    "    \n",
    "    return (J, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decision(X_test, w):\n",
    "    assert(len(w) == X_test.shape[1])\n",
    "    \n",
    "    scores = np.dot(X_test, w)\n",
    "    probs = 1 / (2 + np.expm1(-scores))\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = [ ]\n",
    "allPreds  = [ ]\n",
    "allTruths = [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 14Optimization terminated successfully.\n",
      "         Current function value: 0.687650\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "2 / 14Optimization terminated successfully.\n",
      "         Current function value: 0.690264\n",
      "         Iterations: 4\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "3 / 14Optimization terminated successfully.\n",
      "         Current function value: 0.687208\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "4 / 14Optimization terminated successfully.\n",
      "         Current function value: 0.684365\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "5 / 14Optimization terminated successfully.\n",
      "         Current function value: 0.687649\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "6 / 14Optimization terminated successfully.\n",
      "         Current function value: 0.689686\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "7 / 14Optimization terminated successfully.\n",
      "         Current function value: 0.690316\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "8 / 14Optimization terminated successfully.\n",
      "         Current function value: 0.690929\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "9 / 14Optimization terminated successfully.\n",
      "         Current function value: 0.690514\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "10 / 14Optimization terminated successfully.\n",
      "         Current function value: 0.689401\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "11 / 14Optimization terminated successfully.\n",
      "         Current function value: 0.690708\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "12 / 14Optimization terminated successfully.\n",
      "         Current function value: 0.690920\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "13 / 14Optimization terminated successfully.\n",
      "         Current function value: 0.690952\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "14 / 14Optimization terminated successfully.\n",
      "         Current function value: 0.682634\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n"
     ]
    }
   ],
   "source": [
    "for label_ix in range(nLabels):\n",
    "    #sys.stdout.write('\\r%d / %d' % (label_ix + 1, nLabels))\n",
    "    #sys.stdout.flush()\n",
    "    print('\\r%d / %d ' % (label_ix + 1, nLabels))\n",
    "    \n",
    "    X_train, y_train = gen_training_set(label_ix, data = data_train)\n",
    "    X_test, y_test   = gen_training_set(label_ix, data = data_test)\n",
    "    \n",
    "    allTruths.append(y_test) \n",
    "    \n",
    "    assert( (not np.all(y_train == 0)) or (not np.all(y_train == 1)) )\n",
    "    \n",
    "    #if np.all(y_train == 0) or np.all(y_train == 1): continue\n",
    "    \n",
    "    #if np.all(y_train == 0):\n",
    "    #    allPreds.append(np.zeros(X_test.shape[0], dtype=np.int))\n",
    "    #    continue\n",
    "    #elif np.all(y_train == 1):\n",
    "    #    allPreds.append(np.ones(X_test.shape[0], dtype=np.int))\n",
    "    #    continue\n",
    "    \n",
    "    w = np.random.rand(X_train.shape[1])  # initial guess\n",
    "    opt_method = 'BFGS' #'Newton-CG' \n",
    "    options = {'disp': True}\n",
    "    opt = minimize(obj_rank, w, args=(X_train, y_train), method=opt_method, jac=True, options=options)\n",
    "    if opt.success == True:\n",
    "        w = opt.x\n",
    "        params.append(w)\n",
    "        allPreds.append(np.dot(X_test, w))\n",
    "    else:\n",
    "        sys.stderr.write('Optimisation failed, label_ix=%d\\n' % label_ix)\n",
    "        w = np.zeros(X_train.shape[1])\n",
    "        params.append(w)\n",
    "        allPreds.append(decision(X_test, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(917, 14)\n",
      "(917, 14)\n"
     ]
    }
   ],
   "source": [
    "allPreds = np.array(allPreds).T\n",
    "allTruths = np.array(allTruths).T\n",
    "\n",
    "print(allPreds.shape)\n",
    "print(allTruths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01041459,  0.02343311,  0.01492048, -0.04255027, -0.03156382,\n",
       "        0.02135366,  0.01013724,  0.01079779,  0.02023349,  0.05559079,\n",
       "        0.03785101, -0.02253766, -0.02141111,  0.00042809])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allPreds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Subset01 Loss: 1.0000\n",
      "Average Hamming Loss: 0.3037\n",
      "Average Ranking Loss: 9.9280\n",
      "Average Precision@K Loss: 1.0000\n"
     ]
    }
   ],
   "source": [
    "for lossType in ['Subset01', 'Hamming', 'Ranking', 'Precision@K']:\n",
    "    losses = [ ]\n",
    "    for i in range(allPreds.shape[0]):\n",
    "        pred  = np.array((allPreds[i, :] > 0.5), dtype=np.int)\n",
    "        truth = allTruths[i, :]\n",
    "        losses.append(loss(truth, pred, lossType))\n",
    "        \n",
    "        #print(allPreds[i])\n",
    "        #print(pred)\n",
    "        #print(truth)\n",
    "        #break\n",
    "    \n",
    "    print('Average %s Loss: %1.4f' % (lossType, np.mean(losses)))\n",
    "    #plt.hist(aucs, bins = 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
