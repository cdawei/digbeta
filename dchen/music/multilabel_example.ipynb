{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple example of  multilable learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os, sys, time\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import cython\n",
    "import itertools\n",
    "\n",
    "from scipy.io import arff\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import check_grad\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "yeast_ftrain = os.path.join(data_dir, 'yeast/yeast-train.arff')\n",
    "yeast_ftest  = os.path.join(data_dir, 'yeast/yeast-test.arff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load yeast dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train, meta_train = arff.loadarff(yeast_ftrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test, meta_test = arff.loadarff(yeast_ftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(data_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#features: 103\n"
     ]
    }
   ],
   "source": [
    "nFeatures = np.array(list(data_train[0])[:-14], dtype=np.float).shape[0]\n",
    "print('#features:', nFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.array(list(data_train[0])[:-14], dtype=np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#labels: 14\n"
     ]
    }
   ],
   "source": [
    "nLabels = np.array(list(data_train[0])[-14:], dtype=np.int).shape[0]\n",
    "print('#labels:', nLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.array(list(data_train[0])[-14:], dtype=np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#training examples: 1500\n"
     ]
    }
   ],
   "source": [
    "print('#training examples:', len(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#test examples: 917\n"
     ]
    }
   ],
   "source": [
    "print('#test examples:', len(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of #positive labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nPositives = [np.sum(np.array(list(data_train[ix])[-14:], dtype=np.int)) for ix in range(len(data_train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7feb1de6eb38>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/i8nOrcm7yEO/AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb1e2f29e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(nPositives).hist(bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset(label_ix, data):\n",
    "    \"\"\"\n",
    "        Create the labelled dataset for a given label index\n",
    "        \n",
    "        Input:\n",
    "            - label_ix: label index, number in { 0, ..., # labels }\n",
    "            - data: original data with features + labels\n",
    "            \n",
    "        Output:\n",
    "            - (Feature, Label) pair (X, y)\n",
    "              X comprises the features for each example\n",
    "              y comprises the labels of the corresponding example\n",
    "    \"\"\"\n",
    "\n",
    "    assert(label_ix >= 0)\n",
    "    assert(label_ix < nLabels)\n",
    "\n",
    "    N = len(data)\n",
    "    d = nFeatures\n",
    "\n",
    "    X = np.zeros((N, d), dtype = np.float)\n",
    "    y = np.zeros(N, dtype = np.int)\n",
    "       \n",
    "    for i in range(N):\n",
    "        X[i, :] = list(data[i])[:-14]\n",
    "        y[i]    = list(data[i])[-14:][label_ix]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset_v2(data):\n",
    "    \"\"\"\n",
    "        Create the labelled dataset for a given label index\n",
    "        \n",
    "        Input:\n",
    "            - data: original data with features + labels\n",
    "            \n",
    "        Output:\n",
    "            - (Feature, Label) pair (X, y)\n",
    "              X comprises the features for each example\n",
    "              Y comprises the labels of the corresponding example\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(data)\n",
    "    D = nFeatures\n",
    "    L = nLabels\n",
    "\n",
    "    X = np.zeros((N, D), dtype = np.float)\n",
    "    Y = np.zeros((N, L), dtype = np.int)\n",
    "       \n",
    "    for i in range(N):\n",
    "        X[i, :] = list(data[i])[:-14]\n",
    "        Y[i, :] = list(data[i])[-14:]\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss between a ground truth and a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalPred(truth, pred, lossType = 'Hamming'):\n",
    "    \"\"\"\n",
    "        Compute loss given ground truth and prediction\n",
    "        \n",
    "        Input:\n",
    "            - truth:    binary array of true labels\n",
    "            - pred:     real-valued array of predictions\n",
    "            - lossType: can be subset 0-1, Hamming, ranking, and Precision@K where K = # positive labels.\n",
    "    \"\"\"\n",
    "\n",
    "    assert(len(truth) == len(pred))\n",
    "    L = len(truth)\n",
    "    nPos = np.sum(truth)\n",
    "    \n",
    "    predBin = np.array((pred > 0), dtype=np.int)\n",
    "    \n",
    "    if lossType == 'Subset01':\n",
    "        return 1 - int(np.all(truth == predBin))\n",
    "    \n",
    "    elif lossType == 'Hamming':\n",
    "        return np.sum(truth != predBin) / L\n",
    "    \n",
    "    elif lossType == 'Ranking':\n",
    "        loss = 0\n",
    "        for i in range(L-1):\n",
    "            for j in range(i+1, L):\n",
    "                if truth[i] > truth[j]:\n",
    "                    if pred[i] < pred[j]: \n",
    "                        loss += 1\n",
    "                    if pred[i] == pred[j]:\n",
    "                        loss += 0.5\n",
    "        #return loss / (nPos * (L-nPos))\n",
    "        return loss\n",
    "        \n",
    "    elif lossType == 'Precision@K':\n",
    "        # sorted indices of the labels most likely to be +'ve\n",
    "        idx = np.argsort(pred)[::-1]\n",
    "        \n",
    "        # true labels according to the sorted order\n",
    "        y = truth[idx]\n",
    "        \n",
    "        # fraction of +'ves in the top K predictions\n",
    "        return np.mean(y[:nPos])\n",
    "    \n",
    "    elif lossType == 'Precision@3':\n",
    "        # sorted indices of the labels most likely to be +'ve\n",
    "        idx = np.argsort(pred)[::-1]\n",
    "        \n",
    "        # true labels according to the sorted order\n",
    "        y = truth[idx]\n",
    "        \n",
    "        # fraction of +'ves in the top K predictions\n",
    "        return np.mean(y[:3])\n",
    "    \n",
    "    elif lossType == 'Precision@5':\n",
    "        # sorted indices of the labels most likely to be +'ve\n",
    "        idx = np.argsort(pred)[::-1]\n",
    "        \n",
    "        # true labels according to the sorted order\n",
    "        y = truth[idx]\n",
    "        \n",
    "        # fraction of +'ves in the top K predictions\n",
    "        return np.mean(y[:5])\n",
    "    \n",
    "    else:\n",
    "        assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printEvaluation(allPreds, allTruths):\n",
    "\n",
    "    for lossType in ['Precision@K']: \n",
    "        # ['Subset01', 'Hamming', 'Ranking', 'Precision@K', 'Precision@3', 'Precision@5']:\n",
    "        losses = [ ]\n",
    "        for i in range(allPreds.shape[0]):\n",
    "            pred  = allPreds[i, :]\n",
    "            truth = allTruths[i, :]\n",
    "            losses.append(evalPred(truth, pred, lossType))\n",
    "\n",
    "            #print(allPreds[i])\n",
    "            #print(pred)\n",
    "            #print(truth)\n",
    "            #break\n",
    "\n",
    "        #print('%24s: %1.4f' % ('Average %s Loss' % lossType, np.mean(losses)))\n",
    "        print('%s: %1.4f' % ('Average %s' % lossType, np.mean(losses)))\n",
    "        #plt.hist(aucs, bins = 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary relevance baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a logistic regression model for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers = [ LogisticRegression(class_weight = 'balanced', C = 10**0) for i in range(nLabels) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allPreds  = [ ]\n",
    "allTruths = [ ]\n",
    "coefMat = [ ]\n",
    "labelIndices = [ ]\n",
    "\n",
    "for label_ix in range(nLabels):\n",
    "    X_train, y_train = create_dataset(label_ix, data = data_train)\n",
    "    X_test, y_test   = create_dataset(label_ix, data = data_test)\n",
    "    \n",
    "    allTruths.append(y_test) \n",
    "    \n",
    "    assert( (not np.all(y_train == 0)) or (not np.all(y_train == 1)) )\n",
    "\n",
    "    classifiers[label_ix].fit(X_train, y_train)\n",
    "    allPreds.append(classifiers[label_ix].decision_function(X_test))\n",
    "\n",
    "    coefMat.append(classifiers[label_ix].coef_.reshape(-1))\n",
    "    #labelIndices.append(label_ix)\n",
    "    #print(classifiers[label_ix].coef_)\n",
    "    #print(classifiers[label_ix].intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(917, 14)\n",
      "(917, 14)\n"
     ]
    }
   ],
   "source": [
    "allPreds  = np.array(allPreds).T\n",
    "allTruths = np.array(allTruths).T\n",
    "\n",
    "print(allPreds.shape)\n",
    "print(allTruths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#allPreds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision@K: 0.5149\n"
     ]
    }
   ],
   "source": [
    "printEvaluation(allPreds, allTruths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coefficient matrix `(#Genres, #Songs)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefMat = np.array(coefMat).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 14)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefMat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sns.heatmap(coefMat[:, :30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary relevance with exponential loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a regression model with exponential loss for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obj_exp(w, X, y, C):\n",
    "    \"\"\"\n",
    "        Objective with L2 regularisation and exponential loss\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector\n",
    "            - X: feature matrix, N x D\n",
    "            - y: label vector,   N x 1\n",
    "            - C: regularisation constant\n",
    "    \"\"\"\n",
    "    assert(len(y) == X.shape[0])\n",
    "    assert(len(w) == X.shape[1])\n",
    "    assert(C >= 0)\n",
    "    \n",
    "    N, D = X.shape\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    g = np.zeros_like(w)  # gradient\n",
    "    \n",
    "    for n in range(N):\n",
    "        x = X[n, :]\n",
    "        prod = np.dot(w, x)\n",
    "        \n",
    "        # negative label\n",
    "        if y[n] == 0:\n",
    "            t1 = np.exp(prod)\n",
    "            J += t1\n",
    "            g = g + t1 * x\n",
    "        \n",
    "        # positive label\n",
    "        else:\n",
    "            t2 = np.exp(-prod)\n",
    "            J += t2\n",
    "            g = g - t2 * x\n",
    "    \n",
    "    J = 0.5 * C * np.dot(w, w) + J / N\n",
    "    g = C * w + g / N\n",
    "    \n",
    "    return (J, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_, y_train_ = create_dataset(3, data = data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.482649913762204e-06"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0 = np.random.rand(X_train_.shape[1])\n",
    "C = 1\n",
    "check_grad(lambda w: obj_exp(w, X_train_, y_train_, C)[0], \\\n",
    "           lambda w: obj_exp(w, X_train_, y_train_, C)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.995826\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "2 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.997287\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "3 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.994601\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "4 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.992873\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "5 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.996471\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "6 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.998130\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "7 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.999026\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "8 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.999114\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "9 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.999693\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "10 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.999290\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "11 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.999354\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "12 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.998675\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "13 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.998644\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "14 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.999825\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n"
     ]
    }
   ],
   "source": [
    "params    = [ ]\n",
    "allPreds  = [ ]\n",
    "allTruths = [ ]\n",
    "C = 1\n",
    "\n",
    "for label_ix in range(nLabels):\n",
    "    #sys.stdout.write('\\r%d / %d' % (label_ix + 1, nLabels))\n",
    "    #sys.stdout.flush()\n",
    "    print('\\r%d / %d ' % (label_ix + 1, nLabels))\n",
    "    \n",
    "    X_train, y_train = create_dataset(label_ix, data = data_train)\n",
    "    X_test, y_test   = create_dataset(label_ix, data = data_test)\n",
    "    \n",
    "    allTruths.append(y_test) \n",
    "    \n",
    "    assert( (not np.all(y_train == 0)) or (not np.all(y_train == 1)) )\n",
    "        \n",
    "    opt_method = 'BFGS' #'Newton-CG' \n",
    "    #opt_method = 'nelder-mead'\n",
    "    options = {'disp': True}\n",
    "    \n",
    "    w0 = np.random.rand(X_train.shape[1])  # initial guess\n",
    "    opt = minimize(obj_exp, w0, args=(X_train, y_train, C), method=opt_method, jac=True, options=options)\n",
    "    \n",
    "    if opt.success == True:\n",
    "        w = opt.x\n",
    "        params.append(w)\n",
    "        #allPreds.append(sigmoid(np.dot(X_test, w)))\n",
    "        allPreds.append(np.dot(X_test, w))\n",
    "    else:\n",
    "        sys.stderr.write('Optimisation failed, label_ix=%d\\n' % label_ix)\n",
    "        w = np.zeros(X_train.shape[1])\n",
    "        params.append(w)\n",
    "        allPreds.append(np.dot(X_test, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(917, 14)\n",
      "(917, 14)\n"
     ]
    }
   ],
   "source": [
    "allPreds = np.array(allPreds).T\n",
    "allTruths = np.array(allTruths).T\n",
    "\n",
    "print(allPreds.shape)\n",
    "print(allTruths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#allPreds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision@K: 0.4874\n"
     ]
    }
   ],
   "source": [
    "printEvaluation(allPreds, allTruths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary relevance with bipartite ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a bipartite ranking model for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%cython -a\n",
    "\n",
    "import numpy as np\n",
    "#cimport numpy as np\n",
    "\n",
    "#cpdef obj_biranking(w, X, y):\n",
    "\n",
    "def obj_biranking(w, X, y, C):\n",
    "    \"\"\"\n",
    "        Objective with L2 regularisation and bipartite ranking loss\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector\n",
    "            - X: feature matrix, N x D\n",
    "            - y: label vector,   N x 1\n",
    "            - C: regularisation constant\n",
    "    \"\"\"\n",
    "    assert(len(y) == X.shape[0])\n",
    "    assert(len(w) == X.shape[1])\n",
    "    assert(C >= 0)\n",
    "\n",
    "    #cdef int nPos, nNeg, i, j\n",
    "    #cdef double J, term, denom\n",
    "    nPos = np.sum(y)      # num of positive examples\n",
    "    nNeg = len(y) - nPos  # num of negative examples\n",
    "    \n",
    "    ixPos = np.nonzero(y)[0].tolist()                    # indices positive examples\n",
    "    ixNeg = list(set(np.arange(len(y))) - set(ixPos))    # indices negative examples\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    g = np.zeros_like(w)  # gradient    \n",
    "\n",
    "    scorePos = X[ixPos, :].dot(w)[:,np.newaxis] # nPos x 1\n",
    "    scoreNeg = X[ixNeg, :].dot(w)[:,np.newaxis] # nNeg x 1\n",
    "    scoreDif = scorePos - scoreNeg.T            # nPos x nNeg\n",
    "    #J = np.mean(np.log(1 + np.exp(-scoreDif)))\n",
    "    J = 0.5 * C * np.dot(w, w) + np.mean(np.log1p(np.exp(-scoreDif)))\n",
    "    \n",
    "    A = -1/(1 + np.exp(scoreDif))\n",
    "\n",
    "    T1 = X[ixPos, :].T.dot(A.sum(axis = 1))\n",
    "    T2 = X[ixNeg, :].T.dot(A.sum(axis = 0))\n",
    "    g  = C * w + 1/(nPos * nNeg) * (T1 - T2)\n",
    "    \n",
    "    return (J, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_, y_train_ = create_dataset(6, data = data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8448924291952094e-06"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0 = w = np.random.rand(X_train_.shape[1])\n",
    "C = 1\n",
    "check_grad(lambda w: obj_biranking(w, X_train_, y_train_, C)[0], \\\n",
    "           lambda w: obj_biranking(w, X_train_, y_train_, C)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.1331503772158218e-06 * np.sqrt(nLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.687650\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "2 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.690264\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "3 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.687208\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "4 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.684365\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "5 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.687649\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "6 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.689686\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "7 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.690316\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "8 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.690929\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "9 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.690514\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "10 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.689401\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "11 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.690708\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "12 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.690920\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "13 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.690952\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "14 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.682634\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n"
     ]
    }
   ],
   "source": [
    "params    = [ ]\n",
    "allPreds  = [ ]\n",
    "allTruths = [ ]\n",
    "C = 1\n",
    "\n",
    "for label_ix in range(nLabels):\n",
    "    #sys.stdout.write('\\r%d / %d' % (label_ix + 1, nLabels))\n",
    "    #sys.stdout.flush()\n",
    "    print('\\r%d / %d ' % (label_ix + 1, nLabels))\n",
    "    \n",
    "    X_train, y_train = create_dataset(label_ix, data = data_train)\n",
    "    X_test, y_test   = create_dataset(label_ix, data = data_test)\n",
    "    \n",
    "    allTruths.append(y_test) \n",
    "    \n",
    "    assert( (not np.all(y_train == 0)) or (not np.all(y_train == 1)) )\n",
    "        \n",
    "    opt_method = 'BFGS' #'Newton-CG' \n",
    "    #opt_method = 'nelder-mead'\n",
    "    options = {'disp': True}\n",
    "    \n",
    "    w0 = np.random.rand(X_train.shape[1])  # initial guess\n",
    "    opt = minimize(obj_biranking, w0, args=(X_train, y_train, C), method=opt_method, jac=True, options=options)\n",
    "    \n",
    "    if opt.success == True:\n",
    "        w = opt.x\n",
    "        params.append(w)\n",
    "        allPreds.append(sigmoid(np.dot(X_test, w)))\n",
    "    else:\n",
    "        sys.stderr.write('Optimisation failed, label_ix=%d\\n' % label_ix)\n",
    "        w = np.zeros(X_train.shape[1])\n",
    "        params.append(w)\n",
    "        allPreds.append(np.dot(X_test, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(917, 14)\n",
      "(917, 14)\n"
     ]
    }
   ],
   "source": [
    "allPreds = np.array(allPreds).T\n",
    "allTruths = np.array(allTruths).T\n",
    "\n",
    "print(allPreds.shape)\n",
    "print(allTruths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#allPreds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision@K: 0.4400\n"
     ]
    }
   ],
   "source": [
    "printEvaluation(allPreds, allTruths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-label learning with ranking loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%cython -a\n",
    "\n",
    "import numpy as np\n",
    "#cimport numpy as np\n",
    "\n",
    "#cpdef obj_ranking(w, X, y):\n",
    "\n",
    "def obj_ranking_loop(w, X, Y, C):\n",
    "    \"\"\"\n",
    "        Objective with L2 regularisation and ranking loss\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x L\n",
    "            - C: regularisation constant\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    L = Y.shape[1]\n",
    "    assert(w.shape[0] == L * D)\n",
    "    \n",
    "    W = w.reshape(L, D)  # reshape weight matrix    \n",
    "    \n",
    "    #cdef int nPos, nNeg, i, j\n",
    "    #cdef double J, term, denom\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    for n in range(N):\n",
    "        Jn = 0.0\n",
    "        Gn = np.zeros_like(W)\n",
    "        x = X[n, :]\n",
    "        y = Y[n, :]\n",
    "        nPos = np.sum(y)   # num of positive examples\n",
    "        nNeg = L - nPos    # num of negative examples\n",
    "        denom = nPos * nNeg\n",
    "        \n",
    "        ixPos = np.nonzero(y)[0].tolist()               # indices positive examples\n",
    "        ixNeg = list(set(np.arange(L)) - set(ixPos))    # indices negative examples\n",
    "        \n",
    "        for i in ixPos:\n",
    "            for j in ixNeg:\n",
    "                wDiff = W[i, :] - W[j, :]\n",
    "                sDiff = np.dot(wDiff, x)\n",
    "                term = np.exp(sDiff)\n",
    "                Jn += np.log1p(1.0 / term)\n",
    "                Gn[i, :] = Gn[i, :] - x / (1 + term)        \n",
    "        #for j in ixNeg:\n",
    "        #    for i in ixPos:\n",
    "        #        wDiff = W[i, :] - W[j, :]\n",
    "        #        sDiff = np.dot(wDiff, x)\n",
    "        #        term = np.exp(sDiff)\n",
    "                Gn[j, :] = Gn[j, :] + x / (1 + term)\n",
    "                \n",
    "        J += Jn / denom\n",
    "        G = G + Gn / denom\n",
    "        \n",
    "    J = 0.5 * C * np.dot(w, w) + J / N\n",
    "    G = C * W + G / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.tile([1,2,3], (3,1)) * np.array([0.1, 0.2, 0.3])[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obj_ranking(w, X, Y, C):\n",
    "    \"\"\"\n",
    "        Objective with L2 regularisation and ranking loss\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x L\n",
    "            - C: regularisation constant\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    L = Y.shape[1]\n",
    "    assert(w.shape[0] == L * D)\n",
    "    \n",
    "    W = w.reshape(L, D)  # reshape weight matrix    \n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    for n in range(N):\n",
    "        Jn = 0.0\n",
    "        Gn = np.zeros_like(W)\n",
    "        x = X[n, :]\n",
    "        y = Y[n, :]\n",
    "        nPos = np.sum(y)   # num of positive examples\n",
    "        nNeg = L - nPos    # num of negative examples\n",
    "        denom = nPos * nNeg\n",
    "        \n",
    "        ixPos = np.nonzero(y)[0].tolist()               # indices positive examples\n",
    "        ixNeg = list(set(np.arange(L)) - set(ixPos))    # indices negative examples\n",
    "        \n",
    "        ixmat = np.array(list(itertools.product(ixPos, ixNeg)))  # shape: ixPos*ixNeg by 2\n",
    "        dW = W[ixmat[:, 0], :] - W[ixmat[:, 1], :]\n",
    "        sVec = np.dot(dW, x)\n",
    "        Jn = np.sum(np.log1p(np.exp(-sVec)))\n",
    "        \n",
    "        coeffVec = np.divide(1, 1 + np.exp(sVec))\n",
    "        coeffPos = pd.DataFrame(coeffVec)\n",
    "        coeffPos['gid'] = ixmat[:, 0]\n",
    "        coeffPos = coeffPos.groupby('gid', sort=False).sum()\n",
    "        coeffNeg = pd.DataFrame(coeffVec)\n",
    "        coeffNeg['gid'] = ixmat[:, 1]\n",
    "        coeffNeg = coeffNeg.groupby('gid', sort=False).sum()\n",
    "        \n",
    "        #print(coeffPos)\n",
    "        #print(coeffNeg)\n",
    "        \n",
    "        coeffs = np.ones(L)\n",
    "        coeffs[ixPos] = -coeffPos.loc[ixPos].values.squeeze()\n",
    "        coeffs[ixNeg] = coeffNeg.loc[ixNeg].values.squeeze()\n",
    "        \n",
    "        #print(coeffs)\n",
    "        Gn = np.tile(x, (L, 1)) * coeffs[:, None]\n",
    "                        \n",
    "        J += Jn / denom\n",
    "        G = G + Gn / denom\n",
    "        \n",
    "    J = 0.5 * C * np.dot(w, w) + J / N\n",
    "    G = C * W + G / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = create_dataset_v2(data = data_train)\n",
    "X_test,  Y_test  = create_dataset_v2(data = data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.8432207145699553e-05"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%script false\n",
    "C = 1\n",
    "w0 = np.random.rand(nFeatures * nLabels)\n",
    "check_grad(lambda w: obj_ranking(w, X_train[:10], Y_train[:10], C)[0], \\\n",
    "           lambda w: obj_ranking(w, X_train[:10], Y_train[:10], C)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692868\n",
      "         Iterations: 3\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n"
     ]
    }
   ],
   "source": [
    "allPreds  = None\n",
    "allTruths = Y_test\n",
    "\n",
    "opt_method = 'BFGS' #'Newton-CG' \n",
    "#opt_method = 'nelder-mead'\n",
    "options = {'disp': True}\n",
    "\n",
    "C = 1\n",
    "w = np.random.rand(nFeatures * nLabels)  # initial guess\n",
    "opt = minimize(obj_ranking, w, args=(X_train, Y_train, C), method=opt_method, jac=True, options=options)\n",
    "\n",
    "if opt.success == True:\n",
    "    w = opt.x\n",
    "    #allPreds = sigmoid(np.dot(X_test, w.reshape(nLabels, nFeatures).T))\n",
    "    allPreds = np.dot(X_test, w.reshape(nLabels, nFeatures).T)\n",
    "else:\n",
    "    sys.stderr.write('Optimisation failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(917, 14)\n",
      "(917, 14)\n"
     ]
    }
   ],
   "source": [
    "#allPreds = np.array(allPreds).T\n",
    "#allTruths = np.array(allTruths).T\n",
    "\n",
    "print(allPreds.shape)\n",
    "print(allTruths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allPreds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision@K: 0.4893\n"
     ]
    }
   ],
   "source": [
    "printEvaluation(allPreds, allTruths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p-norm push loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-label learning with p-norm push loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_pnorm_push(w, X, Y, p, C):\n",
    "    \"\"\"\n",
    "        Objective with L2 regularisation and p-norm push loss\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x L\n",
    "            - p: constant for p-norm push loss\n",
    "            - C: regularisation constant\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    L = Y.shape[1]\n",
    "    assert(w.shape[0] == L * D)\n",
    "    assert(p >= 1)\n",
    "    assert(C >= 0)\n",
    "    \n",
    "    W = w.reshape(L, D)  # reshape weight matrix\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    for n in range(N):\n",
    "        Gn = np.zeros_like(W)\n",
    "        x = X[n, :]\n",
    "        y = Y[n, :]\n",
    "        nPos = np.sum(y)   # num of positive examples\n",
    "        nNeg = L - nPos    # num of negative examples\n",
    "        \n",
    "        ixPos = np.nonzero(y)[0].tolist()               # indices positive examples\n",
    "        ixNeg = list(set(np.arange(L)) - set(ixPos))    # indices negative examples\n",
    "        \n",
    "        scalingPos = np.exp(   -np.dot(W[ixPos, :], x)) / nPos\n",
    "        scalingNeg = np.exp(p * np.dot(W[ixNeg, :], x)) / nNeg\n",
    "        \n",
    "        Gn[ixPos, :] = np.tile(-x, (nPos,1)) * scalingPos[:, None]  # scaling each row of a matrix\n",
    "        Gn[ixNeg, :] = np.tile( x, (nNeg,1)) * scalingNeg[:, None]  #         with a different scalar\n",
    "        \n",
    "        J += np.sum(scalingPos) + np.sum(scalingNeg) / p\n",
    "        G = G + Gn\n",
    "        \n",
    "    J = 0.5 * C * np.dot(w, w) + J / N\n",
    "    G = C * W + G / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obj_pnorm_push_loop(w, X, Y, p, C):\n",
    "    \"\"\"\n",
    "        Objective with L2 regularisation and p-norm push loss\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x L\n",
    "            - p: constant for p-norm push loss\n",
    "            - C: regularisation constant\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    L = Y.shape[1]\n",
    "    assert(w.shape[0] == L * D)\n",
    "    assert(p >= 1)\n",
    "    assert(C >= 0)\n",
    "    \n",
    "    W = w.reshape(L, D)  # reshape weight matrix\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    for n in range(N):\n",
    "        Gn = np.zeros_like(W)\n",
    "        x = X[n, :]\n",
    "        y = Y[n, :]\n",
    "        nPos = np.sum(y)   # num of positive examples\n",
    "        nNeg = L - nPos    # num of negative examples\n",
    "        \n",
    "        for k in range(L):\n",
    "            wk = W[k, :]\n",
    "            term = np.dot(wk, x)\n",
    "            if y[k] == 1:\n",
    "                term2 = np.exp(-term) / nPos\n",
    "                J += term2\n",
    "                Gn[k, :] = -x * term2\n",
    "            else:\n",
    "                term2 = np.exp(p * term) / nNeg\n",
    "                J += term2 / p\n",
    "                Gn[k, :] = x * term2\n",
    "        G = G + Gn\n",
    "        \n",
    "    J = 0.5 * C * np.dot(w, w) + J / N\n",
    "    G = C * W + G / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = create_dataset_v2(data = data_train)\n",
    "X_test,  Y_test  = create_dataset_v2(data = data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7422676413714687e-05"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 1\n",
    "C = 1\n",
    "w0 = np.random.rand(nFeatures * nLabels)\n",
    "check_grad(lambda w: obj_pnorm_push(w, X_train, Y_train, p, C)[0], \\\n",
    "           lambda w: obj_pnorm_push(w, X_train, Y_train, p, C)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 1.998892\n",
      "         Iterations: 4\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n"
     ]
    }
   ],
   "source": [
    "allPreds  = None\n",
    "allTruths = Y_test\n",
    "p = 1  # [1, 10]\n",
    "C = 1  # [0, 1]\n",
    "\n",
    "opt_method = 'BFGS' #'Newton-CG' \n",
    "#opt_method = 'nelder-mead'\n",
    "options = {'disp': True}\n",
    "\n",
    "w0 = np.random.rand(nFeatures * nLabels)  # initial guess\n",
    "opt = minimize(obj_pnorm_push, w0, args=(X_train, Y_train, p, C), method=opt_method, jac=True, options=options)\n",
    "\n",
    "if opt.success == True:\n",
    "    w = opt.x\n",
    "    allPreds = np.dot(X_test, w.reshape(nLabels, nFeatures).T)\n",
    "else:\n",
    "    sys.stderr.write('Optimisation failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(917, 14)\n",
      "(917, 14)\n"
     ]
    }
   ],
   "source": [
    "#allPreds = np.array(allPreds).T\n",
    "#allTruths = np.array(allTruths).T\n",
    "\n",
    "print(allPreds.shape)\n",
    "print(allTruths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#allPreds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision@K: 0.4896\n"
     ]
    }
   ],
   "source": [
    "printEvaluation(allPreds, allTruths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for different hyper-parameter configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "p in loss: 1, C for regularisation: 0\n",
      "Evaluation on training set:\n",
      "Average Precision@K: 0.5641\n",
      "\n",
      "Evaluation on test set:\n",
      "Average Precision@K: 0.4720\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 1, C for regularisation: 1\n",
      "Evaluation on training set:\n",
      "Average Precision@K: 0.5161\n",
      "\n",
      "Evaluation on test set:\n",
      "Average Precision@K: 0.4896\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 10, C for regularisation: 0\n",
      "Evaluation on training set:\n",
      "Average Precision@K: 0.5976\n",
      "\n",
      "Evaluation on test set:\n",
      "Average Precision@K: 0.4940\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 10, C for regularisation: 1\n",
      "Evaluation on training set:\n",
      "Average Precision@K: 0.5189\n",
      "\n",
      "Evaluation on test set:\n",
      "Average Precision@K: 0.4906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "allTruths = Y_test\n",
    "allTruths_train = Y_train\n",
    "opt_method = 'BFGS' #'Newton-CG' \n",
    "\n",
    "for p in [1, 10]:\n",
    "    for C in [0, 1]:\n",
    "        print('-------------------------------------')\n",
    "        print('p in loss: {}, C for regularisation: {}'.format(p, C))\n",
    "        allPreds  = None\n",
    "\n",
    "        w0 = np.random.rand(nFeatures * nLabels)  # initial guess\n",
    "        opt = minimize(obj_pnorm_push, w0, args=(X_train, Y_train, p, C), method=opt_method, jac=True)\n",
    "\n",
    "        if opt.success == True:\n",
    "            w = opt.x\n",
    "            allPreds = np.dot(X_test, w.reshape(nLabels, nFeatures).T)\n",
    "            allPreds_train = np.dot(X_train, w.reshape(nLabels, nFeatures).T)\n",
    "        else:\n",
    "            sys.stderr.write('Optimisation failed')\n",
    "        print('Evaluation on training set:')\n",
    "        printEvaluation(allPreds_train, allTruths_train)\n",
    "        print()\n",
    "        print('Evaluation on test set:')\n",
    "        printEvaluation(allPreds, allTruths)\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
