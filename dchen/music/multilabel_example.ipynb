{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple example of  multilable learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os, sys, time\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import cython\n",
    "import itertools\n",
    "\n",
    "from scipy.io import arff\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import check_grad\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "yeast_ftrain = os.path.join(data_dir, 'yeast/yeast-train.arff')\n",
    "yeast_ftest  = os.path.join(data_dir, 'yeast/yeast-test.arff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load yeast dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train, meta_train = arff.loadarff(yeast_ftrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test, meta_test = arff.loadarff(yeast_ftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(data_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#features: 103\n"
     ]
    }
   ],
   "source": [
    "nFeatures = np.array(list(data_train[0])[:-14], dtype=np.float).shape[0]\n",
    "print('#features:', nFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.array(list(data_train[0])[:-14], dtype=np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#labels: 14\n"
     ]
    }
   ],
   "source": [
    "nLabels = np.array(list(data_train[0])[-14:], dtype=np.int).shape[0]\n",
    "print('#labels:', nLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.array(list(data_train[0])[-14:], dtype=np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#training examples: 1500\n"
     ]
    }
   ],
   "source": [
    "print('#training examples:', len(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#test examples: 917\n"
     ]
    }
   ],
   "source": [
    "print('#test examples:', len(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of #positive labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nPositives = [np.sum(np.array(list(data_train[ix])[-14:], dtype=np.int)) for ix in range(len(data_train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7feb1de6eb38>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/i8nOrcm7yEO/AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb1e2f29e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(nPositives).hist(bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset(label_ix, data):\n",
    "    \"\"\"\n",
    "        Create the labelled dataset for a given label index\n",
    "        \n",
    "        Input:\n",
    "            - label_ix: label index, number in { 0, ..., # labels }\n",
    "            - data: original data with features + labels\n",
    "            \n",
    "        Output:\n",
    "            - (Feature, Label) pair (X, y)\n",
    "              X comprises the features for each example\n",
    "              y comprises the labels of the corresponding example\n",
    "    \"\"\"\n",
    "\n",
    "    assert(label_ix >= 0)\n",
    "    assert(label_ix < nLabels)\n",
    "\n",
    "    N = len(data)\n",
    "    d = nFeatures\n",
    "\n",
    "    X = np.zeros((N, d), dtype = np.float)\n",
    "    y = np.zeros(N, dtype = np.int)\n",
    "       \n",
    "    for i in range(N):\n",
    "        X[i, :] = list(data[i])[:-14]\n",
    "        y[i]    = list(data[i])[-14:][label_ix]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset_v2(data):\n",
    "    \"\"\"\n",
    "        Create the labelled dataset for a given label index\n",
    "        \n",
    "        Input:\n",
    "            - data: original data with features + labels\n",
    "            \n",
    "        Output:\n",
    "            - (Feature, Label) pair (X, y)\n",
    "              X comprises the features for each example\n",
    "              Y comprises the labels of the corresponding example\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(data)\n",
    "    D = nFeatures\n",
    "    L = nLabels\n",
    "\n",
    "    X = np.zeros((N, D), dtype = np.float)\n",
    "    Y = np.zeros((N, L), dtype = np.int)\n",
    "       \n",
    "    for i in range(N):\n",
    "        X[i, :] = list(data[i])[:-14]\n",
    "        Y[i, :] = list(data[i])[-14:]\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss between a ground truth and a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalPred(truth, pred, lossType = 'Hamming'):\n",
    "    \"\"\"\n",
    "        Compute loss given ground truth and prediction\n",
    "        \n",
    "        Input:\n",
    "            - truth:    binary array of true labels\n",
    "            - pred:     real-valued array of predictions\n",
    "            - lossType: can be subset 0-1, Hamming, ranking, and Precision@K where K = # positive labels.\n",
    "    \"\"\"\n",
    "\n",
    "    assert(len(truth) == len(pred))\n",
    "    L = len(truth)\n",
    "    nPos = np.sum(truth)\n",
    "    \n",
    "    predBin = np.array((pred > 0), dtype=np.int)\n",
    "    \n",
    "    if lossType == 'Subset01':\n",
    "        return 1 - int(np.all(truth == predBin))\n",
    "    \n",
    "    elif lossType == 'Hamming':\n",
    "        return np.sum(truth != predBin) / L\n",
    "    \n",
    "    elif lossType == 'Ranking':\n",
    "        loss = 0\n",
    "        for i in range(L-1):\n",
    "            for j in range(i+1, L):\n",
    "                if truth[i] > truth[j]:\n",
    "                    if pred[i] < pred[j]: \n",
    "                        loss += 1\n",
    "                    if pred[i] == pred[j]:\n",
    "                        loss += 0.5\n",
    "        #return loss / (nPos * (L-nPos))\n",
    "        return loss\n",
    "        \n",
    "    elif lossType == 'Precision@K':\n",
    "        # sorted indices of the labels most likely to be +'ve\n",
    "        idx = np.argsort(pred)[::-1]\n",
    "        \n",
    "        # true labels according to the sorted order\n",
    "        y = truth[idx]\n",
    "        \n",
    "        # fraction of +'ves in the top K predictions\n",
    "        return np.mean(y[:nPos])\n",
    "    \n",
    "    elif lossType == 'Precision@3':\n",
    "        # sorted indices of the labels most likely to be +'ve\n",
    "        idx = np.argsort(pred)[::-1]\n",
    "        \n",
    "        # true labels according to the sorted order\n",
    "        y = truth[idx]\n",
    "        \n",
    "        # fraction of +'ves in the top K predictions\n",
    "        return np.mean(y[:3])\n",
    "    \n",
    "    elif lossType == 'Precision@5':\n",
    "        # sorted indices of the labels most likely to be +'ve\n",
    "        idx = np.argsort(pred)[::-1]\n",
    "        \n",
    "        # true labels according to the sorted order\n",
    "        y = truth[idx]\n",
    "        \n",
    "        # fraction of +'ves in the top K predictions\n",
    "        return np.mean(y[:5])\n",
    "    \n",
    "    else:\n",
    "        assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avgPrecisionK(allTruths, allPreds):\n",
    "    losses = []\n",
    "    lossType = 'Precision@K'\n",
    "    for i in range(allPreds.shape[0]):\n",
    "        pred  = allPreds[i, :]\n",
    "        truth = allTruths[i, :]\n",
    "        losses.append(evalPred(truth, pred, lossType))\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printEvaluation(allTruths, allPreds):\n",
    "\n",
    "    for lossType in ['Precision@K']: \n",
    "        # ['Subset01', 'Hamming', 'Ranking', 'Precision@K', 'Precision@3', 'Precision@5']:\n",
    "        losses = [ ]\n",
    "        for i in range(allPreds.shape[0]):\n",
    "            pred  = allPreds[i, :]\n",
    "            truth = allTruths[i, :]\n",
    "            losses.append(evalPred(truth, pred, lossType))\n",
    "\n",
    "            #print(allPreds[i])\n",
    "            #print(pred)\n",
    "            #print(truth)\n",
    "            #break\n",
    "\n",
    "        #print('%24s: %1.4f' % ('Average %s Loss' % lossType, np.mean(losses)))\n",
    "        print('%s: %1.4f' % ('Average %s' % lossType, np.mean(losses)))\n",
    "        #plt.hist(aucs, bins = 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary relevance baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a logistic regression model for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers = [ LogisticRegression(class_weight = 'balanced', C = 10**0) for i in range(nLabels) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allPreds  = [ ]\n",
    "allTruths = [ ]\n",
    "coefMat = [ ]\n",
    "labelIndices = [ ]\n",
    "\n",
    "for label_ix in range(nLabels):\n",
    "    X_train, y_train = create_dataset(label_ix, data = data_train)\n",
    "    X_test, y_test   = create_dataset(label_ix, data = data_test)\n",
    "    \n",
    "    allTruths.append(y_test) \n",
    "    \n",
    "    assert( (not np.all(y_train == 0)) or (not np.all(y_train == 1)) )\n",
    "\n",
    "    classifiers[label_ix].fit(X_train, y_train)\n",
    "    allPreds.append(classifiers[label_ix].decision_function(X_test))\n",
    "\n",
    "    coefMat.append(classifiers[label_ix].coef_.reshape(-1))\n",
    "    #labelIndices.append(label_ix)\n",
    "    #print(classifiers[label_ix].coef_)\n",
    "    #print(classifiers[label_ix].intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(917, 14)\n",
      "(917, 14)\n"
     ]
    }
   ],
   "source": [
    "allPreds  = np.array(allPreds).T\n",
    "allTruths = np.array(allTruths).T\n",
    "\n",
    "print(allPreds.shape)\n",
    "print(allTruths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#allPreds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision@K: 0.5149\n"
     ]
    }
   ],
   "source": [
    "printEvaluation(allPreds, allTruths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coefficient matrix `(#Genres, #Songs)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefMat = np.array(coefMat).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 14)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefMat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sns.heatmap(coefMat[:, :30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary relevance with exponential loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a regression model with exponential loss for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obj_exp(w, X, y, C):\n",
    "    \"\"\"\n",
    "        Objective with L2 regularisation and exponential loss\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector\n",
    "            - X: feature matrix, N x D\n",
    "            - y: label vector,   N x 1\n",
    "            - C: regularisation constant\n",
    "    \"\"\"\n",
    "    assert(len(y) == X.shape[0])\n",
    "    assert(len(w) == X.shape[1])\n",
    "    assert(C >= 0)\n",
    "    \n",
    "    N, D = X.shape\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    g = np.zeros_like(w)  # gradient\n",
    "    \n",
    "    for n in range(N):\n",
    "        x = X[n, :]\n",
    "        prod = np.dot(w, x)\n",
    "        \n",
    "        # negative label\n",
    "        if y[n] == 0:\n",
    "            t1 = np.exp(prod)\n",
    "            J += t1\n",
    "            g = g + t1 * x\n",
    "        \n",
    "        # positive label\n",
    "        else:\n",
    "            t2 = np.exp(-prod)\n",
    "            J += t2\n",
    "            g = g - t2 * x\n",
    "    \n",
    "    J = 0.5 * C * np.dot(w, w) + J / N\n",
    "    g = C * w + g / N\n",
    "    \n",
    "    return (J, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_, y_train_ = create_dataset(3, data = data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.482649913762204e-06"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0 = np.random.rand(X_train_.shape[1])\n",
    "C = 1\n",
    "check_grad(lambda w: obj_exp(w, X_train_, y_train_, C)[0], \\\n",
    "           lambda w: obj_exp(w, X_train_, y_train_, C)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.995826\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "2 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.997287\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "3 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.994601\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "4 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.992873\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "5 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.996471\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "6 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.998130\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "7 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.999026\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "8 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.999114\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "9 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.999693\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "10 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.999290\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "11 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.999354\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "12 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.998675\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "13 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.998644\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "14 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.999825\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n"
     ]
    }
   ],
   "source": [
    "params    = [ ]\n",
    "allPreds  = [ ]\n",
    "allTruths = [ ]\n",
    "C = 1\n",
    "\n",
    "for label_ix in range(nLabels):\n",
    "    #sys.stdout.write('\\r%d / %d' % (label_ix + 1, nLabels))\n",
    "    #sys.stdout.flush()\n",
    "    print('\\r%d / %d ' % (label_ix + 1, nLabels))\n",
    "    \n",
    "    X_train, y_train = create_dataset(label_ix, data = data_train)\n",
    "    X_test, y_test   = create_dataset(label_ix, data = data_test)\n",
    "    \n",
    "    allTruths.append(y_test) \n",
    "    \n",
    "    assert( (not np.all(y_train == 0)) or (not np.all(y_train == 1)) )\n",
    "        \n",
    "    opt_method = 'BFGS' #'Newton-CG' \n",
    "    #opt_method = 'nelder-mead'\n",
    "    options = {'disp': True}\n",
    "    \n",
    "    w0 = np.random.rand(X_train.shape[1])  # initial guess\n",
    "    opt = minimize(obj_exp, w0, args=(X_train, y_train, C), method=opt_method, jac=True, options=options)\n",
    "    \n",
    "    if opt.success == True:\n",
    "        w = opt.x\n",
    "        params.append(w)\n",
    "        #allPreds.append(sigmoid(np.dot(X_test, w)))\n",
    "        allPreds.append(np.dot(X_test, w))\n",
    "    else:\n",
    "        sys.stderr.write('Optimisation failed, label_ix=%d\\n' % label_ix)\n",
    "        w = np.zeros(X_train.shape[1])\n",
    "        params.append(w)\n",
    "        allPreds.append(np.dot(X_test, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(917, 14)\n",
      "(917, 14)\n"
     ]
    }
   ],
   "source": [
    "allPreds = np.array(allPreds).T\n",
    "allTruths = np.array(allTruths).T\n",
    "\n",
    "print(allPreds.shape)\n",
    "print(allTruths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#allPreds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision@K: 0.4874\n"
     ]
    }
   ],
   "source": [
    "printEvaluation(allPreds, allTruths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary relevance with bipartite ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a bipartite ranking model for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%cython -a\n",
    "\n",
    "import numpy as np\n",
    "#cimport numpy as np\n",
    "\n",
    "#cpdef obj_biranking(w, X, y):\n",
    "\n",
    "def obj_biranking(w, X, y, C):\n",
    "    \"\"\"\n",
    "        Objective with L2 regularisation and bipartite ranking loss\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector\n",
    "            - X: feature matrix, N x D\n",
    "            - y: label vector,   N x 1\n",
    "            - C: regularisation constant\n",
    "    \"\"\"\n",
    "    assert(len(y) == X.shape[0])\n",
    "    assert(len(w) == X.shape[1])\n",
    "    assert(C >= 0)\n",
    "\n",
    "    #cdef int nPos, nNeg, i, j\n",
    "    #cdef double J, term, denom\n",
    "    nPos = np.sum(y)      # num of positive examples\n",
    "    nNeg = len(y) - nPos  # num of negative examples\n",
    "    \n",
    "    ixPos = np.nonzero(y)[0].tolist()                    # indices positive examples\n",
    "    ixNeg = list(set(np.arange(len(y))) - set(ixPos))    # indices negative examples\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    g = np.zeros_like(w)  # gradient    \n",
    "\n",
    "    scorePos = X[ixPos, :].dot(w)[:,np.newaxis] # nPos x 1\n",
    "    scoreNeg = X[ixNeg, :].dot(w)[:,np.newaxis] # nNeg x 1\n",
    "    scoreDif = scorePos - scoreNeg.T            # nPos x nNeg\n",
    "    #J = np.mean(np.log(1 + np.exp(-scoreDif)))\n",
    "    J = 0.5 * C * np.dot(w, w) + np.mean(np.log1p(np.exp(-scoreDif)))\n",
    "    \n",
    "    A = -1/(1 + np.exp(scoreDif))\n",
    "\n",
    "    T1 = X[ixPos, :].T.dot(A.sum(axis = 1))\n",
    "    T2 = X[ixNeg, :].T.dot(A.sum(axis = 0))\n",
    "    g  = C * w + 1/(nPos * nNeg) * (T1 - T2)\n",
    "    \n",
    "    return (J, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_, y_train_ = create_dataset(6, data = data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8448924291952094e-06"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0 = w = np.random.rand(X_train_.shape[1])\n",
    "C = 1\n",
    "check_grad(lambda w: obj_biranking(w, X_train_, y_train_, C)[0], \\\n",
    "           lambda w: obj_biranking(w, X_train_, y_train_, C)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.1331503772158218e-06 * np.sqrt(nLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.687650\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "2 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.690264\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "3 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.687208\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "4 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.684365\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "5 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.687649\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "6 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.689686\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "7 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.690316\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "8 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.690929\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "9 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.690514\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "10 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.689401\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "11 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.690708\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "12 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.690920\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "13 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.690952\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "14 / 14 \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.682634\n",
      "         Iterations: 5\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n"
     ]
    }
   ],
   "source": [
    "params    = [ ]\n",
    "allPreds  = [ ]\n",
    "allTruths = [ ]\n",
    "C = 1\n",
    "\n",
    "for label_ix in range(nLabels):\n",
    "    #sys.stdout.write('\\r%d / %d' % (label_ix + 1, nLabels))\n",
    "    #sys.stdout.flush()\n",
    "    print('\\r%d / %d ' % (label_ix + 1, nLabels))\n",
    "    \n",
    "    X_train, y_train = create_dataset(label_ix, data = data_train)\n",
    "    X_test, y_test   = create_dataset(label_ix, data = data_test)\n",
    "    \n",
    "    allTruths.append(y_test) \n",
    "    \n",
    "    assert( (not np.all(y_train == 0)) or (not np.all(y_train == 1)) )\n",
    "        \n",
    "    opt_method = 'BFGS' #'Newton-CG' \n",
    "    #opt_method = 'nelder-mead'\n",
    "    options = {'disp': True}\n",
    "    \n",
    "    w0 = np.random.rand(X_train.shape[1])  # initial guess\n",
    "    opt = minimize(obj_biranking, w0, args=(X_train, y_train, C), method=opt_method, jac=True, options=options)\n",
    "    \n",
    "    if opt.success == True:\n",
    "        w = opt.x\n",
    "        params.append(w)\n",
    "        allPreds.append(sigmoid(np.dot(X_test, w)))\n",
    "    else:\n",
    "        sys.stderr.write('Optimisation failed, label_ix=%d\\n' % label_ix)\n",
    "        w = np.zeros(X_train.shape[1])\n",
    "        params.append(w)\n",
    "        allPreds.append(np.dot(X_test, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(917, 14)\n",
      "(917, 14)\n"
     ]
    }
   ],
   "source": [
    "allPreds = np.array(allPreds).T\n",
    "allTruths = np.array(allTruths).T\n",
    "\n",
    "print(allPreds.shape)\n",
    "print(allTruths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#allPreds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision@K: 0.4400\n"
     ]
    }
   ],
   "source": [
    "printEvaluation(allPreds, allTruths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-label learning with ranking loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%cython -a\n",
    "\n",
    "import numpy as np\n",
    "#cimport numpy as np\n",
    "\n",
    "#cpdef obj_ranking(w, X, y):\n",
    "\n",
    "def obj_ranking_loop(w, X, Y, C):\n",
    "    \"\"\"\n",
    "        Objective with L2 regularisation and ranking loss\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x L\n",
    "            - C: regularisation constant\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    L = Y.shape[1]\n",
    "    assert(w.shape[0] == L * D)\n",
    "    \n",
    "    W = w.reshape(L, D)  # reshape weight matrix    \n",
    "    \n",
    "    #cdef int nPos, nNeg, i, j\n",
    "    #cdef double J, term, denom\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    for n in range(N):\n",
    "        Jn = 0.0\n",
    "        Gn = np.zeros_like(W)\n",
    "        x = X[n, :]\n",
    "        y = Y[n, :]\n",
    "        nPos = np.sum(y)   # num of positive examples\n",
    "        nNeg = L - nPos    # num of negative examples\n",
    "        denom = nPos * nNeg\n",
    "        \n",
    "        ixPos = np.nonzero(y)[0].tolist()               # indices positive examples\n",
    "        ixNeg = list(set(np.arange(L)) - set(ixPos))    # indices negative examples\n",
    "        \n",
    "        for i in ixPos:\n",
    "            for j in ixNeg:\n",
    "                wDiff = W[i, :] - W[j, :]\n",
    "                sDiff = np.dot(wDiff, x)\n",
    "                term = np.exp(sDiff)\n",
    "                Jn += np.log1p(1.0 / term)\n",
    "                Gn[i, :] = Gn[i, :] - x / (1 + term)        \n",
    "        #for j in ixNeg:\n",
    "        #    for i in ixPos:\n",
    "        #        wDiff = W[i, :] - W[j, :]\n",
    "        #        sDiff = np.dot(wDiff, x)\n",
    "        #        term = np.exp(sDiff)\n",
    "                Gn[j, :] = Gn[j, :] + x / (1 + term)\n",
    "                \n",
    "        J += Jn / denom\n",
    "        G = G + Gn / denom\n",
    "        \n",
    "    J = 0.5 * C * np.dot(w, w) + J / N\n",
    "    G = C * W + G / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.tile([1,2,3], (3,1)) * np.array([0.1, 0.2, 0.3])[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obj_ranking(w, X, Y, C):\n",
    "    \"\"\"\n",
    "        Objective with L2 regularisation and ranking loss\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x L\n",
    "            - C: regularisation constant\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    L = Y.shape[1]\n",
    "    assert(w.shape[0] == L * D)\n",
    "    \n",
    "    W = w.reshape(L, D)  # reshape weight matrix    \n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    for n in range(N):\n",
    "        Jn = 0.0\n",
    "        Gn = np.zeros_like(W)\n",
    "        x = X[n, :]\n",
    "        y = Y[n, :]\n",
    "        nPos = np.sum(y)   # num of positive examples\n",
    "        nNeg = L - nPos    # num of negative examples\n",
    "        denom = nPos * nNeg\n",
    "        \n",
    "        ixPos = np.nonzero(y)[0].tolist()               # indices positive examples\n",
    "        ixNeg = list(set(np.arange(L)) - set(ixPos))    # indices negative examples\n",
    "        \n",
    "        ixmat = np.array(list(itertools.product(ixPos, ixNeg)))  # shape: ixPos*ixNeg by 2\n",
    "        dW = W[ixmat[:, 0], :] - W[ixmat[:, 1], :]\n",
    "        sVec = np.dot(dW, x)\n",
    "        Jn = np.sum(np.log1p(np.exp(-sVec)))\n",
    "        \n",
    "        coeffVec = np.divide(1, 1 + np.exp(sVec))\n",
    "        coeffPos = pd.DataFrame(coeffVec)\n",
    "        coeffPos['gid'] = ixmat[:, 0]\n",
    "        coeffPos = coeffPos.groupby('gid', sort=False).sum()\n",
    "        coeffNeg = pd.DataFrame(coeffVec)\n",
    "        coeffNeg['gid'] = ixmat[:, 1]\n",
    "        coeffNeg = coeffNeg.groupby('gid', sort=False).sum()\n",
    "        \n",
    "        #print(coeffPos)\n",
    "        #print(coeffNeg)\n",
    "        \n",
    "        coeffs = np.ones(L)\n",
    "        coeffs[ixPos] = -coeffPos.loc[ixPos].values.squeeze()\n",
    "        coeffs[ixNeg] = coeffNeg.loc[ixNeg].values.squeeze()\n",
    "        \n",
    "        #print(coeffs)\n",
    "        Gn = np.tile(x, (L, 1)) * coeffs[:, None]\n",
    "                        \n",
    "        J += Jn / denom\n",
    "        G = G + Gn / denom\n",
    "        \n",
    "    J = 0.5 * C * np.dot(w, w) + J / N\n",
    "    G = C * W + G / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = create_dataset_v2(data = data_train)\n",
    "X_test,  Y_test  = create_dataset_v2(data = data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.8432207145699553e-05"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%script false\n",
    "C = 1\n",
    "w0 = np.random.rand(nFeatures * nLabels)\n",
    "check_grad(lambda w: obj_ranking(w, X_train[:10], Y_train[:10], C)[0], \\\n",
    "           lambda w: obj_ranking(w, X_train[:10], Y_train[:10], C)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692868\n",
      "         Iterations: 3\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n"
     ]
    }
   ],
   "source": [
    "allPreds  = None\n",
    "allTruths = Y_test\n",
    "\n",
    "opt_method = 'BFGS' #'Newton-CG' \n",
    "#opt_method = 'nelder-mead'\n",
    "options = {'disp': True}\n",
    "\n",
    "C = 1\n",
    "w = np.random.rand(nFeatures * nLabels)  # initial guess\n",
    "opt = minimize(obj_ranking, w, args=(X_train, Y_train, C), method=opt_method, jac=True, options=options)\n",
    "\n",
    "if opt.success == True:\n",
    "    w = opt.x\n",
    "    #allPreds = sigmoid(np.dot(X_test, w.reshape(nLabels, nFeatures).T))\n",
    "    allPreds = np.dot(X_test, w.reshape(nLabels, nFeatures).T)\n",
    "else:\n",
    "    sys.stderr.write('Optimisation failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(917, 14)\n",
      "(917, 14)\n"
     ]
    }
   ],
   "source": [
    "#allPreds = np.array(allPreds).T\n",
    "#allTruths = np.array(allTruths).T\n",
    "\n",
    "print(allPreds.shape)\n",
    "print(allTruths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allPreds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision@K: 0.4893\n"
     ]
    }
   ],
   "source": [
    "printEvaluation(allPreds, allTruths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p-norm push loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-label learning with p-norm push loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_pnorm_push(w, X, Y, p, C):\n",
    "    \"\"\"\n",
    "        Objective with L2 regularisation and p-norm push loss\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x L\n",
    "            - p: constant for p-norm push loss\n",
    "            - C: regularisation constant\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    L = Y.shape[1]\n",
    "    assert(w.shape[0] == L * D)\n",
    "    assert(p >= 1)\n",
    "    assert(C >= 0)\n",
    "    \n",
    "    W = w.reshape(L, D)  # reshape weight matrix\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    for n in range(N):\n",
    "        Gn = np.zeros_like(W)\n",
    "        x = X[n, :]\n",
    "        y = Y[n, :]\n",
    "        nPos = np.sum(y)   # num of positive examples\n",
    "        nNeg = L - nPos    # num of negative examples\n",
    "        \n",
    "        ixPos = np.nonzero(y)[0].tolist()               # indices positive examples\n",
    "        ixNeg = list(set(np.arange(L)) - set(ixPos))    # indices negative examples\n",
    "        \n",
    "        scalingPos = np.exp(   -np.dot(W[ixPos, :], x)) / nPos\n",
    "        scalingNeg = np.exp(p * np.dot(W[ixNeg, :], x)) / nNeg\n",
    "        \n",
    "        Gn[ixPos, :] = np.tile(-x, (nPos,1)) * scalingPos[:, None]  # scaling each row of a matrix\n",
    "        Gn[ixNeg, :] = np.tile( x, (nNeg,1)) * scalingNeg[:, None]  #         with a different scalar\n",
    "        \n",
    "        J += np.sum(scalingPos) + np.sum(scalingNeg) / p\n",
    "        G = G + Gn\n",
    "        \n",
    "    J = 0.5 * C * np.dot(w, w) + J / N\n",
    "    G = C * W + G / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obj_pnorm_push_loop(w, X, Y, p, C):\n",
    "    \"\"\"\n",
    "        Objective with L2 regularisation and p-norm push loss\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x L\n",
    "            - p: constant for p-norm push loss\n",
    "            - C: regularisation constant\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    L = Y.shape[1]\n",
    "    assert(w.shape[0] == L * D)\n",
    "    assert(p >= 1)\n",
    "    assert(C >= 0)\n",
    "    \n",
    "    W = w.reshape(L, D)  # reshape weight matrix\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    for n in range(N):\n",
    "        Gn = np.zeros_like(W)\n",
    "        x = X[n, :]\n",
    "        y = Y[n, :]\n",
    "        nPos = np.sum(y)   # num of positive examples\n",
    "        nNeg = L - nPos    # num of negative examples\n",
    "        \n",
    "        for k in range(L):\n",
    "            wk = W[k, :]\n",
    "            term = np.dot(wk, x)\n",
    "            if y[k] == 1:\n",
    "                term2 = np.exp(-term) / nPos\n",
    "                J += term2\n",
    "                Gn[k, :] = -x * term2\n",
    "            else:\n",
    "                term2 = np.exp(p * term) / nNeg\n",
    "                J += term2 / p\n",
    "                Gn[k, :] = x * term2\n",
    "        G = G + Gn\n",
    "        \n",
    "    J = 0.5 * C * np.dot(w, w) + J / N\n",
    "    G = C * W + G / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = create_dataset_v2(data = data_train)\n",
    "X_test,  Y_test  = create_dataset_v2(data = data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7422676413714687e-05"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 1\n",
    "C = 1\n",
    "w0 = np.random.rand(nFeatures * nLabels)\n",
    "check_grad(lambda w: obj_pnorm_push(w, X_train, Y_train, p, C)[0], \\\n",
    "           lambda w: obj_pnorm_push(w, X_train, Y_train, p, C)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 1.998892\n",
      "         Iterations: 4\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n"
     ]
    }
   ],
   "source": [
    "allPreds  = None\n",
    "allTruths = Y_test\n",
    "p = 1  # [1, 10]\n",
    "C = 1  # [0, 1]\n",
    "\n",
    "opt_method = 'BFGS' #'Newton-CG' \n",
    "#opt_method = 'nelder-mead'\n",
    "options = {'disp': True}\n",
    "\n",
    "w0 = np.random.rand(nFeatures * nLabels)  # initial guess\n",
    "opt = minimize(obj_pnorm_push, w0, args=(X_train, Y_train, p, C), method=opt_method, jac=True, options=options)\n",
    "\n",
    "if opt.success == True:\n",
    "    w = opt.x\n",
    "    allPreds = np.dot(X_test, w.reshape(nLabels, nFeatures).T)\n",
    "else:\n",
    "    sys.stderr.write('Optimisation failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(917, 14)\n",
      "(917, 14)\n"
     ]
    }
   ],
   "source": [
    "#allPreds = np.array(allPreds).T\n",
    "#allTruths = np.array(allTruths).T\n",
    "\n",
    "print(allPreds.shape)\n",
    "print(allTruths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#allPreds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision@K: 0.4896\n"
     ]
    }
   ],
   "source": [
    "printEvaluation(allPreds, allTruths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for different hyper-parameter configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "p in loss: 1, C for regularisation: 0\n",
      "Evaluation on training set:\n",
      "Average Precision@K: 0.5641\n",
      "\n",
      "Evaluation on test set:\n",
      "Average Precision@K: 0.4720\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 1, C for regularisation: 1\n",
      "Evaluation on training set:\n",
      "Average Precision@K: 0.5161\n",
      "\n",
      "Evaluation on test set:\n",
      "Average Precision@K: 0.4896\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 10, C for regularisation: 0\n",
      "Evaluation on training set:\n",
      "Average Precision@K: 0.5976\n",
      "\n",
      "Evaluation on test set:\n",
      "Average Precision@K: 0.4940\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 10, C for regularisation: 1\n",
      "Evaluation on training set:\n",
      "Average Precision@K: 0.5189\n",
      "\n",
      "Evaluation on test set:\n",
      "Average Precision@K: 0.4906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "allTruths = Y_test\n",
    "allTruths_train = Y_train\n",
    "opt_method = 'BFGS' #'Newton-CG' \n",
    "\n",
    "for p in [1, 10]:\n",
    "    for C in [0, 1]:\n",
    "        print('-------------------------------------')\n",
    "        print('p in loss: {}, C for regularisation: {}'.format(p, C))\n",
    "        allPreds  = None\n",
    "\n",
    "        w0 = np.random.rand(nFeatures * nLabels)  # initial guess\n",
    "        opt = minimize(obj_pnorm_push, w0, args=(X_train, Y_train, p, C), method=opt_method, jac=True)\n",
    "\n",
    "        if opt.success == True:\n",
    "            w = opt.x\n",
    "            allPreds = np.dot(X_test, w.reshape(nLabels, nFeatures).T)\n",
    "            allPreds_train = np.dot(X_train, w.reshape(nLabels, nFeatures).T)\n",
    "        else:\n",
    "            sys.stderr.write('Optimisation failed')\n",
    "        print('Evaluation on training set:')\n",
    "        printEvaluation(allPreds_train, allTruths_train)\n",
    "        print()\n",
    "        print('Evaluation on test set:')\n",
    "        printEvaluation(allPreds, allTruths)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "p in loss: 1, C for regularisation: 0\n",
      "Average Precision@K on training set:  0.5638\n",
      "Average Precision@K on test set:  0.4716\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 1, C for regularisation: 0.1\n",
      "Average Precision@K on training set:  0.5176\n",
      "Average Precision@K on test set:  0.4898\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 1, C for regularisation: 0.3\n",
      "Average Precision@K on training set:  0.5172\n",
      "Average Precision@K on test set:  0.4903\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 1, C for regularisation: 1\n",
      "Average Precision@K on training set:  0.5162\n",
      "Average Precision@K on test set:  0.4896\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 1, C for regularisation: 3\n",
      "Average Precision@K on training set:  0.5158\n",
      "Average Precision@K on test set:  0.4902\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 1, C for regularisation: 10\n",
      "Average Precision@K on training set:  0.5160\n",
      "Average Precision@K on test set:  0.4902\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 1, C for regularisation: 30\n",
      "Average Precision@K on training set:  0.5160\n",
      "Average Precision@K on test set:  0.4902\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 1, C for regularisation: 100\n",
      "Average Precision@K on training set:  0.5160\n",
      "Average Precision@K on test set:  0.4900\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 1, C for regularisation: 300\n",
      "Average Precision@K on training set:  0.5160\n",
      "Average Precision@K on test set:  0.4900\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 1, C for regularisation: 1000\n",
      "Average Precision@K on training set:  0.5160\n",
      "Average Precision@K on test set:  0.4900\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 1, C for regularisation: 3000\n",
      "Average Precision@K on training set:  0.5160\n",
      "Average Precision@K on test set:  0.4902\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 10, C for regularisation: 0\n",
      "Average Precision@K on training set:  0.5986\n",
      "Average Precision@K on test set:  0.4913\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 10, C for regularisation: 0.1\n",
      "Average Precision@K on training set:  0.5305\n",
      "Average Precision@K on test set:  0.4994\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 10, C for regularisation: 0.3\n",
      "Average Precision@K on training set:  0.5218\n",
      "Average Precision@K on test set:  0.4912\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 10, C for regularisation: 1\n",
      "Average Precision@K on training set:  0.5190\n",
      "Average Precision@K on test set:  0.4908\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 10, C for regularisation: 3\n",
      "Average Precision@K on training set:  0.5166\n",
      "Average Precision@K on test set:  0.4898\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 10, C for regularisation: 10\n",
      "Average Precision@K on training set:  0.5163\n",
      "Average Precision@K on test set:  0.4905\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 10, C for regularisation: 30\n",
      "Average Precision@K on training set:  0.5162\n",
      "Average Precision@K on test set:  0.4902\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 10, C for regularisation: 100\n",
      "Average Precision@K on training set:  0.5160\n",
      "Average Precision@K on test set:  0.4902\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 10, C for regularisation: 300\n",
      "Average Precision@K on training set:  0.5161\n",
      "Average Precision@K on test set:  0.4898\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 10, C for regularisation: 1000\n",
      "Average Precision@K on training set:  0.5159\n",
      "Average Precision@K on test set:  0.4900\n",
      "\n",
      "-------------------------------------\n",
      "p in loss: 10, C for regularisation: 3000\n",
      "Average Precision@K on training set:  0.5159\n",
      "Average Precision@K on test set:  0.4900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "precisions_train = dict()\n",
    "precisions_test = dict()\n",
    "\n",
    "allTruths_test  = Y_test\n",
    "allTruths_train = Y_train\n",
    "\n",
    "p_set = [1, 10]\n",
    "C_set = [0, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000, 3000]\n",
    "opt_method = 'BFGS' #'Newton-CG' \n",
    "\n",
    "for p in p_set:\n",
    "    for C in C_set:\n",
    "        print('-------------------------------------')\n",
    "        print('p in loss: {}, C for regularisation: {}'.format(p, C))\n",
    "        allPreds       = None\n",
    "        allPreds_train = None\n",
    "\n",
    "        w0 = np.random.rand(nFeatures * nLabels)  # initial guess\n",
    "        opt = minimize(obj_pnorm_push, w0, args=(X_train, Y_train, p, C), method=opt_method, jac=True)\n",
    "\n",
    "        if opt.success == True:\n",
    "            w = opt.x\n",
    "            allPreds_test  = np.dot(X_test,  w.reshape(nLabels, nFeatures).T)\n",
    "            allPreds_train = np.dot(X_train, w.reshape(nLabels, nFeatures).T)\n",
    "            precisions_train[(p,C)] = avgPrecisionK(allTruths_train, allPreds_train)\n",
    "            precisions_test[(p,C)]  = avgPrecisionK(allTruths_test,  allPreds_test)\n",
    "        else:\n",
    "            sys.stderr.write('Optimisation failed')\n",
    "            precisions_train[(p,C)] = 0\n",
    "            precisions_test[(p,C)]  = 0\n",
    "\n",
    "        print('%20s %.4f' % ('Average Precision@K on training set: ', precisions_train[(p,C)]))\n",
    "        print('%20s %.4f\\n' % ('Average Precision@K on test set: ', precisions_test[(p,C)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmsAAAHyCAYAAAC0zYnyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feb18d5ab00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=[10, 8])\n",
    "ax = plt.subplot(1,1,1)\n",
    "#colors = itertools.cycle(['r', 'g'])\n",
    "styles = itertools.cycle(['-', '--'])\n",
    "for p in p_set:\n",
    "    ls = styles.__next__()\n",
    "    plt.plot(np.arange(len(C_set)), [precisions_train[(p,C)] for C in C_set], ls=ls, c='r', label='p=%d'%p + ', train')\n",
    "    plt.plot(np.arange(len(C_set)), [precisions_test[(p,C)] for C in C_set], ls=ls, c='g',  label='p=%d'%p + ', test')\n",
    "plt.plot(np.arange(len(C_set)), [0.5149 for C in C_set], ls='-', c='b', label='Logistic Regression, test')\n",
    "plt.legend(loc='best')\n",
    "plt.xticks(np.arange(len(C_set)), C_set, fontsize=10, rotation=0, horizontalalignment='center')\n",
    "plt.xlabel('Regularisation Constant')\n",
    "plt.ylabel('Average Precision@K')\n",
    "plt.title('Performance on Yeast dataset, multi-label learning with p-norm push loss', fontsize=15)\n",
    "fig.savefig('pnorm.svg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
