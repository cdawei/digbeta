{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New song recommendation baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, time, gzip\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import lil_matrix, issparse, hstack, vstack\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import MTC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from tools import calc_RPrecision_HitRate\n",
    "from tools import calc_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPs = [5, 10, 20, 30, 50, 100, 200, 300, 500, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['aotm2011', '30music']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dix = 1\n",
    "dataset_name = datasets[dix]\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/%s/setting1' % dataset_name\n",
    "Y_trndev = pkl.load(gzip.open(os.path.join(data_dir, 'Y_train_dev.pkl.gz'), 'rb'))\n",
    "Y_test = pkl.load(gzip.open(os.path.join(data_dir, 'Y_test.pkl.gz'), 'rb'))\n",
    "song2pop = pkl.load(gzip.open('data/%s/setting2/song2pop.pkl.gz' % dataset_name, 'rb'))\n",
    "songsets = pkl.load(gzip.open(os.path.join(data_dir, 'songs_train_dev_test_s1.pkl.gz'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songset_trndev = songsets['train_song_set'] + songsets['dev_song_set']\n",
    "songset_test = songsets['test_song_set']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given a new song, recommend to the longest playlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random tie breaking if there are more than one longest playlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_indices = np.where(Y_test.sum(axis=0).A.reshape(-1) > 0)[0]\n",
    "lengths = Y_trndev.sum(axis=0).A.reshape(-1)[pl_indices]\n",
    "\n",
    "Y_pred = lil_matrix(Y_test.shape, dtype=np.float)\n",
    "\n",
    "np.random.seed(1234567890)\n",
    "for ix in range(len(songset_test)):\n",
    "    sort_ix = np.argsort(-lengths)\n",
    "    long_ix = [sort_ix[0]]\n",
    "    longest = lengths[sort_ix[0]]\n",
    "    for i in range(1, sort_ix.shape[0]):\n",
    "        if lengths[sort_ix[i]] < longest:\n",
    "            break\n",
    "        else:\n",
    "            short_ix.append(sort_ix[i])\n",
    "    long_ix = np.random.permutation(long_ix)\n",
    "    rec_ix = long_ix[0]\n",
    "    Y_pred[ix, pl_indices[rec_ix]] = 1\n",
    "    lengths[rec_ix] += 1\n",
    "    \n",
    "Y_pred = Y_pred.tocsc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rps_longest = []\n",
    "hitrates_longest = {top: [] for top in TOPs}\n",
    "aucs_longest = []\n",
    "\n",
    "for j in range(Y_test.shape[1]):\n",
    "    if (j+1) % 100 == 0:\n",
    "        sys.stdout.write('\\r%d / %d' % (j+1, Y_test.shape[1]))\n",
    "        sys.stdout.flush()\n",
    "    y_true = Y_test[:, j].toarray().reshape(-1)\n",
    "    if y_true.sum() < 1:\n",
    "        continue\n",
    "    y_pred = Y_pred[:, j].A.reshape(-1)\n",
    "    \n",
    "    rp, hr_dict, auc = calc_metrics(y_true, y_pred, tops=TOPs)\n",
    "    rps_longest.append(rp)\n",
    "    for top in TOPs:\n",
    "        hitrates_longest[top].append(hr_dict[top])\n",
    "    aucs_longest.append(auc)\n",
    "    \n",
    "print('\\n%d / %d' % (len(rps_longest), Y_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_perf = {dataset_name: \n",
    "               {'Test': {'R-Precision': np.mean(rps_longest), \n",
    "                         'Hit-Rate': {top: np.mean(hitrates_longest[top]) for top in hitrates_longest},\n",
    "                         'AUC': np.mean(aucs_longest)}}}\n",
    "longest_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fperf_longest = os.path.join(data_dir, 'perf-longest.pkl')\n",
    "print(fperf_longest)\n",
    "pkl.dump(longest_perf, open(fperf_longest, 'wb'))\n",
    "pkl.load(open(fperf_longest, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given a new song, recommend to the shortest playlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random tie breaking if there are more than one shortest playlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_indices = np.where(Y_test.sum(axis=0).A.reshape(-1) > 0)[0]\n",
    "lengths = Y_trndev.sum(axis=0).A.reshape(-1)[pl_indices]\n",
    "\n",
    "Y_pred = lil_matrix(Y_test.shape, dtype=np.float)\n",
    "\n",
    "np.random.seed(1234567890)\n",
    "for ix in range(len(songset_test)):\n",
    "    sort_ix = np.argsort(lengths)\n",
    "    short_ix = [sort_ix[0]]\n",
    "    shortest = lengths[sort_ix[0]]\n",
    "    for i in range(1, sort_ix.shape[0]):\n",
    "        if lengths[sort_ix[i]] > shortest:\n",
    "            break\n",
    "        else:\n",
    "            short_ix.append(sort_ix[i])\n",
    "    short_ix = np.random.permutation(short_ix)\n",
    "    rec_ix = short_ix[0]\n",
    "    Y_pred[ix, pl_indices[rec_ix]] = 1\n",
    "    lengths[rec_ix] += 1\n",
    "    \n",
    "Y_pred = Y_pred.tocsc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rps_shortest = []\n",
    "hitrates_shortest = {top: [] for top in TOPs}\n",
    "aucs_shortest = []\n",
    "ndcgs_shortest = []\n",
    "\n",
    "for j in range(Y_test.shape[1]):\n",
    "    if (j+1) % 100 == 0:\n",
    "        sys.stdout.write('\\r%d / %d' % (j+1, Y_test.shape[1]))\n",
    "        sys.stdout.flush()\n",
    "    y_true = Y_test[:, j].toarray().reshape(-1)\n",
    "    if y_true.sum() < 1:\n",
    "        continue\n",
    "    y_pred = Y_pred[:, j].A.reshape(-1)\n",
    "    \n",
    "    rp, hr_dict, auc, ndcg = calc_metrics(y_true, y_pred, tops=TOPs)\n",
    "    rps_shortest.append(rp)\n",
    "    for top in TOPs:\n",
    "        hitrates_shortest[top].append(hr_dict[top])\n",
    "    aucs_shortest.append(auc)\n",
    "    ndcgs_shortest.append(ndcg)\n",
    "    \n",
    "print('\\n%d / %d' % (len(rps_shortest), Y_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest_perf = {dataset_name: \n",
    "                 {'Test': {'R-Precision': np.mean(rps_shortest), \n",
    "                           'Hit-Rate': {top: np.mean(hitrates_shortest[top]) for top in hitrates_shortest},\n",
    "                           'AUC': np.mean(aucs_shortest),\n",
    "                           'NDCG': np.mean(ndcgs_shortest)}}}\n",
    "shortest_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fperf_shortest = os.path.join(data_dir, 'perf-shortest.pkl')\n",
    "print(fperf_shortest)\n",
    "pkl.dump(shortest_perf, open(fperf_shortest, 'wb'))\n",
    "pkl.load(open(fperf_shortest, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popularity (in test set) as song score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rps_poptest = []\n",
    "hitrates_poptest = {top: [] for top in TOPs}\n",
    "aucs_poptest = []\n",
    "ndcgs_poptest = []\n",
    "\n",
    "for j in range(Y_test.shape[1]):\n",
    "    if (j+1) % 100 == 0:\n",
    "        sys.stdout.write('\\r%d / %d' % (j+1, Y_test.shape[1]))\n",
    "        sys.stdout.flush()\n",
    "    y_true = Y_test[:, j].toarray().reshape(-1)\n",
    "    if y_true.sum() < 1:\n",
    "        continue    \n",
    "    y_pred = np.asarray([song2pop[sid] for sid, _ in songset_test])\n",
    "    \n",
    "    rp, hr_dict, auc, ndcg = calc_metrics(y_true, y_pred, tops=TOPs)\n",
    "    rps_poptest.append(rp)\n",
    "    for top in TOPs:\n",
    "        hitrates_poptest[top].append(hr_dict[top])\n",
    "    aucs_poptest.append(auc)\n",
    "    ndcgs_poptest.append(ndcg)\n",
    "    \n",
    "print('\\n%d / %d' % (len(rps_poptest), Y_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[20, 5])\n",
    "ax1 = plt.subplot(131)\n",
    "ax1.hist(rps_poptest, bins=100)\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_title('R-Precision')\n",
    "#ax.set_xlim(0, xmax)\n",
    "ax2 = plt.subplot(132)\n",
    "ax2.hist(aucs_poptest, bins=100)\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_title('AUC')\n",
    "ax3 = plt.subplot(133)\n",
    "ax3.hist(ndcgs_poptest, bins=100)\n",
    "ax3.set_yscale('log')\n",
    "ax3.set_title('NDCG')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poptest_perf = {dataset_name: {'Test': {'R-Precision': np.mean(rps_poptest), \n",
    "                                     'Hit-Rate': {top: np.mean(hitrates_poptest[top]) for top in hitrates_poptest},\n",
    "                                     'AUC': np.mean(aucs_poptest),\n",
    "                                     'NDCG': np.mean(ndcgs_poptest)}}}\n",
    "poptest_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fperf_poptest = os.path.join(data_dir, 'perf-poptest.pkl')\n",
    "print(fperf_poptest)\n",
    "pkl.dump(poptest_perf, open(fperf_poptest, 'wb'))\n",
    "pkl.load(open(fperf_poptest, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with only song popularity as feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rps_lrpop = []\n",
    "hitrates_lrpop = {top: [] for top in TOPs}\n",
    "aucs_lrpop = []\n",
    "ndcgs_lrpop = []\n",
    "\n",
    "nsong_trndev = len(songset_trndev)\n",
    "nsong_test = len(songset_test)\n",
    "for j in range(Y_test.shape[1]):\n",
    "    if (j+1) % 10 == 0:\n",
    "        sys.stdout.write('\\r%d / %d' % (j+1, Y_test.shape[1]))\n",
    "        sys.stdout.flush()\n",
    "    y_true = Y_test[:, j].toarray().reshape(-1)\n",
    "    if y_true.sum() < 1:\n",
    "        continue\n",
    "    \n",
    "    X_train = np.asarray([song2pop[sid] for sid, _ in songset_trndev]).reshape(nsong_trndev, 1)\n",
    "    Y_train = Y_trndev[:, j].A.reshape(-1)\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, Y_train)\n",
    "    X_test = np.asarray([song2pop[sid] for sid, _ in songset_test]).reshape(nsong_test, 1)\n",
    "    \n",
    "    y_pred = clf.decision_function(X_test).reshape(-1)\n",
    "    \n",
    "    rp, hr_dict, auc, ndcg = calc_metrics(y_true, y_pred, tops=TOPs)\n",
    "    rps_lrpop.append(rp)\n",
    "    for top in TOPs:\n",
    "        hitrates_lrpop[top].append(hr_dict[top])\n",
    "    aucs_lrpop.append(auc)\n",
    "    ndcgs_lrpop.append(ndcg)\n",
    "    \n",
    "print('\\n%d / %d' % (len(rps_lrpop), Y_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrpop_perf = {dataset_name: {'Test': {'R-Precision': np.mean(rps_lrpop), \n",
    "                                      'Hit-Rate': {top: np.mean(hitrates_lrpop[top]) for top in hitrates_lrpop},\n",
    "                                      'AUC': np.mean(aucs_lrpop),\n",
    "                                      'NDCG': np.mean(ndcgs_lrpop)}}}\n",
    "lrpop_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fperf_lrpop = os.path.join(data_dir, 'perf-lrpop.pkl')\n",
    "print(fperf_lrpop)\n",
    "pkl.dump(lrpop_perf, open(fperf_lrpop, 'wb'))\n",
    "pkl.load(open(fperf_lrpop, 'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
