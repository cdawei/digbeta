{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label classification -- hybrid loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, time\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import check_grad\n",
    "from scipy.special import logsumexp\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer, label_ranking_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('src')\n",
    "from evaluate import avgPrecisionK, evaluatePrecision, evaluateF1, evaluateRankingLoss, f1_score_nowarn\n",
    "from datasets import create_dataset, dataset_names, nLabels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ix = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = dataset_names[data_ix]\n",
    "nLabels = nLabels_dict[dataset_name]\n",
    "print(dataset_name, nLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "SEED = 918273645\n",
    "fmodel_base = os.path.join(data_dir, 'tph-' + dataset_name + '-base.pkl')\n",
    "fmodel_prec = os.path.join(data_dir, 'tph-' + dataset_name + '-prec.pkl')\n",
    "fperf_base = os.path.join(data_dir, 'perf-tph-base.pkl')\n",
    "fperf_prec = os.path.join(data_dir, 'perf-tph-prec.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = create_dataset(dataset_name, train_data=True, shuffle=True, random_state=SEED)\n",
    "X_test,  Y_test  = create_dataset(dataset_name, train_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mean = np.mean(X_train, axis=0).reshape((1, -1))\n",
    "X_train_std = np.std(X_train, axis=0).reshape((1, -1)) + 10 ** (-6)\n",
    "X_train -= X_train_mean\n",
    "X_train /= X_train_std\n",
    "X_test  -= X_train_mean\n",
    "X_test  /= X_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_info(X_train, Y_train, X_test, Y_test):\n",
    "    N_train, D = X_train.shape\n",
    "    K = Y_train.shape[1]\n",
    "    N_test = X_test.shape[0]\n",
    "    print('%-45s %s' % ('Number of training examples:', '{:,}'.format(N_train)))\n",
    "    print('%-45s %s' % ('Number of test examples:', '{:,}'.format(N_test)))\n",
    "    print('%-45s %s' % ('Number of features:', '{:,}'.format(D)))\n",
    "    print('%-45s %s' % ('Number of labels:', '{:,}'.format(K)))\n",
    "    avgK_train = np.mean(np.sum(Y_train, axis=1))\n",
    "    avgK_test  = np.mean(np.sum(Y_test, axis=1))\n",
    "    print('%-45s %.3f (%.2f%%)' % ('Average number of positive labels (train):', avgK_train, 100*avgK_train / K))\n",
    "    print('%-45s %.3f (%.2f%%)' % ('Average number of positive labels (test):', avgK_test, 100*avgK_test / K))\n",
    "    #print('%-45s %.4f%%' % ('Average label occurrence (train):', np.mean(np.sum(Y_train, axis=0)) / N_train))\n",
    "    #print('%-45s %.4f%%' % ('Average label occurrence (test):', np.mean(np.sum(Y_test, axis=0)) / N_test))\n",
    "    print('%-45s %.3f%%' % ('Sparsity (percent) (train):', 100 * np.sum(Y_train) / np.prod(Y_train.shape)))\n",
    "    print('%-45s %.3f%%' % ('Sparsity (percent) (test):', 100 * np.sum(Y_test) / np.prod(Y_test.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%-45s %s' % ('Dataset:', dataset_name))\n",
    "print_dataset_info(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate `max()` using `log-sum-exp()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "xs = np.random.rand(500000).reshape(10, 50000) * np.arange(1, 11)[:, None]\n",
    "maxes = np.max(xs, axis=1)\n",
    "print(xs.shape)\n",
    "print(maxes.shape)\n",
    "\n",
    "#rs = np.array([0.5, 1, 2, 4, 8, 16, 32, 64])\n",
    "rs = np.array([4, 8, 16, 32, 64])\n",
    "mses = []\n",
    "for r in rs:\n",
    "    approx = []\n",
    "    for i in range(xs.shape[0]):\n",
    "        approx.append(np.log(np.sum(np.exp(r * xs[i, :]))) / r)\n",
    "    deltas = np.array(approx) - maxes\n",
    "    mses.append(np.dot(deltas, deltas))\n",
    "\n",
    "#fig = plt.Figure(figsize=[20, 12])\n",
    "plt.plot(rs, mses, ls='--', marker='o', c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top-push loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-label learning with top push loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_toppush_example(w, X, Y, r=1, weighting=True):\n",
    "    \"\"\"\n",
    "        Objective of top push loss for examples\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "            - r: parameter for log-sum-exp approximation\n",
    "            - weighting: if True, divide K+ in top-push loss\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "    assert(r > 0)\n",
    "    \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    # instead of using diagonal matrix to scale each row of a matrix with a different factor,\n",
    "    # we use Mat * Vec[:, None] which is more memory efficient\n",
    "    \n",
    "    if weighting is True:\n",
    "        KPosAll = np.sum(Y, axis=1)  # number of positive labels for each example, N by 1\n",
    "    else:\n",
    "        KPosAll = np.ones(N)\n",
    "        \n",
    "    A_diag = 1.0 / KPosAll\n",
    "    AY = Y * A_diag[:, None]\n",
    "    \n",
    "    T1 = np.dot(X, W.T)  # N by K\n",
    "    #m0 = np.max(T1)  # underflow in np.exp(r*T1 - m1)\n",
    "    m0 = 0.5 * (np.max(T1) + np.min(T1))\n",
    "    m1 = r * m0\n",
    "    #print('----------------')\n",
    "    #print(np.min(T1), np.max(T1), m0)\n",
    "    #print(np.min(r*T1), np.max(r*T1), m1)\n",
    "    #print(np.min(r * T1 - m1), np.max(r * T1 - m1))\n",
    "    T2 = np.multiply(1 - Y, np.exp(r * T1 - m1))  # N by K\n",
    "    B_tilde_diag = np.dot(T2, np.ones(K))\n",
    "    #print(np.max(B_tilde_diag), np.min(B_tilde_diag))  # big numbers here, can cause overflow in T3\n",
    "    \n",
    "    #T3 = np.exp(-T1 + m0) * np.power(B_tilde_diag, 1.0 / r)[:, None]\n",
    "    #T4 = np.multiply(AY, np.log1p(T3))\n",
    "    T3 = (-T1 + m0) + (1.0 / r) * np.log(B_tilde_diag)[:, None]\n",
    "    #print(np.min(T3), np.max(T3))\n",
    "    m2 = 0.5 * (np.min(T3) + np.max(T3))\n",
    "    #T4 = np.logaddexp(0, T3)\n",
    "    T4 = np.logaddexp(-m2, T3-m2) + m2\n",
    "    T5 = np.multiply(AY, T4)  \n",
    "    \n",
    "    #J = np.dot(w, w) * 0.5 / C + np.dot(np.ones(N), np.dot(T5, np.ones(K))) / N\n",
    "    J = np.dot(np.ones(N), np.dot(T5, np.ones(K))) / N\n",
    "    \n",
    "    #T5 = 1.0 / (1.0 + np.divide(1.0, T3))\n",
    "    #T5 = np.divide(T3, 1 + T3)\n",
    "    T6 = np.exp(T3 - T4)\n",
    "    O_diag = np.dot(np.multiply(Y, T6), np.ones(K))\n",
    "    T7 = A_diag * (1.0 / B_tilde_diag) * O_diag\n",
    "    \n",
    "    G1 = np.dot(np.multiply(AY, T6).T, -X)\n",
    "    \n",
    "    #print(np.max(T2), np.min(T2), np.max(T7), np.min(T7))\n",
    "    T8 = T2 * T7[:, None]\n",
    "    G2 = np.dot(T8.T, X)\n",
    "    \n",
    "    #G = W / C + (G1 + G2) / N\n",
    "    G = (G1 + G2) / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_toppush_example_loop(w, X, Y, r=1, weighting=True):\n",
    "    \"\"\"\n",
    "        Objective of top push loss for examples\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "            - C: regularisation constant, C = 1 / lambda\n",
    "            - r: parameter for log-sum-exp approximation\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "    assert(r > 0)\n",
    "    \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    if weighting is True:\n",
    "        KPosAll = np.sum(Y, axis=1)  # number of positive labels for each example, N by 1\n",
    "    else:\n",
    "        KPosAll = np.ones(N)\n",
    "    \n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            if Y[n, k] == 1:\n",
    "                s1 = np.sum([np.exp(r * np.dot(W[j, :] - W[k, :], X[n, :])) for j in range(K) if Y[n, j] == 0])\n",
    "                J += np.log1p(np.power(s1, 1.0 / r)) / KPosAll[n]\n",
    "    #J = np.dot(w, w) * 0.5 / C + J / N\n",
    "    J = J / N\n",
    "    \n",
    "    for k in range(K):\n",
    "        for n in range(N):\n",
    "            if Y[n, k] == 1:\n",
    "                t1 = np.sum([np.exp(r * np.dot(W[j, :] - W[k, :], X[n, :])) for j in range(K) if Y[n, j] == 0])\n",
    "                t2 = -1.0 / (1 + np.power(t1, -1.0 / r))\n",
    "                G[k, :] = G[k, :] + X[n, :] * t2 / KPosAll[n]\n",
    "            else:\n",
    "                sk = 0.0\n",
    "                for k1 in range(K):\n",
    "                    if Y[n, k1] == 1:\n",
    "                        t3 = np.sum([np.exp(r * np.dot(W[j,:] - W[k1, :], X[n, :])) \\\n",
    "                                     for j in range(K) if Y[n, j] == 0])\n",
    "                        t4 = np.exp(r * np.dot(W[k, :] - W[k1, :], X[n, :]))\n",
    "                        sk += t4 / (np.power(t3, 1.0 - 1.0 / r) + t3)\n",
    "                G[k, :] = G[k, :] + X[n, :] * sk / KPosAll[n]\n",
    "                        \n",
    "    #G = W / C + G / N\n",
    "    G = G / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_toppush_label_loop(w, X, Y, r=1, weighting=True):\n",
    "    \"\"\"\n",
    "        Objective of top push loss for each label\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "            - r1: parameter for log-sum-exp approximation\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "    assert(r > 0)\n",
    "    \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    if weighting is True:\n",
    "        NPosAll = np.sum(Y, axis=0)  # number of positive examples for each label, K by 1\n",
    "    else:\n",
    "        NPosAll = np.ones(K)\n",
    "    \n",
    "    for k in range(K):\n",
    "        Jk = 0.0\n",
    "        posInd = np.nonzero(Y[:, k])[0].tolist()\n",
    "        negInd = sorted(set(np.arange(N).tolist()) - set(posInd))\n",
    "        for p in posInd:\n",
    "            t1 = np.sum([np.exp(r * np.dot(W[k, :], X[q, :] - X[p, :])) for q in negInd])\n",
    "            Jk += np.log1p(np.power(t1, 1.0/r))\n",
    "            #t1 = -np.dot(W[k, :], X[p, :]) + logsumexp([r * np.dot(W[k, :], X[q, :]) for q in negInd]) / r\n",
    "            #Jk += np.logaddexp(0, t1)\n",
    "            t2 = np.power(t1, 1.0-1.0/r) + t1\n",
    "            vk = np.zeros(D)\n",
    "            for q in negInd:\n",
    "                vk = vk + np.exp(r * np.dot(W[k, :], X[q, :] - X[p, :])) * (X[q, :] - X[p, :])\n",
    "            G[k, :] = G[k, :] + vk / t2\n",
    "        J += Jk / NPosAll[k]\n",
    "        G[k, :] = G[k, :] / NPosAll[k]\n",
    "        \n",
    "    J = J / K\n",
    "    G = G / K\n",
    "        \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_toppush_example(w, X, Y, r=1, weighting=True):\n",
    "    \"\"\"\n",
    "        Objective of top push loss for examples\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "            - r: parameter for log-sum-exp approximation\n",
    "            - weighting: if True, divide K+ in top-push loss\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "    assert(r > 0)\n",
    "    \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    # instead of using diagonal matrix to scale each row of a matrix with a different factor,\n",
    "    # we use Mat * Vec[:, None] which is more memory efficient\n",
    "    \n",
    "    if weighting is True:\n",
    "        KPosAll = np.sum(Y, axis=1)  # number of positive labels for each example, N by 1\n",
    "    else:\n",
    "        KPosAll = np.ones(N)\n",
    "        \n",
    "    A_diag = 1.0 / KPosAll\n",
    "    AY = Y * A_diag[:, None]\n",
    "    \n",
    "    T1 = np.dot(X, W.T)  # N by K\n",
    "    #m0 = np.max(T1)  # underflow in np.exp(r*T1 - m1)\n",
    "    m0 = 0.5 * (np.max(T1) + np.min(T1))\n",
    "    m1 = r * m0\n",
    "    #print('----------------')\n",
    "    #print(np.min(T1), np.max(T1), m0)\n",
    "    #print(np.min(r*T1), np.max(r*T1), m1)\n",
    "    #print(np.min(r * T1 - m1), np.max(r * T1 - m1))\n",
    "    T2 = np.multiply(1 - Y, np.exp(r * T1 - m1))  # N by K\n",
    "    B_tilde_diag = np.dot(T2, np.ones(K))\n",
    "    #print(np.max(B_tilde_diag), np.min(B_tilde_diag))  # big numbers here, can cause overflow in T3\n",
    "    \n",
    "    #T3 = np.exp(-T1 + m0) * np.power(B_tilde_diag, 1.0 / r)[:, None]\n",
    "    #T4 = np.multiply(AY, np.log1p(T3))\n",
    "    T3 = (-T1 + m0) + (1.0 / r) * np.log(B_tilde_diag)[:, None]\n",
    "    #print(np.min(T3), np.max(T3))\n",
    "    m2 = 0.5 * (np.min(T3) + np.max(T3))\n",
    "    #T4 = np.logaddexp(0, T3)\n",
    "    T4 = np.logaddexp(-m2, T3-m2) + m2\n",
    "    T5 = np.multiply(AY, T4)  \n",
    "    \n",
    "    #J = np.dot(w, w) * 0.5 / C + np.dot(np.ones(N), np.dot(T5, np.ones(K))) / N\n",
    "    J = np.dot(np.ones(N), np.dot(T5, np.ones(K))) / N\n",
    "    \n",
    "    #T5 = 1.0 / (1.0 + np.divide(1.0, T3))\n",
    "    #T5 = np.divide(T3, 1 + T3)\n",
    "    T6 = np.exp(T3 - T4)\n",
    "    O_diag = np.dot(np.multiply(Y, T6), np.ones(K))\n",
    "    T7 = A_diag * (1.0 / B_tilde_diag) * O_diag\n",
    "    \n",
    "    G1 = np.dot(np.multiply(AY, T6).T, -X)\n",
    "    \n",
    "    #print(np.max(T2), np.min(T2), np.max(T7), np.min(T7))\n",
    "    T8 = T2 * T7[:, None]\n",
    "    G2 = np.dot(T8.T, X)\n",
    "    \n",
    "    #G = W / C + (G1 + G2) / N\n",
    "    G = (G1 + G2) / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_toppush_label(w, X, Y, r=1, weighting=True):\n",
    "    \"\"\"\n",
    "        Objective with top push loss for labels\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "            - r: parameter for log-sum-exp approximation\n",
    "            - weighting: if True, divide N+ in top-push loss\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "    assert(r > 0)\n",
    "    \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    # instead of using diagonal matrix to scale each row of a matrix with a different factor,\n",
    "    # we use Mat * Vec[:, None] which is more memory efficient\n",
    "    \n",
    "    if weighting is True:\n",
    "        NPosAll = np.sum(Y, axis=0)  # number of positive examples for each label, K by 1\n",
    "    else:\n",
    "        NPosAll = np.ones(K)\n",
    "    P_diag = 1.0 / NPosAll\n",
    "        \n",
    "    T1 = np.dot(X, W.T)  # N by K\n",
    "    T11 = np.multiply(1-Y, T1)\n",
    "    m0 = 0.5 * (np.max(T11) + np.min(T11))\n",
    "    m1 = r * m0\n",
    "    #print(np.max(T11), np.min(T11))\n",
    "    Q_diag = np.dot(np.ones(N), np.multiply(1-Y, np.exp(r*T11-m1)))  # K by 1\n",
    "    Q1 = np.power(Q_diag, 1/r)  # K by 1\n",
    "    T2 = np.multiply(np.exp(-T1+m0), Y).T * Q1[:, None]  # K by N\n",
    "    T3 = np.log1p(T2) * P_diag[:, None]     # K by N\n",
    "    J = np.dot(np.dot(np.ones(N), T3.T), np.ones(K)) / K\n",
    "    \n",
    "    Denom = np.multiply(Y, np.exp(T1-m0)).T * np.divide(1, Q1)[:, None] + 1  # K by N\n",
    "    T4 = np.einsum('nk,nk->k', 1-Y, np.exp(r*T11-m1))  # K by 1\n",
    "    T5 = np.multiply(1-Y, np.exp(r*T11-m1))  # N by K\n",
    "    T6 = np.dot(T5.T, X)  # K by D\n",
    "    T7 = T6 * np.divide(1, T4)[:, None]  # K by D\n",
    "    T8 = np.einsum('nk,nk->k', Y, np.divide(1, Denom).T)  # K by 1\n",
    "    G1 = T7 * T8[:, None]  # K by D\n",
    "    \n",
    "    T9 = np.multiply(Y, np.divide(1, Denom).T)  # N by K\n",
    "    G2 = np.dot(T9.T, X)  # K by D\n",
    "    \n",
    "    G = (G1 - G2) * P_diag[:, None] / K\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w0 = 0.001 * np.random.randn(Y_train.shape[1] * X_train.shape[1])\n",
    "#check_grad(lambda w: obj_toppush_label(w, X_train, Y_train, r=4)[0], \n",
    "#           lambda w: obj_toppush_label(w, X_train, Y_train, r=4)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp_loop_vec(func_loop, func_vec, X_train, Y_train, r=4):\n",
    "    print('%15s %15s %15s %15s %15s' % ('C','J_Diff', 'J_loop', 'J_vec', 'G_Diff'))\n",
    "    w0 = 0.001 * np.random.randn(Y_train.shape[1] * X_train.shape[1])\n",
    "    for e in range(-6, 10):\n",
    "        C = 10**(e)\n",
    "        #w0 = init_var(X_train, Y_train)\n",
    "        J,  G  = func_loop(w0, X_train, Y_train)#, r=r)\n",
    "        J1, G1 = func_vec(w0, X_train, Y_train)#, r=r)\n",
    "        Gdiff = G1 - G\n",
    "        #print('%-15g %-15g %-15g' % (J1 - J, J, J1))\n",
    "        print('%15g %15g %15g %15g %15g' % (C, J1 - J, J, J1, np.dot(Gdiff, Gdiff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grad_loop(func, X_train, Y_train, r=4):\n",
    "    w0 = 0.001 * np.random.randn(Y_train.shape[1] * X_train.shape[1])\n",
    "    eps = 1.49e-08\n",
    "    w = np.zeros_like(w0)\n",
    "    for i in range(len(w0)):\n",
    "        sys.stdout.write('\\r%d / %d' % (i+1, len(w0)))\n",
    "        wi1 = w0.copy()\n",
    "        wi2 = w0.copy()\n",
    "        wi1[i] = wi1[i] - eps\n",
    "        wi2[i] = wi2[i] + eps\n",
    "        J1, _ = func(wi1, X_train, Y_train, r=r)\n",
    "        J2, _ = func(wi2, X_train, Y_train, r=r)\n",
    "        w[i] = (J2 - J1) / (2 * eps)\n",
    "        #print(w[i])\n",
    "    J, w1 = obj_toppush_loop(w0, X_train, Y_train, C)\n",
    "    diff = w1 - w\n",
    "    return np.sqrt(np.dot(diff, diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cmp_loop_vec(obj_toppush_label_loop, obj_toppush_label, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_grad_loop(obj_toppush_label_loop, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w0 = 0.001 * np.random.randn(Y_train.shape[1] * X_train.shape[1])\n",
    "#check_grad(lambda w: obj_toppush_label_loop(w, X_train, Y_train, r=4)[0], \n",
    "#           lambda w: obj_toppush_label_loop(w, X_train, Y_train, r=4)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_xentropy(w, X, Y, weighting=True, ignorePos=False):\n",
    "    \"\"\"\n",
    "    Objective with logistic loss\n",
    "    \n",
    "    Input:\n",
    "            - w: current weight vector, flattened K x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "        \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    if weighting is True:\n",
    "        NK = N * K\n",
    "    else:\n",
    "        NK = N\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    T1 = np.dot(W, X.T)  # K by N\n",
    "    T2 = np.exp(T1)\n",
    "    T3 = np.divide(T2, 1+T2)\n",
    "    T4 = np.log1p(T2)\n",
    "    T5 = np.log1p(np.divide(1.0, T2))\n",
    "    T6 = np.multiply(Y.T, T5-T4)\n",
    "    if not ignorePos:\n",
    "        T7 = T4 + T6  # K by N\n",
    "    else:\n",
    "        T7 = T6\n",
    "    \n",
    "    J = np.dot(np.ones(K), np.dot(T7, np.ones(N))) / NK\n",
    "    \n",
    "    if not ignorePos:\n",
    "        G = np.dot(T3-Y.T, X) / NK\n",
    "    else:\n",
    "        G = np.dot(-Y.T, X) / NK\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_xentropy_loop(w, X, Y, weighting=True, ignorePos=False):\n",
    "    \"\"\"\n",
    "    Objective with logistic loss\n",
    "    \n",
    "    Input:\n",
    "            - w: current weight vector, flattened K x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "    \"\"\"    \n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "        \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    if weighting is True:\n",
    "        NK = N * K\n",
    "    else:\n",
    "        NK = N\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    for k in range(K):\n",
    "        for n in range(N):\n",
    "            t1 = np.exp(np.dot(W[k, :], X[n, :]))\n",
    "            t2 = np.log1p(t1)\n",
    "            if not ignorePos:\n",
    "                J += t2\n",
    "            if Y[n, k] == 1:\n",
    "                J += (np.log1p(1.0 / t1) - t2)\n",
    "            if not ignorePos:\n",
    "                G[k, :] = G[k, :] + X[n, :] * (t1 / (1 + t1) - Y[n, k])\n",
    "            else:\n",
    "                G[k, :] = G[k, :] + X[n, :] * (-Y[n, k])\n",
    "                \n",
    "    J = J / NK\n",
    "    G = G / NK\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w0 = 0.001 * np.random.randn(Y_train.shape[1] * X_train.shape[1])\n",
    "#check_grad(lambda w: obj_xentropy(w, X_train, Y_train, ignorePos=True)[0], \n",
    "#           lambda w: obj_xentropy(w, X_train, Y_train, ignorePos=True)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cmp_loop_vec(obj_xentropy_loop, obj_xentropy, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_hybrid_TP_LR(w, X, Y, C, C1, r=8, weighting=True):\n",
    "    \"\"\"\n",
    "    Objective with L2 regularisation and top push loss\n",
    "    \"\"\"\n",
    "    \n",
    "    assert C > 0\n",
    "    assert C1 > 0\n",
    "    assert r > 0\n",
    "    \n",
    "    J1, G1 = obj_toppush_example(w, X, Y, r, weighting)\n",
    "    J2, G2 = obj_xentropy(w, X, Y)\n",
    "    \n",
    "    J = np.dot(w, w) * 0.5 / C + J1 + C1 * J2\n",
    "    G = w / C + G1 + C1 * G2\n",
    "    \n",
    "    return (J, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_hybrid_TP_LR2(w, X, Y, C, C1, r=8, weighting=True):\n",
    "    \"\"\"\n",
    "    Objective with L2 regularisation and top push loss\n",
    "    \"\"\"\n",
    "    \n",
    "    assert C > 0\n",
    "    assert C1 > 0\n",
    "    assert r > 0\n",
    "    \n",
    "    J1, G1 = obj_toppush_example(w, X, Y, r, weighting)\n",
    "    J2, G2 = obj_xentropy(w, X, Y, ignorePos=True)\n",
    "    \n",
    "    J = np.dot(w, w) * 0.5 / C + J1 + C1 * J2\n",
    "    G = w / C + G1 + C1 * G2\n",
    "    \n",
    "    return (J, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_hybrid_TP_TP(w, X, Y, C, C1=1, r=8, weighting=True):\n",
    "    \"\"\"\n",
    "    Objective with L2 regularisation and top push loss\n",
    "    \"\"\"\n",
    "    \n",
    "    assert C > 0\n",
    "    assert C1 > 0\n",
    "    assert r > 0\n",
    "    \n",
    "    J1, G1 = obj_toppush_example(w, X, Y, r, weighting)\n",
    "    J2, G2 = obj_toppush_label(w, X, Y, r, weighting)\n",
    "    \n",
    "    J = np.dot(w, w) * 0.5 / C + J1 + C1 * J2\n",
    "    G = w / C + G1 + C1 * G2\n",
    "    \n",
    "    return (J, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_hybrid_LR_TP(w, X, Y, C, C1=1, r=8, weighting=True):\n",
    "    \"\"\"\n",
    "    Objective with L2 regularisation and top push loss\n",
    "    \"\"\"\n",
    "    \n",
    "    assert C > 0\n",
    "    assert C1 > 0\n",
    "    assert r > 0\n",
    "    \n",
    "    J1, G1 = obj_xentropy(w, X, Y)\n",
    "    J2, G2 = obj_toppush_label(w, X, Y, r, weighting)\n",
    "    \n",
    "    J = np.dot(w, w) * 0.5 / C + J1 + C1 * J2\n",
    "    G = w / C + G1 + C1 * G2\n",
    "    \n",
    "    return (J, G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "#X_train = X_train[:50, :]\n",
    "#Y_train = Y_train[:50, :]\n",
    "C = 1\n",
    "C1 = 1\n",
    "w0 = 0.001 * np.random.randn(Y_train.shape[1] * X_train.shape[1])\n",
    "#obj_func = obj_hybrid_TP_LR\n",
    "#obj_func = obj_hybrid_TP_TP\n",
    "#obj_func = obj_hybrid_LR_TP\n",
    "obj_func = obj_hybrid_TP_LR2\n",
    "\n",
    "check_grad(lambda w: obj_func(w, X_train, Y_train, C, C1, r=8)[0], \n",
    "           lambda w: obj_func(w, X_train, Y_train, C, C1, r=8)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLC_hybrid(BaseEstimator):\n",
    "    \"\"\"All methods are necessary for a scikit-learn estimator\"\"\"\n",
    "    \n",
    "    def __init__(self, C=1, C1=1, r=1, weighting=True):\n",
    "        \"\"\"Initialisation\"\"\"\n",
    "        \n",
    "        assert C > 0\n",
    "        assert C1 > 0\n",
    "        assert r > 0\n",
    "        assert type(weighting) == bool\n",
    "        self.C = C\n",
    "        self.C1 = C1\n",
    "        self.r = r\n",
    "        self.weighting = weighting\n",
    "        #self.obj_func = obj_hybrid_TP_LR\n",
    "        #self.obj_func = obj_hybrid_LR_TP\n",
    "        #self.obj_func = obj_hybrid_TP_TP\n",
    "        self.obj_func = obj_hybrid_TP_LR2\n",
    "        self.trained = False\n",
    "        \n",
    "    def fit(self, X_train, Y_train):\n",
    "        \"\"\"Model fitting by optimising the objective\"\"\"\n",
    "        opt_method = 'L-BFGS-B' #'BFGS' #'Newton-CG'\n",
    "        options = {'disp': 1, 'maxiter': 10**5, 'maxfun': 10**5} # , 'iprint': 99}\n",
    "        print('\\nC: %g, C1: %g, r: %g, weighting: %s' % (self.C, self.C1, self.r, self.weighting))\n",
    "            \n",
    "        N, D = X_train.shape\n",
    "        K = Y_train.shape[1]\n",
    "        #w0 = np.random.rand(K * D) - 0.5  # initial guess in range [-1, 1]\n",
    "        w0 = 0.001 * np.random.randn(K * D)\n",
    "        opt = minimize(self.obj_func, w0, args=(X_train, Y_train, self.C, self.C1, self.r, self.weighting), \\\n",
    "                       method=opt_method, jac=True, options=options)\n",
    "        if opt.success is True:\n",
    "            self.W = np.reshape(opt.x, (K, D))\n",
    "            self.trained = True\n",
    "        else:\n",
    "            sys.stderr.write('Optimisation failed')\n",
    "            print(opt.items())\n",
    "            self.trained = False\n",
    "            \n",
    "            \n",
    "    def decision_function(self, X_test):\n",
    "        \"\"\"Make predictions (score is real number)\"\"\"\n",
    "        \n",
    "        assert self.trained is True, \"Can't make prediction before training\"\n",
    "        D = X_test.shape[1]\n",
    "        return np.dot(X_test, self.W.T)\n",
    "        \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return self.decision_function(X_test)\n",
    "    #    \"\"\"Make predictions (score is boolean)\"\"\"   \n",
    "    #    preds = sigmoid(self.decision_function(X_test))\n",
    "    #    #return (preds >= 0)\n",
    "    #    assert self.TH is not None\n",
    "    #    return preds >= self.TH        \n",
    "        \n",
    "    # inherit from BaseEstimator instead of re-implement\n",
    "    #\n",
    "    #def get_params(self, deep = True):\n",
    "    #def set_params(self, **params):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_results(predictor, X_train, Y_train, X_test, Y_test, fname, rankingLoss=False):\n",
    "    \"\"\"\n",
    "        Compute and save performance results\n",
    "    \"\"\"\n",
    "    preds_train = predictor.decision_function(X_train)\n",
    "    preds_test  = predictor.decision_function(X_test)\n",
    "    \n",
    "    print('Training set:')\n",
    "    perf_dict_train = evaluatePrecision(Y_train, preds_train, verbose=1)\n",
    "    print()\n",
    "    print('Test set:')\n",
    "    perf_dict_test = evaluatePrecision(Y_test, preds_test, verbose=1)\n",
    "    \n",
    "    if rankingLoss is True:\n",
    "        print()\n",
    "        print('Training set:')\n",
    "        perf_dict_train.update(evaluateRankingLoss(Y_train, preds_train))\n",
    "        print(label_ranking_loss(Y_train, preds_train))\n",
    "        print()\n",
    "        print('Test set:')\n",
    "        perf_dict_test.update(evaluateRankingLoss(Y_test, preds_test))\n",
    "        print(label_ranking_loss(Y_test, preds_test))\n",
    "\n",
    "    # compute F1 score w.r.t. different thresholds\n",
    "    #TH1 = predictor.cv_results_['mean_test_TH'][clf.best_index_]\n",
    "    #TH2 = np.mean(Y_train, axis=0)\n",
    "    #TH3 = np.mean(TH2)\n",
    "    \n",
    "    #preds_train_bin = sigmoid(preds_train)\n",
    "    #preds_test_bin  = sigmoid(preds_test)\n",
    "    \n",
    "    #F1_train1 = f1_score_nowarn(Y_train, sigmoid(preds_train) >= TH1, average='samples')\n",
    "    #F1_test1  = f1_score_nowarn(Y_test, sigmoid(preds_test) >= TH1, average='samples')\n",
    "    #print('\\nTrain: %.4f, %f' % (F1_train1, f1_score(Y_train, sigmoid(preds_train) >= TH1, average='samples')))\n",
    "    #print('\\nTest : %.4f, %f' % (F1_test1, f1_score(Y_test, sigmoid(preds_test) >= TH1, average='samples')))\n",
    "    \n",
    "    #F1_train2 = f1_score_nowarn(Y_train, (preds_train_bin - TH2) >= 0, average='samples')\n",
    "    #F1_test2  = f1_score_nowarn(Y_test, (preds_test_bin - TH2) >= 0, average='samples')\n",
    "    #print('\\nTrain: %.4f, %f' % (F1_train2, f1_score(Y_train, (preds_train_bin - TH2) >= 0, average='samples')))\n",
    "    #print('\\nTest : %.4f, %f' % (F1_test2, f1_score(Y_test, (preds_test_bin - TH2) >= 0, average='samples')))\n",
    "    \n",
    "    #F1_train3 = f1_score_nowarn(Y_train, preds_train_bin >= TH3, average='samples')\n",
    "    #F1_test3  = f1_score_nowarn(Y_test, preds_test_bin >= TH3, average='samples')\n",
    "    #print('\\nTrain: %.4f, %f' % (F1_train3, f1_score(Y_train, preds_train_bin >= TH3, average='samples')))\n",
    "    #print('\\nTest : %.4f, %f' % (F1_test3, f1_score(Y_test, preds_test_bin >= TH3, average='samples')))\n",
    "    \n",
    "    #perf_dict_train.update({'F1': [(F1_train1,), (F1_train2,), (F1_train3,)]})\n",
    "    #perf_dict_test.update( {'F1': [(F1_test1,),  (F1_test2,),  (F1_test3,)]})\n",
    "    #perf_dict_train.update({'F1': [(F1_train2,), (F1_train3,)]})\n",
    "    #perf_dict_test.update( {'F1': [(F1_test2,),  (F1_test3,)]})\n",
    "    \n",
    "    perf_dict = {'Train': perf_dict_train, 'Test': perf_dict_test}\n",
    "    if os.path.exists(fname):\n",
    "        _dict = pkl.load(open(fname, 'rb'))\n",
    "        if dataset_name not in _dict:\n",
    "            _dict[dataset_name] = perf_dict\n",
    "    else:\n",
    "        _dict = {dataset_name: perf_dict}\n",
    "    pkl.dump(_dict, open(fname, 'wb'))\n",
    "    \n",
    "    print()\n",
    "    print(pkl.load(open(fname, 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_settings = np.seterr(all='ignore')  # seterr to known value\n",
    "np.seterr(all='raise')\n",
    "#np.seterr(all='ignore')\n",
    "#np.seterr(**old_settings)  # restore settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%memit model.fit(X_train[:30], Y_train[:30])\n",
    "#%mprun -f minimize model.fit(X_train[:100], Y_train[:100])\n",
    "#%mprun -f _minimize_slsqp model.fit(X_train[:10], Y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "if os.path.exists(fmodel_base):\n",
    "    clf = pkl.load(open(fmodel_base, 'rb'))\n",
    "else:\n",
    "    clf = clf = MLC_hybrid()\n",
    "    clf.fit(X_train, Y_train)\n",
    "    pkl.dump(clf, open(fmodel_base, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump_results(clf, X_train, Y_train, X_test, Y_test, fperf_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation w.r.t. average precision@K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ranges = range(-6, 7)\n",
    "#ranges = range(-6, 5)\n",
    "#parameters = [{'C': sorted([10**(e) for e in ranges] + [3 * 10**(e) for e in ranges]),\n",
    "parameters = [{'C': [1e-3, 3e-3, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300],#, 1e3],\n",
    "               'C1': [0.5, 1, 2],\n",
    "               'r': [8],\n",
    "               'weighting': [True, False],\n",
    "              }]\n",
    "scorer = {'Prec': make_scorer(avgPrecisionK)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(fmodel_prec):\n",
    "    clf = GridSearchCV(MLC_hybrid(), parameters, scoring=scorer, cv=5, n_jobs=1, refit='Prec')\n",
    "    clf.fit(X_train, Y_train)\n",
    "    #pkl.dump(clf, open(fmodel_prec, 'wb'))\n",
    "else:\n",
    "    clf = pkl.load(open(fmodel_prec, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_results(clf, X_train, Y_train, X_test, Y_test, fperf_prec, rankingLoss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(clf, open(os.path.join(data_dir, 'tph-' + dataset_name + '-tp-lr2.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = MLC_hybrid(C=18, C1=2, r=8, weighting=True)\n",
    "clf2.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_results(clf2, X_train, Y_train, X_test, Y_test, fperf_prec, rankingLoss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1_score_nowarn(Y_test, clf.decision_function(X_test) > 0, average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
