{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label classification -- hybrid loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, time\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import check_grad\n",
    "from scipy.special import logsumexp\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer, label_ranking_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('src')\n",
    "from evaluate import avgPrecisionK, evaluatePrecision, evaluateF1, evaluateRankingLoss, f1_score_nowarn\n",
    "from datasets import create_dataset, dataset_names, nLabels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yeast', 'scene', 'bibtex', 'bookmarks', 'delicious', 'mediamill']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ix = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bibtex 159\n"
     ]
    }
   ],
   "source": [
    "dataset_name = dataset_names[data_ix]\n",
    "nLabels = nLabels_dict[dataset_name]\n",
    "print(dataset_name, nLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "SEED = 918273645\n",
    "fmodel_base = os.path.join(data_dir, 'tph-' + dataset_name + '-base.pkl')\n",
    "fmodel_prec = os.path.join(data_dir, 'tph-' + dataset_name + '-prec.pkl')\n",
    "fmodel_noclsw = os.path.join(data_dir, 'tph-' + dataset_name + '-noclsw.pkl')\n",
    "fperf_base = os.path.join(data_dir, 'perf-tph-base.pkl')\n",
    "fperf_prec = os.path.join(data_dir, 'perf-tph-prec.pkl')\n",
    "fperf_noclsw = os.path.join(data_dir, 'perf-tph-noclsw.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = create_dataset(dataset_name, train_data=True, shuffle=True, random_state=SEED)\n",
    "X_test,  Y_test  = create_dataset(dataset_name, train_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mean = np.mean(X_train, axis=0).reshape((1, -1))\n",
    "X_train_std = np.std(X_train, axis=0).reshape((1, -1)) + 10 ** (-6)\n",
    "X_train -= X_train_mean\n",
    "X_train /= X_train_std\n",
    "X_test  -= X_train_mean\n",
    "X_test  /= X_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_info(X_train, Y_train, X_test, Y_test):\n",
    "    N_train, D = X_train.shape\n",
    "    K = Y_train.shape[1]\n",
    "    N_test = X_test.shape[0]\n",
    "    print('%-45s %s' % ('Number of training examples:', '{:,}'.format(N_train)))\n",
    "    print('%-45s %s' % ('Number of test examples:', '{:,}'.format(N_test)))\n",
    "    print('%-45s %s' % ('Number of features:', '{:,}'.format(D)))\n",
    "    print('%-45s %s' % ('Number of labels:', '{:,}'.format(K)))\n",
    "    avgK_train = np.mean(np.sum(Y_train, axis=1))\n",
    "    avgK_test  = np.mean(np.sum(Y_test, axis=1))\n",
    "    print('%-45s %.3f (%.2f%%)' % ('Average number of positive labels (train):', avgK_train, 100*avgK_train / K))\n",
    "    print('%-45s %.3f (%.2f%%)' % ('Average number of positive labels (test):', avgK_test, 100*avgK_test / K))\n",
    "    #print('%-45s %.4f%%' % ('Average label occurrence (train):', np.mean(np.sum(Y_train, axis=0)) / N_train))\n",
    "    #print('%-45s %.4f%%' % ('Average label occurrence (test):', np.mean(np.sum(Y_test, axis=0)) / N_test))\n",
    "    print('%-45s %.3f%%' % ('Sparsity (percent) (train):', 100 * np.sum(Y_train) / np.prod(Y_train.shape)))\n",
    "    print('%-45s %.3f%%' % ('Sparsity (percent) (test):', 100 * np.sum(Y_test) / np.prod(Y_test.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:                                      bibtex\n",
      "Number of training examples:                  4,880\n",
      "Number of test examples:                      2,515\n",
      "Number of features:                           1,836\n",
      "Number of labels:                             159\n",
      "Average number of positive labels (train):    2.380 (1.50%)\n",
      "Average number of positive labels (test):     2.444 (1.54%)\n",
      "Sparsity (percent) (train):                   1.497%\n",
      "Sparsity (percent) (test):                    1.537%\n"
     ]
    }
   ],
   "source": [
    "print('%-45s %s' % ('Dataset:', dataset_name))\n",
    "print_dataset_info(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate `max()` using `log-sum-exp()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.random.rand(500000).reshape(10, 50000) * np.arange(1, 11)[:, None]\n",
    "maxes = np.max(xs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50000)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print(xs.shape)\n",
    "print(maxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f07c35de128>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f07c5853f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#rs = np.array([0.5, 1, 2, 4, 8, 16, 32, 64])\n",
    "rs = np.array([4, 8, 16, 32, 64])\n",
    "mses = []\n",
    "for r in rs:\n",
    "    approx = []\n",
    "    for i in range(xs.shape[0]):\n",
    "        approx.append(np.log(np.sum(np.exp(r * xs[i, :]))) / r)\n",
    "    deltas = np.array(approx) - maxes\n",
    "    mses.append(np.dot(deltas, deltas))\n",
    "\n",
    "#fig = plt.Figure(figsize=[20, 12])\n",
    "plt.plot(rs, mses, ls='--', marker='o', c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top-push loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-label learning with top push loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_toppush_example(w, X, Y, r=1, weighting=True):\n",
    "    \"\"\"\n",
    "        Objective with L2 regularisation and top push loss\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "            - C: regularisation constant, C = 1 / lambda\n",
    "            - r: parameter for log-sum-exp approximation\n",
    "            - weighting: if True, divide K+ in top-push loss\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "    assert(r > 0)\n",
    "    \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    # instead of using diagonal matrix to scale each row of a matrix with a different factor,\n",
    "    # we use Mat * Vec[:, None] which is more memory efficient\n",
    "    \n",
    "    if weighting is True:\n",
    "        KPosAll = np.sum(Y, axis=1)  # number of positive labels for each example, N by 1\n",
    "    else:\n",
    "        KPosAll = np.ones(N)\n",
    "        \n",
    "    A_diag = 1.0 / KPosAll\n",
    "    AY = Y * A_diag[:, None]\n",
    "    \n",
    "    T1 = np.dot(X, W.T)  # N by K\n",
    "    #m0 = np.max(T1)  # underflow in np.exp(r*T1 - m1)\n",
    "    m0 = 0.5 * (np.max(T1) + np.min(T1))\n",
    "    m1 = r * m0\n",
    "    #print('----------------')\n",
    "    #print(np.min(T1), np.max(T1), m0)\n",
    "    #print(np.min(r*T1), np.max(r*T1), m1)\n",
    "    #print(np.min(r * T1 - m1), np.max(r * T1 - m1))\n",
    "    T2 = np.multiply(1 - Y, np.exp(r * T1 - m1))  # N by K\n",
    "    B_tilde_diag = np.dot(T2, np.ones(K))\n",
    "    #print(np.max(B_tilde_diag), np.min(B_tilde_diag))  # big numbers here, can cause overflow in T3\n",
    "    \n",
    "    #T3 = np.exp(-T1 + m0) * np.power(B_tilde_diag, 1.0 / r)[:, None]\n",
    "    #T4 = np.multiply(AY, np.log1p(T3))\n",
    "    T3 = (-T1 + m0) + (1.0 / r) * np.log(B_tilde_diag)[:, None]\n",
    "    #print(np.min(T3), np.max(T3))\n",
    "    m2 = 0.5 * (np.min(T3) + np.max(T3))\n",
    "    #T4 = np.logaddexp(0, T3)\n",
    "    T4 = np.logaddexp(-m2, T3-m2) + m2\n",
    "    T5 = np.multiply(AY, T4)  \n",
    "    \n",
    "    #J = np.dot(w, w) * 0.5 / C + np.dot(np.ones(N), np.dot(T5, np.ones(K))) / N\n",
    "    J = np.dot(np.ones(N), np.dot(T5, np.ones(K))) / N\n",
    "    \n",
    "    #T5 = 1.0 / (1.0 + np.divide(1.0, T3))\n",
    "    #T5 = np.divide(T3, 1 + T3)\n",
    "    T6 = np.exp(T3 - T4)\n",
    "    O_diag = np.dot(np.multiply(Y, T6), np.ones(K))\n",
    "    T7 = A_diag * (1.0 / B_tilde_diag) * O_diag\n",
    "    \n",
    "    G1 = np.dot(np.multiply(AY, T6).T, -X)\n",
    "    G2 = np.dot((T2 * T7[:, None]).T, X)\n",
    "    \n",
    "    #G = W / C + (G1 + G2) / N\n",
    "    G = (G1 + G2) / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_toppush_example_loop(w, X, Y, r=1, weighting=True):\n",
    "    \"\"\"\n",
    "        Objective with L2 regularisation and top push loss\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "            - C: regularisation constant, C = 1 / lambda\n",
    "            - r: parameter for log-sum-exp approximation\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "    assert(r > 0)\n",
    "    \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    if weighting is True:\n",
    "        KPosAll = np.sum(Y, axis=1)  # number of positive labels for each example, N by 1\n",
    "    else:\n",
    "        KPosAll = np.ones(N)\n",
    "    \n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            if Y[n, k] == 1:\n",
    "                s1 = np.sum([np.exp(r * np.dot(W[j, :] - W[k, :], X[n, :])) for j in range(K) if Y[n, j] == 0])\n",
    "                J += np.log1p(np.power(s1, 1.0 / r)) / KPosAll[n]\n",
    "    #J = np.dot(w, w) * 0.5 / C + J / N\n",
    "    J = J / N\n",
    "    \n",
    "    for k in range(K):\n",
    "        for n in range(N):\n",
    "            if Y[n, k] == 1:\n",
    "                t1 = np.sum([np.exp(r * np.dot(W[j, :] - W[k, :], X[n, :])) for j in range(K) if Y[n, j] == 0])\n",
    "                t2 = -1.0 / (1 + np.power(t1, -1.0 / r))\n",
    "                G[k, :] = G[k, :] + X[n, :] * t2 / KPosAll[n]\n",
    "            else:\n",
    "                sk = 0.0\n",
    "                for k1 in range(K):\n",
    "                    if Y[n, k1] == 1:\n",
    "                        t3 = np.sum([np.exp(r * np.dot(W[j,:] - W[k1, :], X[n, :])) \\\n",
    "                                     for j in range(K) if Y[n, j] == 0])\n",
    "                        t4 = np.exp(r * np.dot(W[k, :] - W[k1, :], X[n, :]))\n",
    "                        sk += t4 / (np.power(t3, 1.0 - 1.0 / r) + t3)\n",
    "                G[k, :] = G[k, :] + X[n, :] * sk / KPosAll[n]\n",
    "                        \n",
    "    #G = W / C + G / N\n",
    "    G = G / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_xentropy(w, X, Y):\n",
    "    \"\"\"\n",
    "    Objective with logistic loss\n",
    "    \n",
    "    Input:\n",
    "            - w: current weight vector, flattened K x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "        \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    NK = N * K\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    T1 = np.dot(W, X.T)  # K by N\n",
    "    T2 = np.exp(T1)\n",
    "    T3 = np.divide(T2, 1+T2)\n",
    "    T4 = np.log1p(T2)\n",
    "    T5 = np.log1p(np.divide(1.0, T2))\n",
    "    T6 = T4 + np.multiply(Y.T, T5-T4)  # K by N\n",
    "    \n",
    "    J = np.dot(np.ones(K), np.dot(T6, np.ones(N))) / NK\n",
    "    G = np.dot(T3-Y.T, X) / NK\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_xentropy_loop(w, X, Y):\n",
    "    \"\"\"\n",
    "    Objective with logistic loss\n",
    "    \n",
    "    Input:\n",
    "            - w: current weight vector, flattened K x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "    \"\"\"    \n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "        \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    NK = N * K\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    for k in range(K):\n",
    "        for n in range(N):\n",
    "            t1 = np.exp(np.dot(W[k, :], X[n, :]))\n",
    "            t2 = np.log1p(t1)\n",
    "            J += t2\n",
    "            if Y[n, k] == 1:\n",
    "                J += (np.log1p(1.0 / t1) - t2)\n",
    "            G[k, :] = G[k, :] + X[n, :] * (t1 / (1 + t1) - Y[n, k])\n",
    "                \n",
    "    J = J / NK\n",
    "    G = G / NK\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_hybrid(w, X, Y, C, r=1, weighting=True):\n",
    "    assert C > 0\n",
    "    \n",
    "    J1, G1 = obj_toppush_example(w, X, Y, r, weighting)\n",
    "    J2, G2 = obj_xentropy(w, X, Y)\n",
    "    \n",
    "    J = np.dot(w, w) * 0.5 / C + J1 + J2\n",
    "    G = w / C + G1 + G2\n",
    "    \n",
    "    return (J, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_hybrid_loop(w, X, Y, C, r=1, weighting=True):\n",
    "    assert C > 0\n",
    "    \n",
    "    J1, G1 = obj_toppush_example_loop(w, X, Y, r, weighting)\n",
    "    J2, G2 = obj_xentropy_loop(w, X, Y)\n",
    "    \n",
    "    J = np.dot(w, w) * 0.5 / C + J1 + J2\n",
    "    G = w / C + G1 + G2\n",
    "    \n",
    "    return (J, G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train[:50, :]\n",
    "#Y_train = Y_train[:50, :]\n",
    "#C = 1\n",
    "##w0 = np.random.rand(Y_train.shape[1] * X_train.shape[1]) - 0.5\n",
    "#w0 = 0.001 * np.random.randn(Y_train.shape[1] * X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_grad(lambda w: obj_hybrid(w, X_train, Y_train, C, r=4)[0], \n",
    "#           lambda w: obj_hybrid(w, X_train, Y_train, C, r=4)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_grad(lambda w: obj_hybrid_loop(w, X_train, Y_train, C, r=2)[0], \n",
    "#           lambda w: obj_hybrid_loop(w, X_train, Y_train, C, r=2)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "print('%15s %15s %15s %15s' % ('J_Diff', 'J_loop', 'J_vec', 'G_Diff'))\n",
    "for e in range(-6, 10):\n",
    "    C = 10**(e)\n",
    "    #w0 = init_var(X_train, Y_train)\n",
    "    J,  G  = obj_hybrid_loop(w0, X_train, Y_train, C, r=4)\n",
    "    J1, G1 = obj_hybrid(w0, X_train, Y_train, C, r=4)\n",
    "    Gdiff = G1 - G\n",
    "    #print('%-15g %-15g %-15g' % (J1 - J, J, J1))\n",
    "    print('%15g %15g %15g %15g' % (J1 - J, J, J1, np.dot(Gdiff, Gdiff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLC_hybrid(BaseEstimator):\n",
    "    \"\"\"All methods are necessary for a scikit-learn estimator\"\"\"\n",
    "    \n",
    "    def __init__(self, C=1, r=1, weighting=True):\n",
    "        \"\"\"Initialisation\"\"\"\n",
    "        \n",
    "        assert C > 0\n",
    "        assert r > 0\n",
    "        assert type(weighting) == bool\n",
    "        self.C = C\n",
    "        self.r = r\n",
    "        self.weighting = weighting\n",
    "        self.trained = False\n",
    "        \n",
    "    def fit(self, X_train, Y_train):\n",
    "        \"\"\"Model fitting by optimising the objective\"\"\"\n",
    "        opt_method = 'L-BFGS-B' #'BFGS' #'Newton-CG'\n",
    "        options = {'disp': 1, 'maxiter': 10**5, 'maxfun': 10**5} # , 'iprint': 99}\n",
    "        print('\\nC: %g, r: %g' % (self.C, self.r))\n",
    "            \n",
    "        N, D = X_train.shape\n",
    "        K = Y_train.shape[1]\n",
    "        #w0 = np.random.rand(K * D) - 0.5  # initial guess in range [-1, 1]\n",
    "        w0 = 0.001 * np.random.randn(K * D)\n",
    "        opt = minimize(obj_hybrid, w0, args=(X_train, Y_train, self.C, self.r, self.weighting), \\\n",
    "                       method=opt_method, jac=True, options=options)\n",
    "        if opt.success is True:\n",
    "            self.W = np.reshape(opt.x, (K, D))\n",
    "            self.trained = True\n",
    "        else:\n",
    "            sys.stderr.write('Optimisation failed')\n",
    "            print(opt.items())\n",
    "            self.trained = False\n",
    "            \n",
    "            \n",
    "    def decision_function(self, X_test):\n",
    "        \"\"\"Make predictions (score is real number)\"\"\"\n",
    "        \n",
    "        assert self.trained is True, \"Can't make prediction before training\"\n",
    "        D = X_test.shape[1]\n",
    "        return np.dot(X_test, self.W.T)\n",
    "        \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return self.decision_function(X_test)\n",
    "    #    \"\"\"Make predictions (score is boolean)\"\"\"   \n",
    "    #    preds = sigmoid(self.decision_function(X_test))\n",
    "    #    #return (preds >= 0)\n",
    "    #    assert self.TH is not None\n",
    "    #    return preds >= self.TH        \n",
    "        \n",
    "    # inherit from BaseEstimator instead of re-implement\n",
    "    #\n",
    "    #def get_params(self, deep = True):\n",
    "    #def set_params(self, **params):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_results(predictor, X_train, Y_train, X_test, Y_test, fname):\n",
    "    \"\"\"\n",
    "        Compute and save performance results\n",
    "    \"\"\"\n",
    "    preds_train = predictor.decision_function(X_train)\n",
    "    preds_test  = predictor.decision_function(X_test)\n",
    "    \n",
    "    print('Training set:')\n",
    "    perf_dict_train = evaluatePrecision(Y_train, preds_train, verbose=1)\n",
    "    print()\n",
    "    print('Test set:')\n",
    "    perf_dict_test = evaluatePrecision(Y_test, preds_test, verbose=1)\n",
    "    \n",
    "    print()\n",
    "    print('Training set:')\n",
    "    perf_dict_train.update(evaluateRankingLoss(Y_train, preds_train))\n",
    "    print(label_ranking_loss(Y_train, preds_train))\n",
    "    print()\n",
    "    print('Test set:')\n",
    "    perf_dict_test.update(evaluateRankingLoss(Y_test, preds_test))\n",
    "    print(label_ranking_loss(Y_test, preds_test))\n",
    "    \n",
    "    # compute F1 score w.r.t. different thresholds\n",
    "    #TH1 = predictor.cv_results_['mean_test_TH'][clf.best_index_]\n",
    "    #TH2 = np.mean(Y_train, axis=0)\n",
    "    #TH3 = np.mean(TH2)\n",
    "    \n",
    "    preds_train_bin = sigmoid(preds_train)\n",
    "    preds_test_bin  = sigmoid(preds_test)\n",
    "    \n",
    "    #F1_train1 = f1_score_nowarn(Y_train, sigmoid(preds_train) >= TH1, average='samples')\n",
    "    #F1_test1  = f1_score_nowarn(Y_test, sigmoid(preds_test) >= TH1, average='samples')\n",
    "    #print('\\nTrain: %.4f, %f' % (F1_train1, f1_score(Y_train, sigmoid(preds_train) >= TH1, average='samples')))\n",
    "    #print('\\nTest : %.4f, %f' % (F1_test1, f1_score(Y_test, sigmoid(preds_test) >= TH1, average='samples')))\n",
    "    \n",
    "    #F1_train2 = f1_score_nowarn(Y_train, (preds_train_bin - TH2) >= 0, average='samples')\n",
    "    #F1_test2  = f1_score_nowarn(Y_test, (preds_test_bin - TH2) >= 0, average='samples')\n",
    "    #print('\\nTrain: %.4f, %f' % (F1_train2, f1_score(Y_train, (preds_train_bin - TH2) >= 0, average='samples')))\n",
    "    #print('\\nTest : %.4f, %f' % (F1_test2, f1_score(Y_test, (preds_test_bin - TH2) >= 0, average='samples')))\n",
    "    \n",
    "    #F1_train3 = f1_score_nowarn(Y_train, preds_train_bin >= TH3, average='samples')\n",
    "    #F1_test3  = f1_score_nowarn(Y_test, preds_test_bin >= TH3, average='samples')\n",
    "    #print('\\nTrain: %.4f, %f' % (F1_train3, f1_score(Y_train, preds_train_bin >= TH3, average='samples')))\n",
    "    #print('\\nTest : %.4f, %f' % (F1_test3, f1_score(Y_test, preds_test_bin >= TH3, average='samples')))\n",
    "    \n",
    "    #perf_dict_train.update({'F1': [(F1_train1,), (F1_train2,), (F1_train3,)]})\n",
    "    #perf_dict_test.update( {'F1': [(F1_test1,),  (F1_test2,),  (F1_test3,)]})\n",
    "    #perf_dict_train.update({'F1': [(F1_train2,), (F1_train3,)]})\n",
    "    #perf_dict_test.update( {'F1': [(F1_test2,),  (F1_test3,)]})\n",
    "    \n",
    "    perf_dict = {'Train': perf_dict_train, 'Test': perf_dict_test}\n",
    "    if os.path.exists(fname):\n",
    "        _dict = pkl.load(open(fname, 'rb'))\n",
    "        if dataset_name not in _dict:\n",
    "            _dict[dataset_name] = perf_dict\n",
    "    else:\n",
    "        _dict = {dataset_name: perf_dict}\n",
    "    pkl.dump(_dict, open(fname, 'wb'))\n",
    "    \n",
    "    print()\n",
    "    print(pkl.load(open(fname, 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'ignore', 'invalid': 'ignore', 'over': 'ignore', 'under': 'ignore'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_settings = np.seterr(all='ignore')  # seterr to known value\n",
    "np.seterr(all='raise')\n",
    "#np.seterr(all='ignore')\n",
    "#np.seterr(**old_settings)  # restore settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%memit model.fit(X_train[:30], Y_train[:30])\n",
    "#%mprun -f minimize model.fit(X_train[:100], Y_train[:100])\n",
    "#%mprun -f _minimize_slsqp model.fit(X_train[:10], Y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C: 1, r: 1\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(fmodel_base):\n",
    "    clf = pkl.load(open(fmodel_base, 'rb'))\n",
    "else:\n",
    "    clf = clf = MLC_hybrid()\n",
    "    clf.fit(X_train, Y_train)\n",
    "    pkl.dump(clf, open(fmodel_base, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Average Precision@3: 0.4112, 0.003\n",
      "Average Precision@5: 0.3082, 0.003\n",
      "Average Precision@10: 0.1918, 0.002\n",
      "Average Precision@K: 0.5746, 0.005\n",
      "\n",
      "Test set:\n",
      "Average Precision@3: 0.3055, 0.005\n",
      "Average Precision@5: 0.2305, 0.004\n",
      "Average Precision@10: 0.1539, 0.002\n",
      "Average Precision@K: 0.4024, 0.008\n",
      "\n",
      "Training set:\n",
      "Average RankingLoss: 13.3486, 0.459\n",
      "0.0255071198235\n",
      "\n",
      "Test set:\n",
      "Average RankingLoss: 31.6911, 1.033\n",
      "0.0757615760342\n",
      "\n",
      "{'bibtex': {'Train': {'Precision@3': (0.41120218579234968, 0.0034960992411208537), 'Precision@5': (0.30823770491803282, 0.00251364226794872), 'Precision@10': (0.1918237704918033, 0.0016122735227508244), 'Precision@K': (0.57464069628004044, 0.0054741116297042447), 'RankingLoss': (13.348565573770491, 0.45856966011523675)}, 'Test': {'Precision@3': (0.30550033134526172, 0.0050511890734753675), 'Precision@5': (0.23045725646123258, 0.0035622997170740795), 'Precision@10': (0.15391650099403578, 0.0022333370770739923), 'Precision@K': (0.40237299502560875, 0.0076150972250788856), 'RankingLoss': (31.691053677932405, 1.0326403489361529)}}}\n"
     ]
    }
   ],
   "source": [
    "dump_results(clf, X_train, Y_train, X_test, Y_test, fperf_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation w.r.t. average precision@K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#mm = MLC_toppush(C=300000, r=4)\n",
    "#mm.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ranges = range(-6, 7)\n",
    "ranges = range(-6, 5)\n",
    "parameters = [{'C': sorted([10**(e) for e in ranges] + [3 * 10**(e) for e in ranges]),\n",
    "               'r': [4]}]\n",
    "scorer = {'Prec': make_scorer(avgPrecisionK)}#, 'TH': make_scorer(get_TH)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C: 1e-06, r: 4\n",
      "\n",
      "C: 1e-06, r: 4\n",
      "\n",
      "C: 1e-06, r: 4\n",
      "\n",
      "C: 1e-06, r: 4\n",
      "\n",
      "C: 1e-06, r: 4\n",
      "\n",
      "C: 3e-06, r: 4\n",
      "\n",
      "C: 3e-06, r: 4\n",
      "\n",
      "C: 3e-06, r: 4\n",
      "\n",
      "C: 3e-06, r: 4\n",
      "\n",
      "C: 3e-06, r: 4\n",
      "\n",
      "C: 1e-05, r: 4\n",
      "\n",
      "C: 1e-05, r: 4\n",
      "\n",
      "C: 1e-05, r: 4\n",
      "\n",
      "C: 1e-05, r: 4\n",
      "\n",
      "C: 1e-05, r: 4\n",
      "\n",
      "C: 3e-05, r: 4\n",
      "\n",
      "C: 3e-05, r: 4\n",
      "\n",
      "C: 3e-05, r: 4\n",
      "\n",
      "C: 3e-05, r: 4\n",
      "\n",
      "C: 3e-05, r: 4\n",
      "\n",
      "C: 0.0001, r: 4\n",
      "\n",
      "C: 0.0001, r: 4\n",
      "\n",
      "C: 0.0001, r: 4\n",
      "\n",
      "C: 0.0001, r: 4\n",
      "\n",
      "C: 0.0001, r: 4\n",
      "\n",
      "C: 0.0003, r: 4\n",
      "\n",
      "C: 0.0003, r: 4\n",
      "\n",
      "C: 0.0003, r: 4\n",
      "\n",
      "C: 0.0003, r: 4\n",
      "\n",
      "C: 0.0003, r: 4\n",
      "\n",
      "C: 0.001, r: 4\n",
      "\n",
      "C: 0.001, r: 4\n",
      "\n",
      "C: 0.001, r: 4\n",
      "\n",
      "C: 0.001, r: 4\n",
      "\n",
      "C: 0.001, r: 4\n",
      "\n",
      "C: 0.003, r: 4\n",
      "\n",
      "C: 0.003, r: 4\n",
      "\n",
      "C: 0.003, r: 4\n",
      "\n",
      "C: 0.003, r: 4\n",
      "\n",
      "C: 0.003, r: 4\n",
      "\n",
      "C: 0.01, r: 4\n",
      "\n",
      "C: 0.01, r: 4\n",
      "\n",
      "C: 0.01, r: 4\n",
      "\n",
      "C: 0.01, r: 4\n",
      "\n",
      "C: 0.01, r: 4\n",
      "\n",
      "C: 0.03, r: 4\n",
      "\n",
      "C: 0.03, r: 4\n",
      "\n",
      "C: 0.03, r: 4\n",
      "\n",
      "C: 0.03, r: 4\n",
      "\n",
      "C: 0.03, r: 4\n",
      "\n",
      "C: 0.1, r: 4\n",
      "\n",
      "C: 0.1, r: 4\n",
      "\n",
      "C: 0.1, r: 4\n",
      "\n",
      "C: 0.1, r: 4\n",
      "\n",
      "C: 0.1, r: 4\n",
      "\n",
      "C: 0.3, r: 4\n",
      "\n",
      "C: 0.3, r: 4\n",
      "\n",
      "C: 0.3, r: 4\n",
      "\n",
      "C: 0.3, r: 4\n",
      "\n",
      "C: 0.3, r: 4\n",
      "\n",
      "C: 1, r: 4\n",
      "\n",
      "C: 1, r: 4\n",
      "\n",
      "C: 1, r: 4\n",
      "\n",
      "C: 1, r: 4\n",
      "\n",
      "C: 1, r: 4\n",
      "\n",
      "C: 3, r: 4\n",
      "\n",
      "C: 3, r: 4\n",
      "\n",
      "C: 3, r: 4\n",
      "\n",
      "C: 3, r: 4\n",
      "\n",
      "C: 3, r: 4\n",
      "\n",
      "C: 10, r: 4\n",
      "\n",
      "C: 10, r: 4\n",
      "\n",
      "C: 10, r: 4\n",
      "\n",
      "C: 10, r: 4\n",
      "\n",
      "C: 10, r: 4\n",
      "\n",
      "C: 30, r: 4\n",
      "\n",
      "C: 30, r: 4\n",
      "\n",
      "C: 30, r: 4\n",
      "\n",
      "C: 30, r: 4\n",
      "\n",
      "C: 30, r: 4\n",
      "\n",
      "C: 100, r: 4\n",
      "\n",
      "C: 100, r: 4\n",
      "\n",
      "C: 100, r: 4\n",
      "\n",
      "C: 100, r: 4\n",
      "\n",
      "C: 100, r: 4\n",
      "\n",
      "C: 300, r: 4\n",
      "\n",
      "C: 300, r: 4\n",
      "\n",
      "C: 300, r: 4\n",
      "\n",
      "C: 300, r: 4\n",
      "\n",
      "C: 300, r: 4\n",
      "\n",
      "C: 1000, r: 4\n",
      "\n",
      "C: 1000, r: 4\n",
      "\n",
      "C: 1000, r: 4\n",
      "\n",
      "C: 1000, r: 4\n",
      "\n",
      "C: 1000, r: 4\n",
      "\n",
      "C: 3000, r: 4\n",
      "\n",
      "C: 3000, r: 4\n",
      "\n",
      "C: 3000, r: 4\n",
      "\n",
      "C: 3000, r: 4\n",
      "\n",
      "C: 3000, r: 4\n",
      "\n",
      "C: 10000, r: 4\n",
      "\n",
      "C: 10000, r: 4\n",
      "\n",
      "C: 10000, r: 4\n",
      "\n",
      "C: 10000, r: 4\n",
      "\n",
      "C: 10000, r: 4\n",
      "\n",
      "C: 30000, r: 4\n",
      "\n",
      "C: 30000, r: 4\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(fmodel_prec):\n",
    "    clf = GridSearchCV(MLC_hybrid(), parameters, scoring=scorer, cv=5, n_jobs=1, refit='Prec')\n",
    "    clf.fit(X_train, Y_train)\n",
    "    pkl.dump(clf, open(fmodel_prec, 'wb'))\n",
    "else:\n",
    "    clf = pkl.load(open(fmodel_prec, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_results(clf, X_train, Y_train, X_test, Y_test, fperf_prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation w.r.t. average precision@K, without weighting positive labels in objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ranges = range(-6, 5)\n",
    "parameters = [{'C': sorted([10**(e) for e in ranges] + [3 * 10**(e) for e in ranges]),\n",
    "               'r': [8]}]\n",
    "if not os.path.exists(fmodel_noclsw):\n",
    "    clf = GridSearchCV(MLC_hybrid(weighting=False), parameters, scoring=scorer, cv=5, n_jobs=1, refit='Prec')\n",
    "    clf.fit(X_train, Y_train)\n",
    "    pkl.dump(clf, open(fmodel_noclsw, 'wb'))\n",
    "else:\n",
    "    clf = pkl.load(open(fmodel_noclsw, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_results(clf, X_train, Y_train, X_test, Y_test, fperf_noclsw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute threshold for F1:\n",
    "- approach I: grid search in range(0.01, 1) with step 0.01 during cross-validation.\n",
    "- approach II: for each label, let the threshold be the average probability (to be +1) in training set.\n",
    "- approach III: let the threshold be the average of all thresholds computed in approach II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "\n",
    "#TH1 = clf.cv_results_['mean_test_TH'][clf.best_index_]\n",
    "#print(TH1)\n",
    "\n",
    "TH2 = np.mean(Y_train, axis=0)\n",
    "TH3 = np.mean(TH2)\n",
    "\n",
    "#F1_train1 = f1_score_nowarn(Y_train, sigmoid(preds_train) >= TH1, average='samples')\n",
    "#F1_test1  = f1_score_nowarn(Y_test, sigmoid(preds_test) >= TH1, average='samples')\n",
    "#print('Train: %.4f \\n' % F1_train1, f1_score(Y_train, sigmoid(preds_train) >= TH1, average='samples'))\n",
    "#print('Test : %.4f \\n' % F1_test1, f1_score(Y_test, sigmoid(preds_test) >= TH1, average='samples'))\n",
    "\n",
    "F1_train2 = f1_score_nowarn(Y_train, (preds_train - TH2) >= 0, average='samples')\n",
    "F1_test2  = f1_score_nowarn(Y_test, (preds_test - TH2) >= 0, average='samples')\n",
    "print('Train: %.4f \\n' % F1_train2, f1_score(Y_train, (preds_train - TH2) >= 0, average='samples'))\n",
    "print('Test : %.4f \\n' % F1_test2, f1_score(Y_test, (preds_test - TH2) >= 0, average='samples'))\n",
    "\n",
    "F1_train3 = f1_score_nowarn(Y_train, preds_train >= TH3, average='samples')\n",
    "F1_test3  = f1_score_nowarn(Y_test, preds_test >= TH3, average='samples')\n",
    "print('Train: %.4f \\n' % F1_train3, f1_score(Y_train, preds_train >= TH3, average='samples'))\n",
    "print('Test : %.4f \\n' % F1_test3, f1_score(Y_test, preds_test >= TH3, average='samples'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histogram of the $K$-th highest prediction scores, where $K$ is the number of positive labels for a given example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "\n",
    "K_train = Y_train.sum(axis=1).astype(np.int)\n",
    "K_test = Y_test.sum(axis=1).astype(np.int)\n",
    "\n",
    "K_pred_train = [sorted(preds_train[n, :])[::-1][K_train[n]-1] for n in range(X_train.shape[0])]\n",
    "K_pred_test = [sorted(preds_test[n, :])[::-1][K_test[n]-1] for n in range(X_test.shape[0])]\n",
    "\n",
    "fig = plt.figure(figsize=[10, 3])\n",
    "plot_data = [K_pred_train, K_pred_test]\n",
    "plt.suptitle('Histogram on %s dataset' % dataset_name)\n",
    "for i in range(2):\n",
    "    ax = plt.subplot(1,2,i+1)\n",
    "    ax.hist(plot_data[i], bins=20)\n",
    "    ax.set_title('%s set' % ('Training' if i == 0 else 'Test'))\n",
    "    ax.set_xlabel('K-th highest prediction score')\n",
    "    ax.set_ylabel('Frequency')\n",
    "plt.savefig(dataset_name + '_kscore_hist.svg')\n",
    "\n",
    "#print(K_pred_train[0], K_train[0])\n",
    "#print(K_pred_test[0], K_test[0])\n",
    "\n",
    "#preds_train[0, :]\n",
    "#preds_test[0, :]\n",
    "\n",
    "precisions_train = [avgPrecision(Y_train, preds_train, k) for k in range(1, nLabels+1)]\n",
    "precisions_test  = [avgPrecision(Y_test,  preds_test,  k) for k in range(1, nLabels+1)]\n",
    "\n",
    "precisionK_train = avgPrecisionK(Y_train, preds_train)\n",
    "precisionK_test  = avgPrecisionK(Y_test,  preds_test)\n",
    "\n",
    "plt.figure(figsize=[10,5])\n",
    "plt.plot(precisions_train, ls='--', c='r', label='Train')\n",
    "plt.plot(precisions_test,  ls='-',  c='g', label='Test')\n",
    "plt.plot([precisionK_train for k in range(nLabels)], ls='-', c='r', label='Train, Precision@K')\n",
    "plt.plot([precisionK_test  for k in range(nLabels)], ls='-', c='g', label='Test, Precision@K')\n",
    "plt.xticks(np.arange(nLabels), np.arange(1,nLabels+1))\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Precision@k')\n",
    "plt.legend(loc='best')\n",
    "plt.title('MLC w. Top-push Loss on ' + dataset_name + ' dataset')\n",
    "#plt.savefig(dataset_name + '_tp.svg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
