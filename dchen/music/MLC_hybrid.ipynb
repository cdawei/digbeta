{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label classification -- hybrid loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, time\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import check_grad\n",
    "from scipy.special import logsumexp\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer, label_ranking_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('src')\n",
    "from evaluate import avgPrecisionK, evaluatePrecision, evaluateF1, evaluateRankingLoss, f1_score_nowarn, calcLoss\n",
    "from datasets import create_dataset, dataset_names, nLabels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yeast', 'scene', 'bibtex', 'bookmarks', 'delicious', 'mediamill']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ix = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bibtex 159\n"
     ]
    }
   ],
   "source": [
    "dataset_name = dataset_names[data_ix]\n",
    "nLabels = nLabels_dict[dataset_name]\n",
    "print(dataset_name, nLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "SEED = 918273645\n",
    "fmodel_base = os.path.join(data_dir, 'tph-' + dataset_name + '-base.pkl')\n",
    "fmodel_prec = os.path.join(data_dir, 'tph-' + dataset_name + '-prec.pkl')\n",
    "fperf_base = os.path.join(data_dir, 'perf-tph-base.pkl')\n",
    "fperf_prec = os.path.join(data_dir, 'perf-tph-prec.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = create_dataset(dataset_name, train_data=True, shuffle=True, random_state=SEED)\n",
    "X_test,  Y_test  = create_dataset(dataset_name, train_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, Y = create_dataset(dataset_name, train_data=True, shuffle=True, random_state=SEED)\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mean = np.mean(X_train, axis=0).reshape((1, -1))\n",
    "X_train_std = np.std(X_train, axis=0).reshape((1, -1)) + 10 ** (-6)\n",
    "X_train -= X_train_mean\n",
    "X_train /= X_train_std\n",
    "X_test  -= X_train_mean\n",
    "X_test  /= X_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_info(X_train, Y_train, X_test, Y_test):\n",
    "    N_train, D = X_train.shape\n",
    "    K = Y_train.shape[1]\n",
    "    N_test = X_test.shape[0]\n",
    "    print('%-45s %s' % ('Number of training examples:', '{:,}'.format(N_train)))\n",
    "    print('%-45s %s' % ('Number of test examples:', '{:,}'.format(N_test)))\n",
    "    print('%-45s %s' % ('Number of features:', '{:,}'.format(D)))\n",
    "    print('%-45s %s' % ('Number of labels:', '{:,}'.format(K)))\n",
    "    avgK_train = np.mean(np.sum(Y_train, axis=1))\n",
    "    avgK_test  = np.mean(np.sum(Y_test, axis=1))\n",
    "    print('%-45s %.3f (%.2f%%)' % ('Average number of positive labels (train):', avgK_train, 100*avgK_train / K))\n",
    "    print('%-45s %.3f (%.2f%%)' % ('Average number of positive labels (test):', avgK_test, 100*avgK_test / K))\n",
    "    #print('%-45s %.4f%%' % ('Average label occurrence (train):', np.mean(np.sum(Y_train, axis=0)) / N_train))\n",
    "    #print('%-45s %.4f%%' % ('Average label occurrence (test):', np.mean(np.sum(Y_test, axis=0)) / N_test))\n",
    "    print('%-45s %.3f%%' % ('Sparsity (percent) (train):', 100 * np.sum(Y_train) / np.prod(Y_train.shape)))\n",
    "    print('%-45s %.3f%%' % ('Sparsity (percent) (test):', 100 * np.sum(Y_test) / np.prod(Y_test.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:                                      bibtex\n",
      "Number of training examples:                  4,880\n",
      "Number of test examples:                      2,515\n",
      "Number of features:                           1,836\n",
      "Number of labels:                             159\n",
      "Average number of positive labels (train):    2.380 (1.50%)\n",
      "Average number of positive labels (test):     2.444 (1.54%)\n",
      "Sparsity (percent) (train):                   1.497%\n",
      "Sparsity (percent) (test):                    1.537%\n"
     ]
    }
   ],
   "source": [
    "print('%-45s %s' % ('Dataset:', dataset_name))\n",
    "print_dataset_info(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate `max()` using `log-sum-exp()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "xs = np.random.rand(500000).reshape(10, 50000) * np.arange(1, 11)[:, None]\n",
    "maxes = np.max(xs, axis=1)\n",
    "print(xs.shape)\n",
    "print(maxes.shape)\n",
    "\n",
    "#rs = np.array([0.5, 1, 2, 4, 8, 16, 32, 64])\n",
    "rs = np.array([4, 8, 16, 32, 64])\n",
    "mses = []\n",
    "for r in rs:\n",
    "    approx = []\n",
    "    for i in range(xs.shape[0]):\n",
    "        approx.append(np.log(np.sum(np.exp(r * xs[i, :]))) / r)\n",
    "    deltas = np.array(approx) - maxes\n",
    "    mses.append(np.dot(deltas, deltas))\n",
    "\n",
    "#fig = plt.Figure(figsize=[20, 12])\n",
    "plt.plot(rs, mses, ls='--', marker='o', c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top-push loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-label learning with top push loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_toppush_example(w, X, Y, r=1, weighting=True):\n",
    "    \"\"\"\n",
    "        Objective of top push loss for examples\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "            - r: parameter for log-sum-exp approximation\n",
    "            - weighting: if True, divide K+ in top-push loss\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "    assert(r > 0)\n",
    "    \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    # instead of using diagonal matrix to scale each row of a matrix with a different factor,\n",
    "    # we use Mat * Vec[:, None] which is more memory efficient\n",
    "    \n",
    "    if weighting is True:\n",
    "        KPosAll = np.sum(Y, axis=1)  # number of positive labels for each example, N by 1\n",
    "    else:\n",
    "        KPosAll = np.ones(N)\n",
    "        \n",
    "    A_diag = 1.0 / KPosAll\n",
    "    AY = Y * A_diag[:, None]\n",
    "    \n",
    "    T1 = np.dot(X, W.T)  # N by K\n",
    "    #m0 = np.max(T1)  # underflow in np.exp(r*T1 - m1)\n",
    "    m0 = 0.5 * (np.max(T1) + np.min(T1))\n",
    "    m1 = r * m0\n",
    "    #print('----------------')\n",
    "    #print(np.min(T1), np.max(T1), m0)\n",
    "    #print(np.min(r*T1), np.max(r*T1), m1)\n",
    "    #print(np.min(r * T1 - m1), np.max(r * T1 - m1))\n",
    "    T2 = np.multiply(1 - Y, np.exp(r * T1 - m1))  # N by K\n",
    "    B_tilde_diag = np.dot(T2, np.ones(K))\n",
    "    #print(np.max(B_tilde_diag), np.min(B_tilde_diag))  # big numbers here, can cause overflow in T3\n",
    "    \n",
    "    #T3 = np.exp(-T1 + m0) * np.power(B_tilde_diag, 1.0 / r)[:, None]\n",
    "    #T4 = np.multiply(AY, np.log1p(T3))\n",
    "    T3 = (-T1 + m0) + (1.0 / r) * np.log(B_tilde_diag)[:, None]\n",
    "    #print(np.min(T3), np.max(T3))\n",
    "    m2 = 0.5 * (np.min(T3) + np.max(T3))\n",
    "    #T4 = np.logaddexp(0, T3)\n",
    "    T4 = np.logaddexp(-m2, T3-m2) + m2\n",
    "    T5 = np.multiply(AY, T4)  \n",
    "    \n",
    "    #J = np.dot(w, w) * 0.5 / C + np.dot(np.ones(N), np.dot(T5, np.ones(K))) / N\n",
    "    J = np.dot(np.ones(N), np.dot(T5, np.ones(K))) / N\n",
    "    \n",
    "    #T5 = 1.0 / (1.0 + np.divide(1.0, T3))\n",
    "    #T5 = np.divide(T3, 1 + T3)\n",
    "    T6 = np.exp(T3 - T4)\n",
    "    O_diag = np.dot(np.multiply(Y, T6), np.ones(K))\n",
    "    T7 = A_diag * (1.0 / B_tilde_diag) * O_diag\n",
    "    \n",
    "    G1 = np.dot(np.multiply(AY, T6).T, -X)\n",
    "    \n",
    "    #print(np.max(T2), np.min(T2), np.max(T7), np.min(T7))\n",
    "    T8 = T2 * T7[:, None]\n",
    "    G2 = np.dot(T8.T, X)\n",
    "    \n",
    "    #G = W / C + (G1 + G2) / N\n",
    "    G = (G1 + G2) / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_toppush_example_loop(w, X, Y, r=1, weighting=True):\n",
    "    \"\"\"\n",
    "        Objective of top push loss for examples\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "            - C: regularisation constant, C = 1 / lambda\n",
    "            - r: parameter for log-sum-exp approximation\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "    assert(r > 0)\n",
    "    \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    if weighting is True:\n",
    "        KPosAll = np.sum(Y, axis=1)  # number of positive labels for each example, N by 1\n",
    "    else:\n",
    "        KPosAll = np.ones(N)\n",
    "    \n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            if Y[n, k] == 1:\n",
    "                s1 = np.sum([np.exp(r * np.dot(W[j, :] - W[k, :], X[n, :])) for j in range(K) if Y[n, j] == 0])\n",
    "                J += np.log1p(np.power(s1, 1.0 / r)) / KPosAll[n]\n",
    "    #J = np.dot(w, w) * 0.5 / C + J / N\n",
    "    J = J / N\n",
    "    \n",
    "    for k in range(K):\n",
    "        for n in range(N):\n",
    "            if Y[n, k] == 1:\n",
    "                t1 = np.sum([np.exp(r * np.dot(W[j, :] - W[k, :], X[n, :])) for j in range(K) if Y[n, j] == 0])\n",
    "                t2 = -1.0 / (1 + np.power(t1, -1.0 / r))\n",
    "                G[k, :] = G[k, :] + X[n, :] * t2 / KPosAll[n]\n",
    "            else:\n",
    "                sk = 0.0\n",
    "                for k1 in range(K):\n",
    "                    if Y[n, k1] == 1:\n",
    "                        t3 = np.sum([np.exp(r * np.dot(W[j,:] - W[k1, :], X[n, :])) \\\n",
    "                                     for j in range(K) if Y[n, j] == 0])\n",
    "                        t4 = np.exp(r * np.dot(W[k, :] - W[k1, :], X[n, :]))\n",
    "                        sk += t4 / (np.power(t3, 1.0 - 1.0 / r) + t3)\n",
    "                G[k, :] = G[k, :] + X[n, :] * sk / KPosAll[n]\n",
    "                        \n",
    "    #G = W / C + G / N\n",
    "    G = G / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_toppush_label_loop(w, X, Y, r=1, weighting=True):\n",
    "    \"\"\"\n",
    "        Objective of top push loss for each label\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "            - r1: parameter for log-sum-exp approximation\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "    assert(r > 0)\n",
    "    \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    if weighting is True:\n",
    "        NPosAll = np.sum(Y, axis=0)  # number of positive examples for each label, K by 1\n",
    "    else:\n",
    "        NPosAll = np.ones(K)\n",
    "    \n",
    "    for k in range(K):\n",
    "        Jk = 0.0\n",
    "        posInd = np.nonzero(Y[:, k])[0].tolist()\n",
    "        negInd = sorted(set(np.arange(N).tolist()) - set(posInd))\n",
    "        for p in posInd:\n",
    "            t1 = np.sum([np.exp(r * np.dot(W[k, :], X[q, :] - X[p, :])) for q in negInd])\n",
    "            Jk += np.log1p(np.power(t1, 1.0/r))\n",
    "            #t1 = -np.dot(W[k, :], X[p, :]) + logsumexp([r * np.dot(W[k, :], X[q, :]) for q in negInd]) / r\n",
    "            #Jk += np.logaddexp(0, t1)\n",
    "            t2 = np.power(t1, 1.0-1.0/r) + t1\n",
    "            vk = np.zeros(D)\n",
    "            for q in negInd:\n",
    "                vk = vk + np.exp(r * np.dot(W[k, :], X[q, :] - X[p, :])) * (X[q, :] - X[p, :])\n",
    "            G[k, :] = G[k, :] + vk / t2\n",
    "        J += Jk / NPosAll[k]\n",
    "        G[k, :] = G[k, :] / NPosAll[k]\n",
    "        \n",
    "    J = J / K\n",
    "    G = G / K\n",
    "        \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_toppush_example(w, X, Y, r=1, weighting=True):\n",
    "    \"\"\"\n",
    "        Objective of top push loss for examples\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "            - r: parameter for log-sum-exp approximation\n",
    "            - weighting: if True, divide K+ in top-push loss\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "    assert(r > 0)\n",
    "    \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    # instead of using diagonal matrix to scale each row of a matrix with a different factor,\n",
    "    # we use Mat * Vec[:, None] which is more memory efficient\n",
    "    \n",
    "    if weighting is True:\n",
    "        KPosAll = np.sum(Y, axis=1)  # number of positive labels for each example, N by 1\n",
    "    else:\n",
    "        KPosAll = np.ones(N)\n",
    "        \n",
    "    A_diag = 1.0 / KPosAll\n",
    "    AY = Y * A_diag[:, None]\n",
    "    \n",
    "    T1 = np.dot(X, W.T)  # N by K\n",
    "    #m0 = np.max(T1)  # underflow in np.exp(r*T1 - m1)\n",
    "    m0 = 0.5 * (np.max(T1) + np.min(T1))\n",
    "    m1 = r * m0\n",
    "    #print('----------------')\n",
    "    #print(np.min(T1), np.max(T1), m0)\n",
    "    #print(np.min(r*T1), np.max(r*T1), m1)\n",
    "    #print(np.min(r * T1 - m1), np.max(r * T1 - m1))\n",
    "    T2 = np.multiply(1 - Y, np.exp(r * T1 - m1))  # N by K\n",
    "    B_tilde_diag = np.dot(T2, np.ones(K))\n",
    "    #print(np.max(B_tilde_diag), np.min(B_tilde_diag))  # big numbers here, can cause overflow in T3\n",
    "    \n",
    "    #T3 = np.exp(-T1 + m0) * np.power(B_tilde_diag, 1.0 / r)[:, None]\n",
    "    #T4 = np.multiply(AY, np.log1p(T3))\n",
    "    T3 = (-T1 + m0) + (1.0 / r) * np.log(B_tilde_diag)[:, None]\n",
    "    #print(np.min(T3), np.max(T3))\n",
    "    m2 = 0.5 * (np.min(T3) + np.max(T3))\n",
    "    #T4 = np.logaddexp(0, T3)\n",
    "    T4 = np.logaddexp(-m2, T3-m2) + m2\n",
    "    T5 = np.multiply(AY, T4)  \n",
    "    \n",
    "    #J = np.dot(w, w) * 0.5 / C + np.dot(np.ones(N), np.dot(T5, np.ones(K))) / N\n",
    "    J = np.dot(np.ones(N), np.dot(T5, np.ones(K))) / N\n",
    "    \n",
    "    #T5 = 1.0 / (1.0 + np.divide(1.0, T3))\n",
    "    #T5 = np.divide(T3, 1 + T3)\n",
    "    T6 = np.exp(T3 - T4)\n",
    "    O_diag = np.dot(np.multiply(Y, T6), np.ones(K))\n",
    "    T7 = A_diag * (1.0 / B_tilde_diag) * O_diag\n",
    "    \n",
    "    G1 = np.dot(np.multiply(AY, T6).T, -X)\n",
    "    \n",
    "    #print(np.max(T2), np.min(T2), np.max(T7), np.min(T7))\n",
    "    T8 = T2 * T7[:, None]\n",
    "    G2 = np.dot(T8.T, X)\n",
    "    \n",
    "    #G = W / C + (G1 + G2) / N\n",
    "    G = (G1 + G2) / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_toppush_label(w, X, Y, r=1, weighting=True):\n",
    "    \"\"\"\n",
    "        Objective with top push loss for labels\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "            - r: parameter for log-sum-exp approximation\n",
    "            - weighting: if True, divide N+ in top-push loss\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "    assert(r > 0)\n",
    "    \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    # instead of using diagonal matrix to scale each row of a matrix with a different factor,\n",
    "    # we use Mat * Vec[:, None] which is more memory efficient\n",
    "    \n",
    "    if weighting is True:\n",
    "        NPosAll = np.sum(Y, axis=0)  # number of positive examples for each label, K by 1\n",
    "    else:\n",
    "        NPosAll = np.ones(K)\n",
    "    P_diag = 1.0 / NPosAll\n",
    "        \n",
    "    T1 = np.dot(X, W.T)  # N by K\n",
    "    T11 = np.multiply(1-Y, T1)\n",
    "    m0 = 0.5 * (np.max(T11) + np.min(T11))\n",
    "    m1 = r * m0\n",
    "    #print(np.max(T11), np.min(T11))\n",
    "    Q_diag = np.dot(np.ones(N), np.multiply(1-Y, np.exp(r*T11-m1)))  # K by 1\n",
    "    Q1 = np.power(Q_diag, 1/r)  # K by 1\n",
    "    T2 = np.multiply(np.exp(-T1+m0), Y).T * Q1[:, None]  # K by N\n",
    "    T3 = np.log1p(T2) * P_diag[:, None]     # K by N\n",
    "    J = np.dot(np.dot(np.ones(N), T3.T), np.ones(K)) / K\n",
    "    \n",
    "    Denom = np.multiply(Y, np.exp(T1-m0)).T * np.divide(1, Q1)[:, None] + 1  # K by N\n",
    "    T4 = np.einsum('nk,nk->k', 1-Y, np.exp(r*T11-m1))  # K by 1\n",
    "    T5 = np.multiply(1-Y, np.exp(r*T11-m1))  # N by K\n",
    "    T6 = np.dot(T5.T, X)  # K by D\n",
    "    T7 = T6 * np.divide(1, T4)[:, None]  # K by D\n",
    "    T8 = np.einsum('nk,nk->k', Y, np.divide(1, Denom).T)  # K by 1\n",
    "    G1 = T7 * T8[:, None]  # K by D\n",
    "    \n",
    "    T9 = np.multiply(Y, np.divide(1, Denom).T)  # N by K\n",
    "    G2 = np.dot(T9.T, X)  # K by D\n",
    "    \n",
    "    G = (G1 - G2) * P_diag[:, None] / K\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w0 = 0.001 * np.random.randn(Y_train.shape[1] * X_train.shape[1])\n",
    "#check_grad(lambda w: obj_toppush_label(w, X_train, Y_train, r=4)[0], \n",
    "#           lambda w: obj_toppush_label(w, X_train, Y_train, r=4)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp_loop_vec(func_loop, func_vec, X_train, Y_train, r=4):\n",
    "    print('%15s %15s %15s %15s %15s' % ('C','J_Diff', 'J_loop', 'J_vec', 'G_Diff'))\n",
    "    w0 = 0.001 * np.random.randn(Y_train.shape[1] * X_train.shape[1])\n",
    "    for e in range(-6, 10):\n",
    "        C = 10**(e)\n",
    "        #w0 = init_var(X_train, Y_train)\n",
    "        J,  G  = func_loop(w0, X_train, Y_train)#, r=r)\n",
    "        J1, G1 = func_vec(w0, X_train, Y_train)#, r=r)\n",
    "        Gdiff = G1 - G\n",
    "        #print('%-15g %-15g %-15g' % (J1 - J, J, J1))\n",
    "        print('%15g %15g %15g %15g %15g' % (C, J1 - J, J, J1, np.dot(Gdiff, Gdiff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grad_loop(func, X_train, Y_train, r=4):\n",
    "    w0 = 0.001 * np.random.randn(Y_train.shape[1] * X_train.shape[1])\n",
    "    eps = 1.49e-08\n",
    "    w = np.zeros_like(w0)\n",
    "    for i in range(len(w0)):\n",
    "        sys.stdout.write('\\r%d / %d' % (i+1, len(w0)))\n",
    "        wi1 = w0.copy()\n",
    "        wi2 = w0.copy()\n",
    "        wi1[i] = wi1[i] - eps\n",
    "        wi2[i] = wi2[i] + eps\n",
    "        J1, _ = func(wi1, X_train, Y_train, r=r)\n",
    "        J2, _ = func(wi2, X_train, Y_train, r=r)\n",
    "        w[i] = (J2 - J1) / (2 * eps)\n",
    "        #print(w[i])\n",
    "    J, w1 = obj_toppush_loop(w0, X_train, Y_train, C)\n",
    "    diff = w1 - w\n",
    "    return np.sqrt(np.dot(diff, diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cmp_loop_vec(obj_toppush_label_loop, obj_toppush_label, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_grad_loop(obj_toppush_label_loop, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w0 = 0.001 * np.random.randn(Y_train.shape[1] * X_train.shape[1])\n",
    "#check_grad(lambda w: obj_toppush_label_loop(w, X_train, Y_train, r=4)[0], \n",
    "#           lambda w: obj_toppush_label_loop(w, X_train, Y_train, r=4)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_xentropy(w, X, Y, weighting=True, ignorePos=False):\n",
    "    \"\"\"\n",
    "    Objective with logistic loss\n",
    "    \n",
    "    Input:\n",
    "            - w: current weight vector, flattened K x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "        \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    if weighting is True:\n",
    "        NK = N * K\n",
    "    else:\n",
    "        NK = N\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    T1 = np.dot(W, X.T)  # K by N\n",
    "    T2 = np.exp(T1)\n",
    "    T3 = np.divide(T2, 1+T2)\n",
    "    T4 = np.log1p(T2)\n",
    "    T5 = np.log1p(np.divide(1.0, T2))\n",
    "    T6 = np.multiply(Y.T, T5-T4)\n",
    "    if not ignorePos:\n",
    "        T7 = T4 + T6  # K by N\n",
    "    else:\n",
    "        T7 = T6\n",
    "    \n",
    "    J = np.dot(np.ones(K), np.dot(T7, np.ones(N))) / NK\n",
    "    \n",
    "    if not ignorePos:\n",
    "        G = np.dot(T3-Y.T, X) / NK\n",
    "    else:\n",
    "        G = np.dot(-Y.T, X) / NK\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_xentropy_loop(w, X, Y, weighting=True, ignorePos=False):\n",
    "    \"\"\"\n",
    "    Objective with logistic loss\n",
    "    \n",
    "    Input:\n",
    "            - w: current weight vector, flattened K x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "    \"\"\"    \n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "        \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    if weighting is True:\n",
    "        NK = N * K\n",
    "    else:\n",
    "        NK = N\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    for k in range(K):\n",
    "        for n in range(N):\n",
    "            t1 = np.exp(np.dot(W[k, :], X[n, :]))\n",
    "            t2 = np.log1p(t1)\n",
    "            if not ignorePos:\n",
    "                J += t2\n",
    "            if Y[n, k] == 1:\n",
    "                J += (np.log1p(1.0 / t1) - t2)\n",
    "            if not ignorePos:\n",
    "                G[k, :] = G[k, :] + X[n, :] * (t1 / (1 + t1) - Y[n, k])\n",
    "            else:\n",
    "                G[k, :] = G[k, :] + X[n, :] * (-Y[n, k])\n",
    "                \n",
    "    J = J / NK\n",
    "    G = G / NK\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w0 = 0.001 * np.random.randn(Y_train.shape[1] * X_train.shape[1])\n",
    "#check_grad(lambda w: obj_xentropy(w, X_train, Y_train, ignorePos=True)[0], \n",
    "#           lambda w: obj_xentropy(w, X_train, Y_train, ignorePos=True)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cmp_loop_vec(obj_xentropy_loop, obj_xentropy, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_hybrid_TP_LR(w, X, Y, C, C1, r=8, weighting=True):\n",
    "    \"\"\"\n",
    "    Objective with L2 regularisation and top push loss\n",
    "    \"\"\"\n",
    "    \n",
    "    assert C > 0\n",
    "    assert C1 > 0\n",
    "    assert r > 0\n",
    "    \n",
    "    J1, G1 = obj_toppush_example(w, X, Y, r, weighting)\n",
    "    J2, G2 = obj_xentropy(w, X, Y)\n",
    "    \n",
    "    J = np.dot(w, w) * 0.5 / C + J1 + C1 * J2\n",
    "    G = w / C + G1 + C1 * G2\n",
    "    \n",
    "    return (J, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_hybrid_TP_LR2(w, X, Y, C, C1, r=8, weighting=True):\n",
    "    \"\"\"\n",
    "    Objective with L2 regularisation and top push loss\n",
    "    \"\"\"\n",
    "    \n",
    "    assert C > 0\n",
    "    assert C1 > 0\n",
    "    assert r > 0\n",
    "    \n",
    "    J1, G1 = obj_toppush_example(w, X, Y, r, weighting)\n",
    "    J2, G2 = obj_xentropy(w, X, Y, ignorePos=True)\n",
    "    \n",
    "    J = np.dot(w, w) * 0.5 / C + J1 + C1 * J2\n",
    "    G = w / C + G1 + C1 * G2\n",
    "    \n",
    "    return (J, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_hybrid_TP_TP(w, X, Y, C, C1=1, r=8, weighting=True):\n",
    "    \"\"\"\n",
    "    Objective with L2 regularisation and top push loss\n",
    "    \"\"\"\n",
    "    \n",
    "    assert C > 0\n",
    "    assert C1 > 0\n",
    "    assert r > 0\n",
    "    \n",
    "    J1, G1 = obj_toppush_example(w, X, Y, r, weighting)\n",
    "    J2, G2 = obj_toppush_label(w, X, Y, r, weighting)\n",
    "    \n",
    "    J = np.dot(w, w) * 0.5 / C + J1 + C1 * J2\n",
    "    G = w / C + G1 + C1 * G2\n",
    "    \n",
    "    return (J, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_hybrid_LR_TP(w, X, Y, C, C1=1, r=8, weighting=True):\n",
    "    \"\"\"\n",
    "    Objective with L2 regularisation and top push loss\n",
    "    \"\"\"\n",
    "    \n",
    "    assert C > 0\n",
    "    assert C1 > 0\n",
    "    assert r > 0\n",
    "    \n",
    "    J1, G1 = obj_xentropy(w, X, Y)\n",
    "    J2, G2 = obj_toppush_label(w, X, Y, r, weighting)\n",
    "    \n",
    "    J = np.dot(w, w) * 0.5 / C + J1 + C1 * J2\n",
    "    G = w / C + G1 + C1 * G2\n",
    "    \n",
    "    return (J, G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "#X_train = X_train[:50, :]\n",
    "#Y_train = Y_train[:50, :]\n",
    "C = 1\n",
    "C1 = 1\n",
    "w0 = 0.001 * np.random.randn(Y_train.shape[1] * X_train.shape[1])\n",
    "#obj_func = obj_hybrid_TP_LR\n",
    "#obj_func = obj_hybrid_TP_TP\n",
    "#obj_func = obj_hybrid_LR_TP\n",
    "obj_func = obj_hybrid_TP_LR2\n",
    "\n",
    "check_grad(lambda w: obj_func(w, X_train, Y_train, C, C1, r=8)[0], \n",
    "           lambda w: obj_func(w, X_train, Y_train, C, C1, r=8)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLC_hybrid(BaseEstimator):\n",
    "    \"\"\"All methods are necessary for a scikit-learn estimator\"\"\"\n",
    "    \n",
    "    def __init__(self, C=1, C1=1, r=1, weighting=True):\n",
    "        \"\"\"Initialisation\"\"\"\n",
    "        \n",
    "        assert C > 0\n",
    "        assert C1 > 0\n",
    "        assert r > 0\n",
    "        assert type(weighting) == bool\n",
    "        self.C = C\n",
    "        self.C1 = C1\n",
    "        self.r = r\n",
    "        self.weighting = weighting\n",
    "        #self.obj_func = obj_hybrid_TP_LR\n",
    "        #self.obj_func = obj_hybrid_LR_TP\n",
    "        #self.obj_func = obj_hybrid_TP_TP\n",
    "        self.obj_func = obj_hybrid_TP_LR2\n",
    "        self.trained = False\n",
    "        \n",
    "    def fit(self, X_train, Y_train):\n",
    "        \"\"\"Model fitting by optimising the objective\"\"\"\n",
    "        opt_method = 'L-BFGS-B' #'BFGS' #'Newton-CG'\n",
    "        options = {'disp': 1, 'maxiter': 10**5, 'maxfun': 10**5} # , 'iprint': 99}\n",
    "        print('\\nC: %g, C1: %g, r: %g, weighting: %s' % (self.C, self.C1, self.r, self.weighting))\n",
    "            \n",
    "        N, D = X_train.shape\n",
    "        K = Y_train.shape[1]\n",
    "        #w0 = np.random.rand(K * D) - 0.5  # initial guess in range [-1, 1]\n",
    "        w0 = 0.001 * np.random.randn(K * D)\n",
    "        opt = minimize(self.obj_func, w0, args=(X_train, Y_train, self.C, self.C1, self.r, self.weighting), \\\n",
    "                       method=opt_method, jac=True, options=options)\n",
    "        if opt.success is True:\n",
    "            self.W = np.reshape(opt.x, (K, D))\n",
    "            self.trained = True\n",
    "        else:\n",
    "            sys.stderr.write('Optimisation failed')\n",
    "            print(opt.items())\n",
    "            self.trained = False\n",
    "            \n",
    "            \n",
    "    def decision_function(self, X_test):\n",
    "        \"\"\"Make predictions (score is real number)\"\"\"\n",
    "        \n",
    "        assert self.trained is True, \"Can't make prediction before training\"\n",
    "        D = X_test.shape[1]\n",
    "        return np.dot(X_test, self.W.T)\n",
    "        \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return self.decision_function(X_test)\n",
    "    #    \"\"\"Make predictions (score is boolean)\"\"\"   \n",
    "    #    preds = sigmoid(self.decision_function(X_test))\n",
    "    #    #return (preds >= 0)\n",
    "    #    assert self.TH is not None\n",
    "    #    return preds >= self.TH        \n",
    "        \n",
    "    # inherit from BaseEstimator instead of re-implement\n",
    "    #\n",
    "    #def get_params(self, deep = True):\n",
    "    #def set_params(self, **params):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_results(predictor, X_train, Y_train, X_test, Y_test, fname, rankingLoss=False):\n",
    "    \"\"\"\n",
    "        Compute and save performance results\n",
    "    \"\"\"\n",
    "    preds_train = predictor.decision_function(X_train)\n",
    "    preds_test  = predictor.decision_function(X_test)\n",
    "    \n",
    "    print('Training set:')\n",
    "    perf_dict_train = evaluatePrecision(Y_train, preds_train, verbose=1)\n",
    "    print()\n",
    "    print('Test set:')\n",
    "    perf_dict_test = evaluatePrecision(Y_test, preds_test, verbose=1)\n",
    "    \n",
    "    if rankingLoss is True:\n",
    "        print()\n",
    "        print('Training set:')\n",
    "        perf_dict_train.update(evaluateRankingLoss(Y_train, preds_train))\n",
    "        print(label_ranking_loss(Y_train, preds_train))\n",
    "        print()\n",
    "        print('Test set:')\n",
    "        perf_dict_test.update(evaluateRankingLoss(Y_test, preds_test))\n",
    "        print(label_ranking_loss(Y_test, preds_test))\n",
    "\n",
    "    # compute F1 score w.r.t. different thresholds\n",
    "    #TH1 = predictor.cv_results_['mean_test_TH'][clf.best_index_]\n",
    "    #TH2 = np.mean(Y_train, axis=0)\n",
    "    #TH3 = np.mean(TH2)\n",
    "    \n",
    "    #preds_train_bin = sigmoid(preds_train)\n",
    "    #preds_test_bin  = sigmoid(preds_test)\n",
    "    \n",
    "    #F1_train1 = f1_score_nowarn(Y_train, sigmoid(preds_train) >= TH1, average='samples')\n",
    "    #F1_test1  = f1_score_nowarn(Y_test, sigmoid(preds_test) >= TH1, average='samples')\n",
    "    #print('\\nTrain: %.4f, %f' % (F1_train1, f1_score(Y_train, sigmoid(preds_train) >= TH1, average='samples')))\n",
    "    #print('\\nTest : %.4f, %f' % (F1_test1, f1_score(Y_test, sigmoid(preds_test) >= TH1, average='samples')))\n",
    "    \n",
    "    #F1_train2 = f1_score_nowarn(Y_train, (preds_train_bin - TH2) >= 0, average='samples')\n",
    "    #F1_test2  = f1_score_nowarn(Y_test, (preds_test_bin - TH2) >= 0, average='samples')\n",
    "    #print('\\nTrain: %.4f, %f' % (F1_train2, f1_score(Y_train, (preds_train_bin - TH2) >= 0, average='samples')))\n",
    "    #print('\\nTest : %.4f, %f' % (F1_test2, f1_score(Y_test, (preds_test_bin - TH2) >= 0, average='samples')))\n",
    "    \n",
    "    #F1_train3 = f1_score_nowarn(Y_train, preds_train_bin >= TH3, average='samples')\n",
    "    #F1_test3  = f1_score_nowarn(Y_test, preds_test_bin >= TH3, average='samples')\n",
    "    #print('\\nTrain: %.4f, %f' % (F1_train3, f1_score(Y_train, preds_train_bin >= TH3, average='samples')))\n",
    "    #print('\\nTest : %.4f, %f' % (F1_test3, f1_score(Y_test, preds_test_bin >= TH3, average='samples')))\n",
    "    \n",
    "    #perf_dict_train.update({'F1': [(F1_train1,), (F1_train2,), (F1_train3,)]})\n",
    "    #perf_dict_test.update( {'F1': [(F1_test1,),  (F1_test2,),  (F1_test3,)]})\n",
    "    #perf_dict_train.update({'F1': [(F1_train2,), (F1_train3,)]})\n",
    "    #perf_dict_test.update( {'F1': [(F1_test2,),  (F1_test3,)]})\n",
    "    \n",
    "    perf_dict = {'Train': perf_dict_train, 'Test': perf_dict_test}\n",
    "    if os.path.exists(fname):\n",
    "        _dict = pkl.load(open(fname, 'rb'))\n",
    "        if dataset_name not in _dict:\n",
    "            _dict[dataset_name] = perf_dict\n",
    "    else:\n",
    "        _dict = {dataset_name: perf_dict}\n",
    "    pkl.dump(_dict, open(fname, 'wb'))\n",
    "    \n",
    "    print()\n",
    "    print(pkl.load(open(fname, 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'ignore', 'invalid': 'ignore', 'over': 'ignore', 'under': 'ignore'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_settings = np.seterr(all='ignore')  # seterr to known value\n",
    "np.seterr(all='raise')\n",
    "#np.seterr(all='ignore')\n",
    "#np.seterr(**old_settings)  # restore settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%memit model.fit(X_train[:30], Y_train[:30])\n",
    "#%mprun -f minimize model.fit(X_train[:100], Y_train[:100])\n",
    "#%mprun -f _minimize_slsqp model.fit(X_train[:10], Y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "if os.path.exists(fmodel_base):\n",
    "    clf = pkl.load(open(fmodel_base, 'rb'))\n",
    "else:\n",
    "    clf = clf = MLC_hybrid()\n",
    "    clf.fit(X_train, Y_train)\n",
    "    pkl.dump(clf, open(fmodel_base, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump_results(clf, X_train, Y_train, X_test, Y_test, fperf_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation w.r.t. average precision@K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ranges = range(-6, 7)\n",
    "#ranges = range(-6, 5)\n",
    "#parameters = [{'C': sorted([10**(e) for e in ranges] + [3 * 10**(e) for e in ranges]),\n",
    "parameters = [{'C': [1e-3, 3e-3, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300],#, 1e3],\n",
    "               'C1': [0.5, 1, 2],\n",
    "               'r': [8],\n",
    "               'weighting': [True, False],\n",
    "              }]\n",
    "scorer = {'Prec': make_scorer(avgPrecisionK)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fmodel_prec = os.path.join(data_dir, 'tph-' + dataset_name + '-tp-lr2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C: 0.001, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.001, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.001, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.001, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.001, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.001, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.001, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.001, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.001, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.001, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.001, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.001, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.001, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.001, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.001, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.001, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.001, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.001, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.001, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.001, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.001, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.001, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.001, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.001, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.001, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.001, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.001, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.001, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.001, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.001, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.003, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.003, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.003, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.003, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.003, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.003, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.003, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.003, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.003, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.003, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.003, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.003, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.003, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.003, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.003, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.003, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.003, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.003, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.003, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.003, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.003, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.003, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.003, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.003, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.003, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.003, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.003, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.003, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.003, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.003, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.01, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.01, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.01, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.01, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.01, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.01, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.01, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.01, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.01, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.01, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.01, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.01, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.01, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.01, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.01, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.01, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.01, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.01, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.01, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.01, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.01, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.01, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.01, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.01, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.01, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.01, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.01, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.01, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.01, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.01, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.03, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.03, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.03, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.03, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.03, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.03, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.03, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.03, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.03, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.03, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.03, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.03, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.03, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.03, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.03, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.03, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.03, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.03, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.03, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.03, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.03, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.03, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.03, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.03, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.03, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.03, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.03, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.03, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.03, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.03, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.1, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.1, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.1, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.1, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.1, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.1, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.1, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.1, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.1, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.1, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.1, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.1, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.1, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.1, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.1, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.1, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.1, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.1, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.1, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.1, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.1, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.1, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.1, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.1, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.1, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.1, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.1, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.1, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.1, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.1, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.3, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.3, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.3, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.3, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.3, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 0.3, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.3, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.3, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.3, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.3, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 0.3, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.3, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.3, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.3, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.3, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 0.3, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.3, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.3, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.3, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.3, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 0.3, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.3, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.3, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.3, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.3, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 0.3, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.3, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.3, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.3, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 0.3, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 1, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 1, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 1, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 1, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 1, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 1, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 1, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 1, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 1, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 1, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 1, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 1, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 1, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 1, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 1, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 1, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 1, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 1, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 1, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 1, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 1, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 1, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 1, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 1, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 1, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 1, C1: 2, r: 8, weighting: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C: 1, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 1, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 1, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 1, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 3, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 3, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 3, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 3, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 3, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 3, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 3, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 3, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 3, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 3, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 3, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 3, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 3, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 3, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 3, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 3, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 3, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 3, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 3, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 3, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 3, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 3, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 3, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 3, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 3, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 3, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 3, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 3, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 3, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 3, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 10, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 10, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 10, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 10, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 10, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 10, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 10, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 10, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 10, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 10, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 10, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 10, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 10, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 10, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 10, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 10, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 10, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 10, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 10, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 10, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 10, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 10, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 10, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 10, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 10, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 10, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 10, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 10, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 10, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 10, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 30, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 30, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 30, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 30, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 30, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 30, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 30, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 30, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 30, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 30, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 30, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 30, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 30, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 30, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 30, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 30, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 30, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 30, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 30, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 30, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 30, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 30, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 30, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 30, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 30, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 30, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 30, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 30, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 30, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 30, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 100, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 100, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 100, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 100, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 100, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 100, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 100, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 100, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 100, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 100, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 100, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 100, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 100, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 100, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 100, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 100, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 100, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 100, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 100, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 100, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 100, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 100, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 100, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 100, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 100, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 100, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 100, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 100, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 100, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 100, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 300, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 300, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 300, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 300, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 300, C1: 0.5, r: 8, weighting: True\n",
      "\n",
      "C: 300, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 300, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 300, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 300, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 300, C1: 0.5, r: 8, weighting: False\n",
      "\n",
      "C: 300, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 300, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 300, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 300, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 300, C1: 1, r: 8, weighting: True\n",
      "\n",
      "C: 300, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 300, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 300, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 300, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 300, C1: 1, r: 8, weighting: False\n",
      "\n",
      "C: 300, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 300, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 300, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 300, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 300, C1: 2, r: 8, weighting: True\n",
      "\n",
      "C: 300, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 300, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 300, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 300, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 300, C1: 2, r: 8, weighting: False\n",
      "\n",
      "C: 30, C1: 0.5, r: 8, weighting: True\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(fmodel_prec):\n",
    "    clf = GridSearchCV(MLC_hybrid(), parameters, scoring=scorer, cv=5, n_jobs=1, refit='Prec')\n",
    "    clf.fit(X_train, Y_train)\n",
    "    #pkl.dump(clf, open(fmodel_prec, 'wb'))\n",
    "else:\n",
    "    clf = pkl.load(open(fmodel_prec, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Average Precision@3: 0.6483, 0.005\n",
      "Average Precision@5: 0.4448, 0.004\n",
      "Average Precision@10: 0.2331, 0.002\n",
      "Average Precision@K: 0.9752, 0.002\n",
      "\n",
      "Test set:\n",
      "Average Precision@3: 0.3958, 0.009\n",
      "Average Precision@5: 0.2859, 0.007\n",
      "Average Precision@10: 0.1737, 0.004\n",
      "Average Precision@K: 0.5114, 0.012\n",
      "\n",
      "Training set:\n",
      "Average RankingLoss: 1.0784, 0.153\n",
      "0.00152134586631\n",
      "\n",
      "Test set:\n",
      "Average RankingLoss: 27.8924, 1.644\n",
      "0.0726339483582\n",
      "\n",
      "{'bibtex': {'Train': {'Precision@3': (0.63934426229508201, 0.004116089338362155), 'Precision@5': (0.43979508196721306, 0.0036297213637683607), 'Precision@10': (0.23174180327868854, 0.0021432623293462356), 'Precision@K': (0.95607687877923542, 0.0022795006116223477), 'RankingLoss': (1.5038934426229509, 0.1477394961812328)}, 'Test': {'Precision@3': (0.39549370444002652, 0.0056192723394597075), 'Precision@5': (0.28882703777335988, 0.0040664892761898943), 'Precision@10': (0.1759840954274354, 0.0024834010761927527), 'Precision@K': (0.51715523525838114, 0.0077593318677975781), 'RankingLoss': (27.534791252485089, 1.0264545560002458)}}, 'bookmarks': {'Train': {'Precision@3': (0.34539999999999998, 0.0010341301229106086), 'Precision@5': (0.24130333333333334, 0.00073864780348457861), 'Precision@10': (0.1427316666666667, 0.00045618900804909228), 'Precision@K': (0.57970501224103088, 0.001766823147943173), 'RankingLoss': (27.798950000000001, 0.30160114745851291)}, 'Test': {'Precision@3': (0.26227742676622628, 0.0014423346837764233), 'Precision@5': (0.1896754738655945, 0.00099707929009384771), 'Precision@10': (0.11838742102240095, 0.00061333783234079225), 'Precision@K': (0.42684295751839713, 0.0026110600018119153), 'RankingLoss': (43.424109707064908, 0.53076698979391912)}}, 'yeast': {'Train': {'Precision@3': (0.60622222222222222, 0.0082989731131333511), 'Precision@5': (0.50306666666666666, 0.0066381353919215506), 'Precision@10': (0.3289333333333333, 0.003757321346877813), 'Precision@K': (0.56522720057720066, 0.00771359187755076), 'RankingLoss': (12.018666666666666, 0.2594502541343337)}, 'Test': {'Precision@3': (0.5478007997091966, 0.010687224337764846), 'Precision@5': (0.46019629225736097, 0.0083077564831782573), 'Precision@10': (0.31679389312977096, 0.0048083286305502923), 'Precision@K': (0.51514946945699402, 0.0098793558493042392), 'RankingLoss': (14.001090512540895, 0.34801021320200004)}}}\n"
     ]
    }
   ],
   "source": [
    "dump_results(clf, X_train, Y_train, X_test, Y_test, fperf_prec, rankingLoss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = clf.decision_function(X_train)\n",
    "tploss_train = calcLoss(Y_train, preds_train, 'TopPush', njobs=4)\n",
    "pak_train = calcLoss(Y_train, preds_train, 'Precision@K', njobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = clf.decision_function(X_test)\n",
    "tploss_test = calcLoss(Y_test, preds_test, 'TopPush', njobs=4)\n",
    "pak_test = calcLoss(Y_test, preds_test, 'Precision@K', njobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss, pak, title):\n",
    "    # the data\n",
    "    x = loss\n",
    "    y = 1 - pak\n",
    "    \n",
    "    print('away from diagonal portion:', np.mean(loss != 1-pak))\n",
    "\n",
    "    nullfmt = NullFormatter()         # no labels\n",
    "\n",
    "    # definitions for the axes\n",
    "    left, width = 0.1, 0.65\n",
    "    bottom, height = 0.1, 0.65\n",
    "    bottom_h = left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.2]\n",
    "    rect_histy = [left_h, bottom, 0.2, height]\n",
    "\n",
    "    # start with a rectangular Figure\n",
    "    plt.figure(1, figsize=(8, 8))\n",
    "\n",
    "    axScatter = plt.axes(rect_scatter)\n",
    "    axHistx = plt.axes(rect_histx)\n",
    "    axHisty = plt.axes(rect_histy)\n",
    "\n",
    "    # no labels\n",
    "    axHistx.xaxis.set_major_formatter(nullfmt)\n",
    "    axHisty.yaxis.set_major_formatter(nullfmt)\n",
    "\n",
    "    # the scatter plot:\n",
    "    axScatter.scatter(x, y, color='b', alpha=0.5)\n",
    "    axScatter.plot([0, 1], [0, 1], ls='--', color='g')\n",
    "    axScatter.set_xlabel('Top push loss', fontdict={'fontsize': 12})\n",
    "    axScatter.set_ylabel('1 - precision@K', fontdict={'fontsize': 12})\n",
    "\n",
    "    # now determine nice limits by hand:\n",
    "    #binwidth = 0.25\n",
    "    #xymax = np.max([np.max(np.fabs(x)), np.max(np.fabs(y))])\n",
    "    #lim = (int(xymax/binwidth) + 1) * binwidth\n",
    "\n",
    "    #axScatter.set_xlim((-lim, lim))\n",
    "    #axScatter.set_ylim((-lim, lim))\n",
    "\n",
    "    #bins = np.arange(-lim, lim + binwidth, binwidth)\n",
    "\n",
    "    axHistx.hist(x, bins=10, color='g', alpha=0.3)\n",
    "    axHistx.set_yscale('log')\n",
    "    axHisty.hist(y, bins=10, color='g', alpha=0.3, orientation='horizontal')\n",
    "    axHisty.set_xscale('log')\n",
    "\n",
    "    #axHistx.set_xlim(axScatter.get_xlim())\n",
    "    #axHisty.set_ylim(axScatter.get_ylim())\n",
    "\n",
    "    axHistx.set_title(title, fontdict={'fontsize': 15}, loc='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "away from diagonal portion: 0.0343237704918\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb654071d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(tploss_train, pak_train, 'Training set (' + dataset_name + ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3904,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tploss_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tploss_train != 1-pak_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(976,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tploss_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "away from diagonal portion: 0.282786885246\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb6519d0080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(tploss_test, pak_test, 'Test set (' + dataset_name + ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(clf, open(os.path.join(data_dir, 'tph-' + dataset_name + '-tp-lr2.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = pkl.load(open(os.path.join(data_dir, 'tph-' + dataset_name + '-tp-lr2.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 30, 'C1': 2, 'r': 8, 'weighting': True}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Average Precision@3: 0.6464, 0.004\n",
      "Average Precision@5: 0.4438, 0.004\n",
      "Average Precision@10: 0.2328, 0.002\n",
      "Average Precision@K: 0.9691, 0.002\n",
      "\n",
      "Test set:\n",
      "Average Precision@3: 0.3928, 0.006\n",
      "Average Precision@5: 0.2839, 0.004\n",
      "Average Precision@10: 0.1738, 0.002\n",
      "Average Precision@K: 0.5132, 0.008\n",
      "\n",
      "{'bibtex': {'Train': {'Precision@3': (0.63934426229508201, 0.004116089338362155), 'Precision@5': (0.43979508196721306, 0.0036297213637683607), 'Precision@10': (0.23174180327868854, 0.0021432623293462356), 'Precision@K': (0.95607687877923542, 0.0022795006116223477), 'RankingLoss': (1.5038934426229509, 0.1477394961812328)}, 'Test': {'Precision@3': (0.39549370444002652, 0.0056192723394597075), 'Precision@5': (0.28882703777335988, 0.0040664892761898943), 'Precision@10': (0.1759840954274354, 0.0024834010761927527), 'Precision@K': (0.51715523525838114, 0.0077593318677975781), 'RankingLoss': (27.534791252485089, 1.0264545560002458)}}, 'bookmarks': {'Train': {'Precision@3': (0.34539999999999998, 0.0010341301229106086), 'Precision@5': (0.24130333333333334, 0.00073864780348457861), 'Precision@10': (0.1427316666666667, 0.00045618900804909228), 'Precision@K': (0.57970501224103088, 0.001766823147943173), 'RankingLoss': (27.798950000000001, 0.30160114745851291)}, 'Test': {'Precision@3': (0.26227742676622628, 0.0014423346837764233), 'Precision@5': (0.1896754738655945, 0.00099707929009384771), 'Precision@10': (0.11838742102240095, 0.00061333783234079225), 'Precision@K': (0.42684295751839713, 0.0026110600018119153), 'RankingLoss': (43.424109707064908, 0.53076698979391912)}}, 'yeast': {'Train': {'Precision@3': (0.60622222222222222, 0.0082989731131333511), 'Precision@5': (0.50306666666666666, 0.0066381353919215506), 'Precision@10': (0.3289333333333333, 0.003757321346877813), 'Precision@K': (0.56522720057720066, 0.00771359187755076), 'RankingLoss': (12.018666666666666, 0.2594502541343337)}, 'Test': {'Precision@3': (0.5478007997091966, 0.010687224337764846), 'Precision@5': (0.46019629225736097, 0.0083077564831782573), 'Precision@10': (0.31679389312977096, 0.0048083286305502923), 'Precision@K': (0.51514946945699402, 0.0098793558493042392), 'RankingLoss': (14.001090512540895, 0.34801021320200004)}}}\n"
     ]
    }
   ],
   "source": [
    "dump_results(clf, X_train, Y_train, X_test, Y_test, fperf_prec, rankingLoss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40466389830009836"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_nowarn(Y_test, clf.decision_function(X_test) > 1.09, average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = MLC_hybrid(C=300, C1=2, r=8, weighting=True)\n",
    "clf2.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_results(clf2, X_train, Y_train, X_test, Y_test, fperf_prec, rankingLoss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1_score_nowarn(Y_test, clf.decision_function(X_test) > 0, average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
