{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label classification -- hybrid loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, time\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import check_grad\n",
    "from scipy.special import logsumexp\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer, label_ranking_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('src')\n",
    "from evaluate import avgPrecisionK, evaluatePrecision, evaluateF1, evaluateRankingLoss, f1_score_nowarn\n",
    "from datasets import create_dataset, dataset_names, nLabels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yeast', 'scene', 'bibtex', 'bookmarks', 'delicious', 'mediamill']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ix = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yeast 14\n"
     ]
    }
   ],
   "source": [
    "dataset_name = dataset_names[data_ix]\n",
    "nLabels = nLabels_dict[dataset_name]\n",
    "print(dataset_name, nLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "SEED = 918273645\n",
    "fmodel_base = os.path.join(data_dir, 'tph-' + dataset_name + '-base.pkl')\n",
    "fmodel_prec = os.path.join(data_dir, 'tph-' + dataset_name + '-prec.pkl')\n",
    "fmodel_noclsw = os.path.join(data_dir, 'tph-' + dataset_name + '-noclsw.pkl')\n",
    "fperf_base = os.path.join(data_dir, 'perf-tph-base.pkl')\n",
    "fperf_prec = os.path.join(data_dir, 'perf-tph-prec.pkl')\n",
    "fperf_noclsw = os.path.join(data_dir, 'perf-tph-noclsw.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = create_dataset(dataset_name, train_data=True, shuffle=True, random_state=SEED)\n",
    "X_test,  Y_test  = create_dataset(dataset_name, train_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mean = np.mean(X_train, axis=0).reshape((1, -1))\n",
    "X_train_std = np.std(X_train, axis=0).reshape((1, -1)) + 10 ** (-6)\n",
    "X_train -= X_train_mean\n",
    "X_train /= X_train_std\n",
    "X_test  -= X_train_mean\n",
    "X_test  /= X_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_info(X_train, Y_train, X_test, Y_test):\n",
    "    N_train, D = X_train.shape\n",
    "    K = Y_train.shape[1]\n",
    "    N_test = X_test.shape[0]\n",
    "    print('%-45s %s' % ('Number of training examples:', '{:,}'.format(N_train)))\n",
    "    print('%-45s %s' % ('Number of test examples:', '{:,}'.format(N_test)))\n",
    "    print('%-45s %s' % ('Number of features:', '{:,}'.format(D)))\n",
    "    print('%-45s %s' % ('Number of labels:', '{:,}'.format(K)))\n",
    "    avgK_train = np.mean(np.sum(Y_train, axis=1))\n",
    "    avgK_test  = np.mean(np.sum(Y_test, axis=1))\n",
    "    print('%-45s %.3f (%.2f%%)' % ('Average number of positive labels (train):', avgK_train, 100*avgK_train / K))\n",
    "    print('%-45s %.3f (%.2f%%)' % ('Average number of positive labels (test):', avgK_test, 100*avgK_test / K))\n",
    "    #print('%-45s %.4f%%' % ('Average label occurrence (train):', np.mean(np.sum(Y_train, axis=0)) / N_train))\n",
    "    #print('%-45s %.4f%%' % ('Average label occurrence (test):', np.mean(np.sum(Y_test, axis=0)) / N_test))\n",
    "    print('%-45s %.3f%%' % ('Sparsity (percent) (train):', 100 * np.sum(Y_train) / np.prod(Y_train.shape)))\n",
    "    print('%-45s %.3f%%' % ('Sparsity (percent) (test):', 100 * np.sum(Y_test) / np.prod(Y_test.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:                                      yeast\n",
      "Number of training examples:                  1,500\n",
      "Number of test examples:                      917\n",
      "Number of features:                           103\n",
      "Number of labels:                             14\n",
      "Average number of positive labels (train):    4.228 (30.20%)\n",
      "Average number of positive labels (test):     4.252 (30.37%)\n",
      "Sparsity (percent) (train):                   30.200%\n",
      "Sparsity (percent) (test):                    30.371%\n"
     ]
    }
   ],
   "source": [
    "print('%-45s %s' % ('Dataset:', dataset_name))\n",
    "print_dataset_info(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate `max()` using `log-sum-exp()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.random.rand(500000).reshape(10, 50000) * np.arange(1, 11)[:, None]\n",
    "maxes = np.max(xs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50000)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print(xs.shape)\n",
    "print(maxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8754f72e80>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f875dcc7940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#rs = np.array([0.5, 1, 2, 4, 8, 16, 32, 64])\n",
    "rs = np.array([4, 8, 16, 32, 64])\n",
    "mses = []\n",
    "for r in rs:\n",
    "    approx = []\n",
    "    for i in range(xs.shape[0]):\n",
    "        approx.append(np.log(np.sum(np.exp(r * xs[i, :]))) / r)\n",
    "    deltas = np.array(approx) - maxes\n",
    "    mses.append(np.dot(deltas, deltas))\n",
    "\n",
    "#fig = plt.Figure(figsize=[20, 12])\n",
    "plt.plot(rs, mses, ls='--', marker='o', c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top-push loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-label learning with top push loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_toppush_example(w, X, Y, r=1, weighting=True):\n",
    "    \"\"\"\n",
    "        Objective of top push loss for examples\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "            - r: parameter for log-sum-exp approximation\n",
    "            - weighting: if True, divide K+ in top-push loss\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "    assert(r > 0)\n",
    "    \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    # instead of using diagonal matrix to scale each row of a matrix with a different factor,\n",
    "    # we use Mat * Vec[:, None] which is more memory efficient\n",
    "    \n",
    "    if weighting is True:\n",
    "        KPosAll = np.sum(Y, axis=1)  # number of positive labels for each example, N by 1\n",
    "    else:\n",
    "        KPosAll = np.ones(N)\n",
    "        \n",
    "    A_diag = 1.0 / KPosAll\n",
    "    AY = Y * A_diag[:, None]\n",
    "    \n",
    "    T1 = np.dot(X, W.T)  # N by K\n",
    "    #m0 = np.max(T1)  # underflow in np.exp(r*T1 - m1)\n",
    "    m0 = 0.5 * (np.max(T1) + np.min(T1))\n",
    "    m1 = r * m0\n",
    "    #print('----------------')\n",
    "    #print(np.min(T1), np.max(T1), m0)\n",
    "    #print(np.min(r*T1), np.max(r*T1), m1)\n",
    "    #print(np.min(r * T1 - m1), np.max(r * T1 - m1))\n",
    "    T2 = np.multiply(1 - Y, np.exp(r * T1 - m1))  # N by K\n",
    "    B_tilde_diag = np.dot(T2, np.ones(K))\n",
    "    #print(np.max(B_tilde_diag), np.min(B_tilde_diag))  # big numbers here, can cause overflow in T3\n",
    "    \n",
    "    #T3 = np.exp(-T1 + m0) * np.power(B_tilde_diag, 1.0 / r)[:, None]\n",
    "    #T4 = np.multiply(AY, np.log1p(T3))\n",
    "    T3 = (-T1 + m0) + (1.0 / r) * np.log(B_tilde_diag)[:, None]\n",
    "    #print(np.min(T3), np.max(T3))\n",
    "    m2 = 0.5 * (np.min(T3) + np.max(T3))\n",
    "    #T4 = np.logaddexp(0, T3)\n",
    "    T4 = np.logaddexp(-m2, T3-m2) + m2\n",
    "    T5 = np.multiply(AY, T4)  \n",
    "    \n",
    "    #J = np.dot(w, w) * 0.5 / C + np.dot(np.ones(N), np.dot(T5, np.ones(K))) / N\n",
    "    J = np.dot(np.ones(N), np.dot(T5, np.ones(K))) / N\n",
    "    \n",
    "    #T5 = 1.0 / (1.0 + np.divide(1.0, T3))\n",
    "    #T5 = np.divide(T3, 1 + T3)\n",
    "    T6 = np.exp(T3 - T4)\n",
    "    O_diag = np.dot(np.multiply(Y, T6), np.ones(K))\n",
    "    T7 = A_diag * (1.0 / B_tilde_diag) * O_diag\n",
    "    \n",
    "    G1 = np.dot(np.multiply(AY, T6).T, -X)\n",
    "    G2 = np.dot((T2 * T7[:, None]).T, X)\n",
    "    \n",
    "    #G = W / C + (G1 + G2) / N\n",
    "    G = (G1 + G2) / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_toppush_example_loop(w, X, Y, r=1, weighting=True):\n",
    "    \"\"\"\n",
    "        Objective of top push loss for examples\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "            - C: regularisation constant, C = 1 / lambda\n",
    "            - r: parameter for log-sum-exp approximation\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "    assert(r > 0)\n",
    "    \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    if weighting is True:\n",
    "        KPosAll = np.sum(Y, axis=1)  # number of positive labels for each example, N by 1\n",
    "    else:\n",
    "        KPosAll = np.ones(N)\n",
    "    \n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            if Y[n, k] == 1:\n",
    "                s1 = np.sum([np.exp(r * np.dot(W[j, :] - W[k, :], X[n, :])) for j in range(K) if Y[n, j] == 0])\n",
    "                J += np.log1p(np.power(s1, 1.0 / r)) / KPosAll[n]\n",
    "    #J = np.dot(w, w) * 0.5 / C + J / N\n",
    "    J = J / N\n",
    "    \n",
    "    for k in range(K):\n",
    "        for n in range(N):\n",
    "            if Y[n, k] == 1:\n",
    "                t1 = np.sum([np.exp(r * np.dot(W[j, :] - W[k, :], X[n, :])) for j in range(K) if Y[n, j] == 0])\n",
    "                t2 = -1.0 / (1 + np.power(t1, -1.0 / r))\n",
    "                G[k, :] = G[k, :] + X[n, :] * t2 / KPosAll[n]\n",
    "            else:\n",
    "                sk = 0.0\n",
    "                for k1 in range(K):\n",
    "                    if Y[n, k1] == 1:\n",
    "                        t3 = np.sum([np.exp(r * np.dot(W[j,:] - W[k1, :], X[n, :])) \\\n",
    "                                     for j in range(K) if Y[n, j] == 0])\n",
    "                        t4 = np.exp(r * np.dot(W[k, :] - W[k1, :], X[n, :]))\n",
    "                        sk += t4 / (np.power(t3, 1.0 - 1.0 / r) + t3)\n",
    "                G[k, :] = G[k, :] + X[n, :] * sk / KPosAll[n]\n",
    "                        \n",
    "    #G = W / C + G / N\n",
    "    G = G / N\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-16-ee95fe8e7f03>, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-ee95fe8e7f03>\"\u001b[0;36m, line \u001b[0;32m35\u001b[0m\n\u001b[0;31m    T1 = np.dot(X, W.T)  # N by K\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def obj_toppush_label(w, X, Y, r=1, weighting=True):\n",
    "    \"\"\"\n",
    "        Objective with top push loss for labels\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "            - r: parameter for log-sum-exp approximation\n",
    "            - weighting: if True, divide N+ in top-push loss\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "    assert(r > 0)\n",
    "    \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    # instead of using diagonal matrix to scale each row of a matrix with a different factor,\n",
    "    # we use Mat * Vec[:, None] which is more memory efficient\n",
    "    \n",
    "    if weighting is True:\n",
    "        NPosAll = np.sum(Y, axis=0)  # number of positive examples for each label, K by 1\n",
    "    else:\n",
    "        NPosAll = np.ones(K)\n",
    "    \n",
    "    A_diag = 1.0 / NPosAll\n",
    "    \n",
    "    for k in range(K):\n",
    "        \n",
    "        \n",
    "    T1 = np.dot(X, W.T)  # N by K\n",
    "    T2 = np.multiply(1-Y, np.exp(r*T1)) \n",
    "    T3 = np.dot(np.ones(N), T2) # K by 1\n",
    "    T4 = np.power(T3, 1.0/r)\n",
    "    T5 = np.multiply(Y, np.exp(-T1)) # N by K\n",
    "    T6 = np.log1p(T5.T * T4[:, None])  # K by N\n",
    "    T7 = np.multiply(np.dot(T6, np.ones(N)) * A_diag)  # K by 1\n",
    "    \n",
    "    J = np.dot(T7, np.ones(K)) / K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_toppush_label_loop(w, X, Y, r=1, weighting=True):\n",
    "    \"\"\"\n",
    "        Objective of top push loss for each label\n",
    "        \n",
    "        Input:\n",
    "            - w: current weight vector, flattened L x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "            - r1: parameter for log-sum-exp approximation\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "    assert(r > 0)\n",
    "    \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    if weighting is True:\n",
    "        NPosAll = np.sum(Y, axis=0)  # number of positive examples for each label, K by 1\n",
    "    else:\n",
    "        NPosAll = np.ones(K)\n",
    "    \n",
    "    for k in range(K):\n",
    "        Jk = 0.0\n",
    "        posInd = np.nonzero(Y[:, k])[0].tolist()\n",
    "        negInd = sorted(set(np.arange(N).tolist()) - set(posInd))\n",
    "        for p in posInd:\n",
    "            t1 = np.sum([np.exp(r * np.dot(W[k, :], X[q, :] - X[p, :])) for q in negInd])\n",
    "            t2 = np.power(t1, 1.0-1.0/r) + t1\n",
    "            vk = np.zeros(D)\n",
    "            for q in negInd:\n",
    "                vk = vk + np.exp(r * np.dot(W[k, :], X[q, :] - X[p, :])) * (X[q, :] - X[p, :])\n",
    "            Jk += np.log1p(np.power(t1, 1.0/r))\n",
    "            G[k, :] = G[k, :] + vk / t2\n",
    "        J += Jk / NPosAll[k]\n",
    "        G[k, :] = G[k, :] / NPosAll[k]\n",
    "        \n",
    "    J = J / K\n",
    "    G = G / K\n",
    "        \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = 0.001 * np.random.randn(Y_train.shape[1] * X_train.shape[1])\n",
    "check_grad(lambda w: obj_toppush_label_loop(w, X_train, Y_train, r=4)[0], \n",
    "           lambda w: obj_toppush_label_loop(w, X_train, Y_train, r=4)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_xentropy(w, X, Y, weighting=True):\n",
    "    \"\"\"\n",
    "    Objective with logistic loss\n",
    "    \n",
    "    Input:\n",
    "            - w: current weight vector, flattened K x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "        \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    if weighting is True:\n",
    "        NK = N * K\n",
    "    else:\n",
    "        NK = N\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    T1 = np.dot(W, X.T)  # K by N\n",
    "    T2 = np.exp(T1)\n",
    "    T3 = np.divide(T2, 1+T2)\n",
    "    T4 = np.log1p(T2)\n",
    "    T5 = np.log1p(np.divide(1.0, T2))\n",
    "    T6 = T4 + np.multiply(Y.T, T5-T4)  # K by N\n",
    "    \n",
    "    J = np.dot(np.ones(K), np.dot(T6, np.ones(N))) / NK\n",
    "    G = np.dot(T3-Y.T, X) / NK\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_xentropy_loop(w, X, Y, weighting=True):\n",
    "    \"\"\"\n",
    "    Objective with logistic loss\n",
    "    \n",
    "    Input:\n",
    "            - w: current weight vector, flattened K x D\n",
    "            - X: feature matrix, N x D\n",
    "            - Y: label matrix,   N x K\n",
    "    \"\"\"    \n",
    "    N, D = X.shape\n",
    "    K = Y.shape[1]\n",
    "    assert(w.shape[0] == K * D)\n",
    "        \n",
    "    W = w.reshape(K, D)  # theta\n",
    "    if weighting is True:\n",
    "        NK = N * K\n",
    "    else:\n",
    "        NK = N\n",
    "    \n",
    "    J = 0.0  # cost\n",
    "    G = np.zeros_like(W)  # gradient matrix\n",
    "    \n",
    "    for k in range(K):\n",
    "        for n in range(N):\n",
    "            t1 = np.exp(np.dot(W[k, :], X[n, :]))\n",
    "            t2 = np.log1p(t1)\n",
    "            J += t2\n",
    "            if Y[n, k] == 1:\n",
    "                J += (np.log1p(1.0 / t1) - t2)\n",
    "            G[k, :] = G[k, :] + X[n, :] * (t1 / (1 + t1) - Y[n, k])\n",
    "                \n",
    "    J = J / NK\n",
    "    G = G / NK\n",
    "    \n",
    "    return (J, G.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_hybrid(w, X, Y, C, C1, r=1, weighting=True):\n",
    "    \"\"\"\n",
    "    Objective with L2 regularisation and top push loss\n",
    "    \"\"\"\n",
    "    \n",
    "    assert C > 0\n",
    "    assert C1 > 0\n",
    "    assert r > 0\n",
    "    \n",
    "    J1, G1 = obj_toppush_example(w, X, Y, r, weighting)\n",
    "    J2, G2 = obj_xentropy(w, X, Y)\n",
    "    \n",
    "    J = np.dot(w, w) * 0.5 / C + J1 + C1 * J2\n",
    "    G = w / C + G1 + C1 * G2\n",
    "    \n",
    "    return (J, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_hybrid_loop(w, X, Y, C, C1, r=1, weighting=True):\n",
    "    assert C > 0\n",
    "    assert C1 > 0\n",
    "    assert r > 0\n",
    "    \n",
    "    J1, G1 = obj_toppush_example_loop(w, X, Y, r, weighting)\n",
    "    J2, G2 = obj_xentropy_loop(w, X, Y)\n",
    "    \n",
    "    J = np.dot(w, w) * 0.5 / C + J1 + C1 * J2\n",
    "    G = w / C + G1 + C1 * G2\n",
    "    \n",
    "    return (J, G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train[:50, :]\n",
    "#Y_train = Y_train[:50, :]\n",
    "#C = 1\n",
    "##w0 = np.random.rand(Y_train.shape[1] * X_train.shape[1]) - 0.5\n",
    "#w0 = 0.001 * np.random.randn(Y_train.shape[1] * X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_grad(lambda w: obj_hybrid(w, X_train, Y_train, C, r=4)[0], \n",
    "#           lambda w: obj_hybrid(w, X_train, Y_train, C, r=4)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_grad(lambda w: obj_hybrid_loop(w, X_train, Y_train, C, r=2)[0], \n",
    "#           lambda w: obj_hybrid_loop(w, X_train, Y_train, C, r=2)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "print('%15s %15s %15s %15s' % ('J_Diff', 'J_loop', 'J_vec', 'G_Diff'))\n",
    "for e in range(-6, 10):\n",
    "    C = 10**(e)\n",
    "    #w0 = init_var(X_train, Y_train)\n",
    "    J,  G  = obj_hybrid_loop(w0, X_train, Y_train, C, r=4)\n",
    "    J1, G1 = obj_hybrid(w0, X_train, Y_train, C, r=4)\n",
    "    Gdiff = G1 - G\n",
    "    #print('%-15g %-15g %-15g' % (J1 - J, J, J1))\n",
    "    print('%15g %15g %15g %15g' % (J1 - J, J, J1, np.dot(Gdiff, Gdiff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLC_hybrid(BaseEstimator):\n",
    "    \"\"\"All methods are necessary for a scikit-learn estimator\"\"\"\n",
    "    \n",
    "    def __init__(self, C=1, C1=1, r=1, weighting=True):\n",
    "        \"\"\"Initialisation\"\"\"\n",
    "        \n",
    "        assert C > 0\n",
    "        assert C1 > 0\n",
    "        assert r > 0\n",
    "        assert type(weighting) == bool\n",
    "        self.C = C\n",
    "        self.C1 = C1\n",
    "        self.r = r\n",
    "        self.weighting = weighting\n",
    "        self.trained = False\n",
    "        \n",
    "    def fit(self, X_train, Y_train):\n",
    "        \"\"\"Model fitting by optimising the objective\"\"\"\n",
    "        opt_method = 'L-BFGS-B' #'BFGS' #'Newton-CG'\n",
    "        options = {'disp': 1, 'maxiter': 10**5, 'maxfun': 10**5} # , 'iprint': 99}\n",
    "        print('\\nC: %g, C1: %g, r: %g, weighting: %s' % (self.C, self.C1, self.r, self.weighting))\n",
    "            \n",
    "        N, D = X_train.shape\n",
    "        K = Y_train.shape[1]\n",
    "        #w0 = np.random.rand(K * D) - 0.5  # initial guess in range [-1, 1]\n",
    "        w0 = 0.001 * np.random.randn(K * D)\n",
    "        opt = minimize(obj_hybrid, w0, args=(X_train, Y_train, self.C, self.C1, self.r, self.weighting), \\\n",
    "                       method=opt_method, jac=True, options=options)\n",
    "        if opt.success is True:\n",
    "            self.W = np.reshape(opt.x, (K, D))\n",
    "            self.trained = True\n",
    "        else:\n",
    "            sys.stderr.write('Optimisation failed')\n",
    "            print(opt.items())\n",
    "            self.trained = False\n",
    "            \n",
    "            \n",
    "    def decision_function(self, X_test):\n",
    "        \"\"\"Make predictions (score is real number)\"\"\"\n",
    "        \n",
    "        assert self.trained is True, \"Can't make prediction before training\"\n",
    "        D = X_test.shape[1]\n",
    "        return np.dot(X_test, self.W.T)\n",
    "        \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return self.decision_function(X_test)\n",
    "    #    \"\"\"Make predictions (score is boolean)\"\"\"   \n",
    "    #    preds = sigmoid(self.decision_function(X_test))\n",
    "    #    #return (preds >= 0)\n",
    "    #    assert self.TH is not None\n",
    "    #    return preds >= self.TH        \n",
    "        \n",
    "    # inherit from BaseEstimator instead of re-implement\n",
    "    #\n",
    "    #def get_params(self, deep = True):\n",
    "    #def set_params(self, **params):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_results(predictor, X_train, Y_train, X_test, Y_test, fname, rankingLoss=False):\n",
    "    \"\"\"\n",
    "        Compute and save performance results\n",
    "    \"\"\"\n",
    "    preds_train = predictor.decision_function(X_train)\n",
    "    preds_test  = predictor.decision_function(X_test)\n",
    "    \n",
    "    print('Training set:')\n",
    "    perf_dict_train = evaluatePrecision(Y_train, preds_train, verbose=1)\n",
    "    print()\n",
    "    print('Test set:')\n",
    "    perf_dict_test = evaluatePrecision(Y_test, preds_test, verbose=1)\n",
    "    \n",
    "    if rankingLoss is True:\n",
    "        print()\n",
    "        print('Training set:')\n",
    "        perf_dict_train.update(evaluateRankingLoss(Y_train, preds_train))\n",
    "        print(label_ranking_loss(Y_train, preds_train))\n",
    "        print()\n",
    "        print('Test set:')\n",
    "        perf_dict_test.update(evaluateRankingLoss(Y_test, preds_test))\n",
    "        print(label_ranking_loss(Y_test, preds_test))\n",
    "\n",
    "    # compute F1 score w.r.t. different thresholds\n",
    "    #TH1 = predictor.cv_results_['mean_test_TH'][clf.best_index_]\n",
    "    #TH2 = np.mean(Y_train, axis=0)\n",
    "    #TH3 = np.mean(TH2)\n",
    "    \n",
    "    #preds_train_bin = sigmoid(preds_train)\n",
    "    #preds_test_bin  = sigmoid(preds_test)\n",
    "    \n",
    "    #F1_train1 = f1_score_nowarn(Y_train, sigmoid(preds_train) >= TH1, average='samples')\n",
    "    #F1_test1  = f1_score_nowarn(Y_test, sigmoid(preds_test) >= TH1, average='samples')\n",
    "    #print('\\nTrain: %.4f, %f' % (F1_train1, f1_score(Y_train, sigmoid(preds_train) >= TH1, average='samples')))\n",
    "    #print('\\nTest : %.4f, %f' % (F1_test1, f1_score(Y_test, sigmoid(preds_test) >= TH1, average='samples')))\n",
    "    \n",
    "    #F1_train2 = f1_score_nowarn(Y_train, (preds_train_bin - TH2) >= 0, average='samples')\n",
    "    #F1_test2  = f1_score_nowarn(Y_test, (preds_test_bin - TH2) >= 0, average='samples')\n",
    "    #print('\\nTrain: %.4f, %f' % (F1_train2, f1_score(Y_train, (preds_train_bin - TH2) >= 0, average='samples')))\n",
    "    #print('\\nTest : %.4f, %f' % (F1_test2, f1_score(Y_test, (preds_test_bin - TH2) >= 0, average='samples')))\n",
    "    \n",
    "    #F1_train3 = f1_score_nowarn(Y_train, preds_train_bin >= TH3, average='samples')\n",
    "    #F1_test3  = f1_score_nowarn(Y_test, preds_test_bin >= TH3, average='samples')\n",
    "    #print('\\nTrain: %.4f, %f' % (F1_train3, f1_score(Y_train, preds_train_bin >= TH3, average='samples')))\n",
    "    #print('\\nTest : %.4f, %f' % (F1_test3, f1_score(Y_test, preds_test_bin >= TH3, average='samples')))\n",
    "    \n",
    "    #perf_dict_train.update({'F1': [(F1_train1,), (F1_train2,), (F1_train3,)]})\n",
    "    #perf_dict_test.update( {'F1': [(F1_test1,),  (F1_test2,),  (F1_test3,)]})\n",
    "    #perf_dict_train.update({'F1': [(F1_train2,), (F1_train3,)]})\n",
    "    #perf_dict_test.update( {'F1': [(F1_test2,),  (F1_test3,)]})\n",
    "    \n",
    "    perf_dict = {'Train': perf_dict_train, 'Test': perf_dict_test}\n",
    "    if os.path.exists(fname):\n",
    "        _dict = pkl.load(open(fname, 'rb'))\n",
    "        if dataset_name not in _dict:\n",
    "            _dict[dataset_name] = perf_dict\n",
    "    else:\n",
    "        _dict = {dataset_name: perf_dict}\n",
    "    pkl.dump(_dict, open(fname, 'wb'))\n",
    "    \n",
    "    print()\n",
    "    print(pkl.load(open(fname, 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_settings = np.seterr(all='ignore')  # seterr to known value\n",
    "np.seterr(all='raise')\n",
    "#np.seterr(all='ignore')\n",
    "#np.seterr(**old_settings)  # restore settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%memit model.fit(X_train[:30], Y_train[:30])\n",
    "#%mprun -f minimize model.fit(X_train[:100], Y_train[:100])\n",
    "#%mprun -f _minimize_slsqp model.fit(X_train[:10], Y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "if os.path.exists(fmodel_base):\n",
    "    clf = pkl.load(open(fmodel_base, 'rb'))\n",
    "else:\n",
    "    clf = clf = MLC_hybrid()\n",
    "    clf.fit(X_train, Y_train)\n",
    "    pkl.dump(clf, open(fmodel_base, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump_results(clf, X_train, Y_train, X_test, Y_test, fperf_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation w.r.t. average precision@K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#mm = MLC_toppush(C=300000, r=4)\n",
    "#mm.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ranges = range(-6, 7)\n",
    "#ranges = range(-6, 5)\n",
    "#parameters = [{'C': sorted([10**(e) for e in ranges] + [3 * 10**(e) for e in ranges]),\n",
    "parameters = [{'C': [1e-3, 3e-3, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1e3],\n",
    "               'C1': [0.1, 0.3, 1, 3, 10],\n",
    "               'r': [8],\n",
    "               'weighting': [True, False],\n",
    "              }]\n",
    "scorer = {'Prec': make_scorer(avgPrecisionK)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(fmodel_prec):\n",
    "    clf = GridSearchCV(MLC_hybrid(), parameters, scoring=scorer, cv=5, n_jobs=1, refit='Prec')\n",
    "    clf.fit(X_train, Y_train)\n",
    "    pkl.dump(clf, open(fmodel_prec, 'wb'))\n",
    "else:\n",
    "    clf = pkl.load(open(fmodel_prec, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLC_hybrid(C=25, C1=3, r=8, weighting=True)\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1_score_nowarn(Y_test, clf.decision_function(X_test) > 0, average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_results(clf, X_train, Y_train, X_test, Y_test, fperf_prec, rankingLoss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
