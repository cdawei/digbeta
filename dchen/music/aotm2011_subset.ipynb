{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A representative subset of AotM-2011 Playlists with MSD Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "import gzip\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "from scipy.sparse import lil_matrix, issparse\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('src')\n",
    "from BinaryRelevance import BinaryRelevance\n",
    "#from PClassificationMLC import PClassificationMLC\n",
    "from PCMLC import PCMLC as PClassificationMLC\n",
    "from evaluate import calc_F1, calc_precisionK, calc_rank, f1_score_nowarn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/aotm-2011'\n",
    "#faotm = os.path.join(data_dir, 'aotm2011-subset.pkl')\n",
    "faotm = os.path.join(data_dir, 'aotm2011-user-playlist.pkl')\n",
    "ffeature = 'data/msd/songID2Features.pkl.gz'\n",
    "fgenre = 'data/msd/song2genre.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load playlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load playlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_playlists = pkl.load(open(faotm, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#user    :', len(user_playlists))\n",
    "print('#playlist:', np.sum([len(user_playlists[u]) for u in user_playlists]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_lengths = [len(pl) for u in user_playlists for pl in user_playlists[u]]\n",
    "#plt.hist(pl_lengths, bins=100)\n",
    "print('Average playlist length: %.1f' % np.mean(pl_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = sorted(user_playlists.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_user = {u: {sid for pl in user_playlists[u] for sid in pl} for u in users}  # user: a set of songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the number of playlists per user, and the number of songs covered by the user's playlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf = pd.DataFrame(index=users, columns=['#playlist', '#song'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf['#playlist'] = [len(user_playlists[u]) for u in users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf['#song'] = [len(songs_user[u]) for u in users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(111)\n",
    "udf['#playlist'].hist(bins=200, ax=ax)\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_npl = sorted([(u, len(user_playlists[u])) for u in users], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#u_npl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1000  # sample 0.1%\n",
    "subset = [u_npl[ix] for ix in np.arange(0, len(u_npl), step)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_subset = [t[0] for t in subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#udf_subset = udf[ udf['#playlist'] == 10]\n",
    "#udf_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uid_subset += udf_subset.index[[3,4]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#udf_subset = udf[ udf['#playlist'] == 30]\n",
    "#udf_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uid_subset += udf_subset.index[[1,2]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#udf_subset = udf[ udf['#playlist'].isin(np.arange(95, 105))]\n",
    "#udf_subset.sort_values(by='#playlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uid_subset += udf_subset.index[[2,5]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#udf.sort_values(by=['#playlist'], ascending=False).iloc[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#uid_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#udf[uid_subset]  # tuple are used as multiindex in pandas\n",
    "#udf[[uid_subset]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user whose playlists cover a *proper number of playlists*, e.g. 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists_subset = [pl for u in uid_subset for pl in user_playlists[u]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(playlists_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_set = sorted({sid for u in uid_subset for sid in songs_user[u]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(song_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split songs for setting I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split songs (90/10 split) such that the distributions of song popularity (the number of occurrence in playlists) in training and dev set are similiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_pl_mat = np.zeros((len(song_set), len(playlists_subset)))\n",
    "songind = {sid: ix for ix, sid in enumerate(song_set)}\n",
    "for j in range(len(playlists_subset)):\n",
    "    pl = playlists_subset[j]\n",
    "    ind = [songind[sid] for sid in pl]\n",
    "    song_pl_mat[ind, j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_pop = np.sum(song_pl_mat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(song_pop, bins=20)\n",
    "#print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.arange(0, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortix = np.argsort(song_pop)\n",
    "ratio = 0.1  # 90/10 split\n",
    "step = int(1./ratio)\n",
    "split_ix = np.arange(0, len(song_pop), step)\n",
    "dev_ix = [sortix[ix] for ix in split_ix]\n",
    "dev_song_set = [song_set[ix] for ix in dev_ix]\n",
    "train_song_set = sorted(set(song_set) - set(dev_song_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of song popularity in training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_song_pop = [song_pop[songind[sid]] for sid in train_song_set]\n",
    "ax = plt.subplot(111)\n",
    "ax.hist(train_song_pop, bins=30)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(0, song_pop.max()+1)\n",
    "print(len(train_song_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of song popularity in dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_song_pop = [song_pop[songind[sid]] for sid in dev_song_set]\n",
    "ax = plt.subplot(111)\n",
    "ax.hist(dev_song_pop, bins=30)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(0, song_pop.max()+1)\n",
    "print(len(dev_song_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split playlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split playlists (80/20 split) such that the distributions of playlist length (the number of songs in playlists) for each user in training and dev set are similiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_playlists = []\n",
    "dev_playlists = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.2\n",
    "step = 1./ratio\n",
    "#np.arange(0, 10, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)\n",
    "rounding_prob = step - int(step)\n",
    "for u in uid_subset:\n",
    "    u_playlists = user_playlists[u]\n",
    "    if len(u_playlists) < 3: \n",
    "        train_playlists.append((u, u_playlists[0]))\n",
    "        continue\n",
    "    sorted_pl = sorted(u_playlists, key=lambda pl: len(pl))\n",
    "    if step == int(step):\n",
    "        step = int(step)\n",
    "        dev_ix = np.arange(0, len(sorted_pl), step)\n",
    "    else:\n",
    "        split_ix = np.arange(0, len(sorted_pl), step)\n",
    "        dev_ix = [ix for ix in [int(x) if np.random.rand() < rounding_prob or int(x) == len(sorted_pl)-1 \\\n",
    "                                else int(x)+1 for x in split_ix]]  # avoid index out of bounds\n",
    "    dev_playlists += [(u, sorted_pl[ix]) for ix in dev_ix]\n",
    "    train_playlists += [(u, sorted_pl[ix]) for ix in range(len(sorted_pl)) if ix not in dev_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmax = np.max([len(pl) for pl in playlists_subset]) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of playlist length in training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(111)\n",
    "ax.hist([len(t[1]) for t in train_playlists], bins=50)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(0, xmax)\n",
    "print(len(train_playlists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of playlist length in training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(111)\n",
    "ax.hist([len(t[1]) for t in dev_playlists], bins=50)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(0, xmax)\n",
    "print(len(dev_playlists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold 10% of songs in the dev set of playlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold 10% of songs uniformly at random for each playlist in dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_known_songs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.1\n",
    "np.random.seed(987654321)\n",
    "num_held = 0\n",
    "for u, pl in dev_playlists:\n",
    "    sample_size = ratio * len(pl)\n",
    "    rounding_prob = sample_size - int(sample_size)\n",
    "    sample_size = int(sample_size) if np.random.rand() < rounding_prob else int(sample_size) + 1\n",
    "    sample_ix = np.random.permutation(np.arange(len(pl)))[sample_size:]\n",
    "    dev_known_songs += np.array(pl)[sample_ix].tolist()\n",
    "    num_held += sample_size\n",
    "print('#song being held:', num_held)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_known_songs = sorted(set(dev_known_songs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dev_known_songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load song features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `song_id` --> `feature array` mapping: map a song to the audio features of one of its corresponding tracks in MSD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2Features = pkl.load(gzip.open(ffeature, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2genre = pkl.load(open(fgenre, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all songs have genre info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all([sid in song2genre for sid in song_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create song-playlist matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Songs as rows, playlists as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset_subset(playlists, song_set, features_MSD, song2genre):\n",
    "    \"\"\"\n",
    "    Create labelled dataset: rows are songs, columns are users.\n",
    "    \n",
    "    Input:\n",
    "        - playlists: a set of playlists\n",
    "        - song_set: a set of songIDs\n",
    "        - features_MSD: dictionary that maps songIDs to features from MSD\n",
    "        - song2genre: dictionary that maps songIDs to genre\n",
    "    Output:\n",
    "        - (Feature, Label) pair (X, Y)\n",
    "          X: #songs by #features\n",
    "          Y: #songs by #users\n",
    "    \"\"\" \n",
    "    song_indices = {sid: ix for ix, sid in enumerate(song_set)}\n",
    "    N = len(song_set)\n",
    "    K = len(playlists)\n",
    "    \n",
    "    genre_set = sorted({v for v in song2genre.values()})\n",
    "    genre_indices = {genre: ix for ix, genre in enumerate(genre_set)}\n",
    "    \n",
    "    def onehot_genre(songID):\n",
    "        \"\"\"\n",
    "        One-hot encoding of genres.\n",
    "        Data imputation: one extra entry for songs without genre info.\n",
    "        Should try other options: \n",
    "            mean imputation, sampling from the distribution of genre popularity.\n",
    "        \"\"\"\n",
    "        num = len(genre_set) + 1\n",
    "        vec = np.zeros(num, dtype=np.float)\n",
    "        if songID in song2genre:\n",
    "            genre_ix = genre_indices[song2genre[songID]]\n",
    "            vec[genre_ix] = 1\n",
    "        else:\n",
    "            vec[-1] = 1\n",
    "        return vec\n",
    "    \n",
    "    #X = np.array([features_MSD[sid] for sid in song_set])  # without using genre\n",
    "    X = np.array([np.concatenate([features_MSD[sid], onehot_genre(sid)], axis=-1) for sid in song_set])\n",
    "    Y = np.zeros((N, K), dtype=np.bool)\n",
    "    \n",
    "    for k in range(K):\n",
    "        pl = playlists[k]\n",
    "        indices = [song_indices[sid] for sid in pl if sid in song_indices]\n",
    "        Y[indices, k] = True\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_normalised_reciprocal_rank(Y_true, Y_pred):\n",
    "    \"\"\"\n",
    "    Compute the mean of normalised reciprocal rank (reciprocal rank are normalised by the best possible ranks)\n",
    "    \"\"\"\n",
    "    normalised_reci_rank = []\n",
    "    npos = np.sum(Y_true, axis=0)\n",
    "    for k in range(Y_true.shape[1]):\n",
    "        ranks = calc_rank(Y_pred[:, k])[Y_true[:, k]]\n",
    "        if len(ranks) > 0:\n",
    "            ideal = np.sum([1./nk for nk in range(1, npos[k]+1)])\n",
    "            real = np.sum([1./r for r in ranks])\n",
    "            normalised_reci_rank.append(real / ideal)  # normalise the reciprocal ranks by the best possible ranks\n",
    "    return np.mean(normalised_reci_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_pl(Y_true, Y_pred):\n",
    "    nzcol = np.nonzero(np.sum(Y_true, axis=0))[0]  # columns with at least one True\n",
    "    print('Average over %d columns' % len(nzcol))\n",
    "    print('%-15s %.4f' % ('Mean P@K:', np.mean(calc_precisionK(Y_true.T, Y_pred.T))))\n",
    "    print('%-15s %.4f' % ('Mean AUC:', roc_auc_score(Y_true[:, nzcol], Y_pred[:, nzcol], average='macro')))\n",
    "    print('%-15s %.4f' % ('MAP:', average_precision_score(Y_true[:, nzcol], Y_pred[:, nzcol], average='macro')))\n",
    "    print('%-15s %.4f' % ('Mean NRR:', mean_normalised_reciprocal_rank(Y_true, Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting I: hold a subset of songs, use all playlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists1 = [t[1] for t in train_playlists + dev_playlists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = gen_dataset_subset(playlists=playlists1, song_set=train_song_set, \n",
    "                                      features_MSD=song2Features, song2genre=song2genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev, Y_dev = gen_dataset_subset(playlists=playlists1, song_set=dev_song_set, \n",
    "                                  features_MSD=song2Features, song2genre=song2genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mean = np.mean(X_train, axis=0).reshape((1, -1))\n",
    "X_train_std = np.std(X_train, axis=0).reshape((1, -1)) + 10 ** (-6)\n",
    "X_train -= X_train_mean\n",
    "X_train /= X_train_std\n",
    "X_dev   -= X_train_mean\n",
    "X_dev   /= X_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train: %15s %15s' % (X_train.shape, Y_train.shape))\n",
    "print('Dev  : %15s %15s' % (X_dev.shape,   Y_dev.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.mean(X_train, axis=0)))\n",
    "print(np.mean( np.std(X_train, axis=0)) - 1)\n",
    "print(np.mean(np.mean(X_dev, axis=0)))\n",
    "print(np.mean( np.std(X_dev, axis=0)) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sum(Y_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sum(Y_dev, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M1. BR - Independent logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "br = BinaryRelevance(C=1, n_jobs=4)\n",
    "br.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: normalise **per playlist**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dev set:')\n",
    "eval_pl(Y_dev, br.predict(X_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set:')\n",
    "eval_pl(Y_train, br.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M2. PC - Multilabel p-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P-Classification ~ P-norm push ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = PClassificationMLC(C1=1, weighting='labels')\n",
    "pc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: normalise **per playlist**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dev set:')\n",
    "eval_pl(Y_dev, pc.predict(X_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set:')\n",
    "eval_pl(Y_train, pc.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting II: hold a subset of songs in a subset of playlists, use all songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = gen_dataset_subset(playlists=[t[1] for t in train_playlists + dev_playlists], song_set=song_set, \n",
    "                          features_MSD=song2Features, song2genre=song2genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set all entries corresponding to playlists in dev set to NaN, except those songs in dev playlists that we observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = Y.copy().astype(np.float)  # note: np.nan is float\n",
    "cols = np.arange(Y.shape[1])[-len(dev_playlists):]\n",
    "Y_train[:, cols] = np.nan\n",
    "song_indices = {sid: ix for ix, sid in enumerate(song_set)}\n",
    "assert len(cols) == len(dev_playlists)\n",
    "num_known = 0\n",
    "for j in range(len(cols)):\n",
    "    pl = dev_playlists[j][1]\n",
    "    rows = [song_indices[sid] for sid in (set(pl) & set(dev_known_songs))]\n",
    "    Y_train[rows, cols[j]] = 1\n",
    "    num_known += len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.isnan(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dev_playlists) * len(song_set) - num_known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dev_song_set) * Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dev_song_set)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(np.isnan(Y_train)), len(dev_playlists) * len(dev_song_set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mean = np.mean(X_train, axis=0).reshape((1, -1))\n",
    "X_train_std = np.std(X_train, axis=0).reshape((1, -1)) + 10 ** (-6)\n",
    "X_train -= X_train_mean\n",
    "X_train /= X_train_std\n",
    "X_dev = X_train[len(nondev_song_set2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train: %15s %15s' % (X_train.shape, Y_train.shape))\n",
    "print('Dev  : %15s %15s' % (X_dev.shape,   Y_dev.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.mean(X_train, axis=0)))\n",
    "print(np.mean( np.std(X_train, axis=0)) - 1)\n",
    "print(np.mean(np.mean(X_dev, axis=0)))\n",
    "print(np.mean( np.std(X_dev, axis=0)) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sum(Y_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sum(Y_dev, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M3. Independent logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "br2 = BinaryRelevance(C=1, n_jobs=4)\n",
    "br2.fit(X_train, np.nan_to_num(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_start = -len(dev_playlists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: normalise **per playlist**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dev set:')\n",
    "eval_pl(Y_dev, br2.predict(X_dev)[:, col_start:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_gt = np.nan_to_num(Y_train).astype(np.bool)\n",
    "Y_train_pred = br2.predict(X_train)\n",
    "Y_train_pred[:, col_start:] = 0   # remove test region\n",
    "print('Training set:')\n",
    "eval_pl(Y_train_gt, Y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M4. Multilabel p-classification with some playlist fully observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_of_playlists2 = [t[0] for t in train_playlists + dev_playlists]\n",
    "#user_of_playlists2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_user_mat = np.zeros((len(playlists2), len(playlists2)), dtype=np.bool)\n",
    "for i in range(len(playlists2)):\n",
    "    for j in range(i+1, len(playlists2)):\n",
    "        if user_of_playlists2[i] == user_of_playlists2[j]:\n",
    "            same_user_mat[i, j] = True\n",
    "            same_user_mat[j, i] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same_user_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pla = PClassificationMLC(C1=1, C2=1, C3=10, weighting='both', similarMat=same_user_mat)\n",
    "pla.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_start = -len(dev_playlists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: normalise **per playlist**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pl(Y_dev, pla.predict(X_dev)[:, col_start:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_gt = np.nan_to_num(Y_train).astype(np.bool)\n",
    "Y_train_pred = pla.predict(X_train)\n",
    "Y_train_pred[:, col_start:] = 0   # remove test region\n",
    "eval_pl(Y_train_gt, Y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the if the regulariser is effective**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "rows, cols = np.nonzero(same_user_mat)\n",
    "for row, col in zip(rows, cols):\n",
    "    diff = pla.W[row] - pla.W[col]\n",
    "    print('%g' % np.sqrt(np.dot(pla.W[row], pla.W[row])))\n",
    "    print('%g' % np.sqrt(np.dot(pla.W[col], pla.W[col])))\n",
    "    print('%g' % np.sqrt(np.dot(diff, diff)))\n",
    "    print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute matrix $M$ such that $M_{jk} = \\sqrt{(w_j - w_k)^\\top (w_j - w_k)}, \\forall j, k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.dot(pla.W, pla.W.T)\n",
    "B = np.tile(np.diag(A), (A.shape[0], 1))\n",
    "M = np.sqrt(-2 * A + (B + B.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise $M$ by the vector with maximum norm in $W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aa = np.arange(6).reshape(3, 2)\n",
    "#np.einsum('ij,ij->i', aa, aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denorm = np.sqrt(np.einsum('ij,ij->i', pla.W, pla.W))  # compute the norm for each row in W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1 = M / np.max(denorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.matshow(M1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#user_of_playlists2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = np.nonzero(same_user_mat)\n",
    "M2 = M1[rows, cols]\n",
    "print(np.min(M2), np.max(M2), np.mean(M2), np.std(M2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = same_user_mat.copy()\n",
    "np.fill_diagonal(mat, 1)   # remove the diagnoal from consideration\n",
    "rows, cols = np.where(mat == 0)\n",
    "M3 = M1[rows, cols]\n",
    "print(np.min(M3), np.max(M3), np.mean(M3), np.std(M3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check performance per user**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_set = sorted(set(user_of_playlists2))\n",
    "#user_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_of_playlists2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pla = pla.predict(X_dev)[:, col_start:]\n",
    "dev_col_start = len(train_playlists)\n",
    "for u in user_set:\n",
    "    uind = np.where(np.array(user_of_playlists2, dtype=np.object) == u)[0]\n",
    "    ntrain = len(uind)\n",
    "    if len(uind) < 2: continue  # filtering out users with less than 2 playlists\n",
    "    uind -= dev_col_start\n",
    "    uind = uind[uind >= 0]\n",
    "    ntest = len(uind)\n",
    "    #print(uind)\n",
    "    if len(uind) < 1: continue\n",
    "    print('--------------------')\n",
    "    print('USER:', u)\n",
    "    print('#train: %d, #test: %d' % (ntrain, ntest))\n",
    "    eval_pl(Y_dev[:, uind], Y_pla[:, uind])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
