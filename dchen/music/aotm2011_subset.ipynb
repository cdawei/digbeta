{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A representative subset of AotM-2011 Playlists with MSD Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "import gzip\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "from scipy.optimize import check_grad\n",
    "from scipy.sparse import lil_matrix, issparse\n",
    "from collections import Counter\n",
    "import itertools as itt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('src')\n",
    "from BinaryRelevance import BinaryRelevance\n",
    "#from PClassificationMLC import PClassificationMLC\n",
    "from PCMLC import PCMLC as PClassificationMLC\n",
    "from PCMLC import obj_pclassification\n",
    "from evaluate import calc_F1, calc_precisionK, calc_rank, f1_score_nowarn, evalPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/aotm-2011'\n",
    "#faotm = os.path.join(data_dir, 'aotm2011-subset.pkl')\n",
    "faotm = os.path.join(data_dir, 'aotm2011-user-playlist.pkl')\n",
    "#ffeature = 'data/msd/songID2Features.pkl.gz'\n",
    "ffeature = 'data/msd/song2feature.pkl.gz'\n",
    "fgenre = 'data/msd/song2genre.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load playlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load playlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_playlists = pkl.load(open(faotm, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#user    :', len(user_playlists))\n",
    "print('#playlist:', np.sum([len(user_playlists[u]) for u in user_playlists]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_lengths = [len(pl) for u in user_playlists for pl in user_playlists[u]]\n",
    "#plt.hist(pl_lengths, bins=100)\n",
    "print('Average playlist length: %.1f' % np.mean(pl_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = sorted(user_playlists.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_user = {u: {sid for pl in user_playlists[u] for sid in pl} for u in users}  # user: a set of songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the number of playlists per user, and the number of songs covered by the user's playlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf = pd.DataFrame(index=users, columns=['#playlist', '#song'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf['#playlist'] = [len(user_playlists[u]) for u in users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf['#song'] = [len(songs_user[u]) for u in users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(111)\n",
    "udf['#playlist'].hist(bins=200, ax=ax)\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load song features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `song_id` --> `feature array` mapping: map a song to the audio features of one of its corresponding tracks in MSD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2feature = pkl.load(gzip.open(ffeature, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#song_set_all = sorted({sid for u in user_playlists for pl in user_playlists[u] for sid in pl})\n",
    "#len(song_set_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#song_ages = np.array([int(song2Features[sid][-1]) for sid in song_set_all])\n",
    "#missing_ix = np.where(song_ages == 2018)[0]\n",
    "#print('%d songs missing age info' % len(missing_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_age = (np.sum(song_ages) - np.sum(song_ages[missing_ix])) / (len(song_ages) - len(missing_ix))\n",
    "#mean_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user whose playlists cover a *proper number of playlists*, e.g. 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_npl = sorted([(u, len(user_playlists[u])) for u in users], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#u_npl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1000  # sample 0.1%\n",
    "subset = [u_npl[ix] for ix in np.arange(0, len(u_npl), step)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_subset = [t[0] for t in subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#udf[uid_subset]  # tuple are used as multiindex in pandas\n",
    "#udf[[uid_subset]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists_subset = [pl for u in uid_subset for pl in user_playlists[u]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(playlists_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_set = sorted({(sid, song2feature[sid][-1]) for u in uid_subset for sid in songs_user[u]}, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(song_set))\n",
    "#song_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split songs for setting I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split songs (80/20 split) the latest released (year) songs are in dev set ~~such that the distributions of song popularity (the number of occurrence in playlists) in training and dev set are similiar~~."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_nsongs = int(len(song_set) * 0.2)\n",
    "dev_song_set = song_set[:dev_nsongs]\n",
    "train_song_set = song_set[dev_nsongs:]\n",
    "print('#songs in training set:', len(train_song_set))\n",
    "print('#songs in test set    :', len(dev_song_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev_song_set\n",
    "#train_song_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_pl_mat = np.zeros((len(song_set), len(playlists_subset)))\n",
    "songind = {sid: ix for ix, (sid, _) in enumerate(song_set)}\n",
    "for j in range(len(playlists_subset)):\n",
    "    pl = playlists_subset[j]\n",
    "    ind = [songind[sid] for sid in pl]\n",
    "    song_pl_mat[ind, j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_pop = np.sum(song_pl_mat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(song_pop, bins=20)\n",
    "#print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sortix = np.argsort(song_pop)\n",
    "#ratio = 0.2  # 80/20 split\n",
    "#step = int(1./ratio)\n",
    "#split_ix = np.arange(0, len(song_pop), step)\n",
    "#dev_ix = [sortix[ix] for ix in split_ix]\n",
    "#dev_song_set = [song_set[ix] for ix in dev_ix]\n",
    "#train_song_set = sorted(set(song_set) - set(dev_song_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of song popularity in training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_song_pop = [song_pop[songind[sid]] for (sid, _) in train_song_set]\n",
    "ax = plt.subplot(111)\n",
    "ax.hist(train_song_pop, bins=30)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(0, song_pop.max()+1)\n",
    "print(len(train_song_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of song popularity in dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_song_pop = [song_pop[songind[sid]] for (sid, _) in dev_song_set]\n",
    "ax = plt.subplot(111)\n",
    "ax.hist(dev_song_pop, bins=30)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(0, song_pop.max()+1)\n",
    "print(len(dev_song_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split playlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split playlists (80/20 split) uniformly at random ~~such that the distributions of playlist length (the number of songs in playlists) for each user in training and dev set are similiar~~."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_playlists = []\n",
    "dev_playlists = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.2\n",
    "np.random.seed(123456789)\n",
    "for u in uid_subset:\n",
    "    for pl in user_playlists[u]:\n",
    "        if np.random.rand() < 0.2:\n",
    "            dev_playlists.append((pl, u))\n",
    "        else:\n",
    "            train_playlists.append((pl, u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_playlists), len(dev_playlists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratio = 0.2\n",
    "#step = 1./ratio\n",
    "#np.arange(0, 10, step)\n",
    "#np.random.seed(123456789)\n",
    "#rounding_prob = step - int(step)\n",
    "#for u in uid_subset:\n",
    "#    u_playlists = user_playlists[u]\n",
    "#    if len(u_playlists) < 3: \n",
    "#        train_playlists.append((u, u_playlists[0]))\n",
    "#        continue\n",
    "#    sorted_pl = sorted(u_playlists, key=lambda pl: len(pl))\n",
    "#    if step == int(step):\n",
    "#        step = int(step)\n",
    "#        dev_ix = np.arange(0, len(sorted_pl), step)\n",
    "#    else:\n",
    "#        split_ix = np.arange(0, len(sorted_pl), step)\n",
    "#        dev_ix = [ix for ix in [int(x) if np.random.rand() < rounding_prob or int(x) == len(sorted_pl)-1 \\\n",
    "#                                else int(x)+1 for x in split_ix]]  # avoid index out of bounds\n",
    "#    dev_playlists += [(u, sorted_pl[ix]) for ix in dev_ix]\n",
    "#    train_playlists += [(u, sorted_pl[ix]) for ix in range(len(sorted_pl)) if ix not in dev_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmax = np.max([len(pl) for pl in playlists_subset]) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of playlist length in training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(111)\n",
    "ax.hist([len(t[0]) for t in train_playlists], bins=50)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(0, xmax)\n",
    "print(len(train_playlists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of playlist length in training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(111)\n",
    "ax.hist([len(t[0]) for t in dev_playlists], bins=50)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(0, xmax)\n",
    "print(len(dev_playlists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold part of songs in the dev set of playlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold the last half of songs for each playlist in dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_playlists_obs = [pl[:-int(len(pl)/2)] for (pl, _) in dev_playlists]\n",
    "dev_playlists_held = [pl[-int(len(pl)/2):] for (pl, _) in dev_playlists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dev_playlists)):\n",
    "    assert np.all(dev_playlists[i][0] == dev_playlists_obs[i] + dev_playlists_held[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('obs: %d, held: %d' % (np.sum([len(ppl) for ppl in dev_playlists_obs]), \n",
    "                             np.sum([len(ppl) for ppl in dev_playlists_held])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratio = 0.1\n",
    "#np.random.seed(987654321)\n",
    "#num_held = 0\n",
    "#for u, pl in dev_playlists:\n",
    "#    sample_size = ratio * len(pl)\n",
    "#    rounding_prob = sample_size - int(sample_size)\n",
    "#    sample_size = int(sample_size) if np.random.rand() < rounding_prob else int(sample_size) + 1\n",
    "#    sample_ix = np.random.permutation(np.arange(len(pl)))[sample_size:]\n",
    "#    dev_known_songs += np.array(pl)[sample_ix].tolist()\n",
    "#    num_held += sample_size\n",
    "#print('#song being held:', num_held)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2genre = pkl.load(open(fgenre, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all songs have genre info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all([sid in song2genre for sid in song_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create song-playlist matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Songs as rows, playlists as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset_subset(playlists, song_set, song2feature, song2genre):\n",
    "    \"\"\"\n",
    "    Create labelled dataset: rows are songs, columns are users.\n",
    "    \n",
    "    Input:\n",
    "        - playlists: a set of playlists\n",
    "        - song_set: a set of songIDs\n",
    "        - song2feature: dictionary that maps songIDs to features from MSD\n",
    "        - song2genre: dictionary that maps songIDs to genre\n",
    "    Output:\n",
    "        - (Feature, Label) pair (X, Y)\n",
    "          X: #songs by #features\n",
    "          Y: #songs by #users\n",
    "    \"\"\" \n",
    "    song_indices = {sid: ix for ix, sid in enumerate(song_set)}\n",
    "    N = len(song_set)\n",
    "    K = len(playlists)\n",
    "    \n",
    "    genre_set = sorted({v for v in song2genre.values()})\n",
    "    genre_indices = {genre: ix for ix, genre in enumerate(genre_set)}\n",
    "    \n",
    "    def onehot_genre(songID):\n",
    "        \"\"\"\n",
    "        One-hot encoding of genres.\n",
    "        Data imputation: \n",
    "            - one extra entry for songs without genre info\n",
    "            - mean imputation\n",
    "            - sampling from the distribution of genre popularity\n",
    "        \"\"\"\n",
    "        num = len(genre_set) # + 1\n",
    "        vec = np.zeros(num, dtype=np.float)\n",
    "        if songID in song2genre:\n",
    "            genre_ix = genre_indices[song2genre[songID]]\n",
    "            vec[genre_ix] = 1\n",
    "        else:\n",
    "            vec[:] = np.nan\n",
    "            #vec[-1] = 1\n",
    "        return vec\n",
    "    \n",
    "    #X = np.array([features_MSD[sid] for sid in song_set])  # without using genre\n",
    "    X = np.array([np.concatenate([song2feature[sid], onehot_genre(sid)], axis=-1) for sid in song_set])\n",
    "    Y = np.zeros((N, K), dtype=np.bool)\n",
    "    \n",
    "    for k in range(K):\n",
    "        pl = playlists[k]\n",
    "        indices = [song_indices[sid] for sid in pl if sid in song_indices]\n",
    "        Y[indices, k] = True\n",
    "        \n",
    "    # genre imputation\n",
    "    genre_ix_start = -len(genre_set)\n",
    "    genre_nan = np.isnan(X[:, genre_ix_start:])\n",
    "    genre_mean = np.nansum(X[:, genre_ix_start:], axis=0) / (X.shape[0] - np.sum(genre_nan, axis=0))\n",
    "    #print(np.nansum(X[:, genre_ix_start:], axis=0))\n",
    "    #print(genre_set)\n",
    "    #print(genre_mean)\n",
    "    for j in range(len(genre_set)):\n",
    "        X[genre_nan[:,j], j+genre_ix_start] = genre_mean[j]\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_normalised_reciprocal_rank(Y_true, Y_pred):\n",
    "    \"\"\"\n",
    "    Compute the mean of normalised reciprocal rank (reciprocal rank are normalised by the best possible ranks)\n",
    "    \"\"\"\n",
    "    normalised_reci_rank = []\n",
    "    npos = np.sum(Y_true, axis=0)\n",
    "    for k in range(Y_true.shape[1]):\n",
    "        ranks = calc_rank(Y_pred[:, k])[Y_true[:, k]]\n",
    "        if len(ranks) > 0:\n",
    "            ideal = np.sum([1./nk for nk in range(1, npos[k]+1)])\n",
    "            real = np.sum([1./r for r in ranks])\n",
    "            normalised_reci_rank.append(real / ideal)  # normalise the reciprocal ranks by the best possible ranks\n",
    "    return np.mean(normalised_reci_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_pl(Y_true, Y_pred, top=100, useLoop=False):\n",
    "    if useLoop is False:\n",
    "        assert Y_true.shape == Y_pred.shape\n",
    "        assert top <= Y_true.shape[0]\n",
    "        nzcol = np.nonzero(np.sum(Y_true, axis=0))[0]  # columns with at least one True\n",
    "        ncols = len(nzcol)\n",
    "        topix = np.argsort(-Y_pred, axis=0)[:top, :]\n",
    "        npos = np.sum(Y_true, axis=0)\n",
    "        hr = np.mean([np.sum(Y_true[topix[:, j], j]) / npos[j] for j in nzcol])\n",
    "        paks, valid_indices = calc_precisionK(Y_true.T, Y_pred.T)\n",
    "        pak = np.mean(paks[valid_indices])\n",
    "        auc = roc_auc_score(Y_true[:, nzcol], Y_pred[:, nzcol], average='macro')\n",
    "        ap  = average_precision_score(Y_true[:, nzcol], Y_pred[:, nzcol], average='macro')\n",
    "        nrr = mean_normalised_reciprocal_rank(Y_true, Y_pred)\n",
    "    else:\n",
    "        assert type(Y_true) == list\n",
    "        assert type(Y_pred) == list\n",
    "        assert len(Y_true) == len(Y_pred)\n",
    "        hitrates, paks, aucs, aps, nrrs = [], [], [], [], []\n",
    "        for j in range(len(Y_true)):\n",
    "            if np.sum(Y_true[j]) < 1: continue   # filtering out cases where all ground truths are negative.\n",
    "            gt = Y_true[j].reshape(-1)\n",
    "            pred = Y_pred[j].reshape(-1)\n",
    "            assert gt.shape == pred.shape\n",
    "            assert top <= gt.shape[0]\n",
    "            topix = np.argsort(-pred)[:top]\n",
    "            hitrates.append(np.sum(gt[topix]) / np.sum(gt))\n",
    "            #paks.append(calc_precisionK(gt.reshape(1,-1), pred.reshape(1,-1)))  # incorrect\n",
    "            paks.append(evalPred(gt, pred, metricType='Precision@K'))\n",
    "            aucs.append(roc_auc_score(gt, pred))\n",
    "            aps.append(average_precision_score(gt, pred))\n",
    "            nrrs.append(mean_normalised_reciprocal_rank(gt.reshape(-1,1), pred.reshape(-1,1)))\n",
    "        hr, pak, auc, ap, nrr = [np.mean(x) for x in [hitrates, paks, aucs, aps, nrrs]]\n",
    "        ncols = len(paks)\n",
    "      \n",
    "    print('Average over %d columns' % ncols)\n",
    "    print('%-20s %.4f' % ('Mean HitRate@100:', hr))\n",
    "    print('%-20s %.4f' % ('Mean P@K:', pak))\n",
    "    print('%-20s %.4f' % ('Mean AUC:', auc))\n",
    "    print('%-20s %.4f' % ('MAP:', ap))\n",
    "    print('%-20s %.4f' % ('Mean NRR:', nrr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the adjacent matrix of playlists (nodes), playlists of the same user form a *clique*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_of_playlists2 = [t[1] for t in train_playlists + dev_playlists]\n",
    "#user_of_playlists2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npl = len(user_of_playlists2)\n",
    "same_user_mat = np.zeros((npl, npl), dtype=np.bool)\n",
    "for u in set(user_of_playlists2):\n",
    "    clique = np.where(u == np.array(user_of_playlists2, dtype=np.object))[0]\n",
    "    for x, y in itt.combinations(clique, 2):\n",
    "        same_user_mat[x, y] = True\n",
    "        same_user_mat[y, x] = True\n",
    "#same_user_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting I: hold a subset of songs, use all playlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = gen_dataset_subset(playlists=[t[0] for t in train_playlists + dev_playlists], \n",
    "                                      song_set=[t[0] for t in train_song_set], \n",
    "                                      song2feature=song2feature, song2genre=song2genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev, Y_dev = gen_dataset_subset(playlists=[t[0] for t in train_playlists + dev_playlists], \n",
    "                                  song_set=[t[0] for t in dev_song_set], \n",
    "                                  song2feature=song2feature, song2genre=song2genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mean = np.mean(X_train, axis=0).reshape((1, -1))\n",
    "X_train_std = np.std(X_train, axis=0).reshape((1, -1)) + 10 ** (-6)\n",
    "X_train -= X_train_mean\n",
    "X_train /= X_train_std\n",
    "X_dev   -= X_train_mean\n",
    "X_dev   /= X_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train: %15s %15s' % (X_train.shape, Y_train.shape))\n",
    "print('Dev  : %15s %15s' % (X_dev.shape,   Y_dev.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.mean(X_train, axis=0)))\n",
    "print(np.mean( np.std(X_train, axis=0)) - 1)\n",
    "print(np.mean(np.mean(X_dev, axis=0)))\n",
    "print(np.mean( np.std(X_dev, axis=0)) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "w0 = 0.001 * np.random.randn(Y_dev.shape[1] * X_dev.shape[1] + 1)\n",
    "check_grad(lambda w: obj_pclassification(w, X_dev, Y_dev, p=1, C1=2, C2=3, C3=5,\n",
    "                                         weighting='samples', similarMat=same_user_mat)[0], \n",
    "           lambda w: obj_pclassification(w, X_dev, Y_dev, p=1, C1=2, C2=3, C3=5,\n",
    "                                         weighting='samples', similarMat=same_user_mat)[1], w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sum(Y_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sum(Y_dev, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M1. BR - Independent logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "br = BinaryRelevance(C=1, n_jobs=4)\n",
    "br.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: normalise **per playlist**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dev set:')\n",
    "eval_pl(Y_dev, br.predict(X_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set:')\n",
    "eval_pl(Y_train, br.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M2. PC - Multilabel p-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P-Classification ~ P-norm push ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = PClassificationMLC(C1=1, C2=1, C3=30, weighting='both', similarMat=same_user_mat)\n",
    "pc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "pkl_dir = 'data/aotm-2011/setting1'\n",
    "pkl.dump(X_train, gzip.open(pkl_dir + '/X_train.pkl.gz', 'wb'))\n",
    "pkl.dump(csr_matrix(Y_train), gzip.open(pkl_dir + '/Y_train.pkl.gz', 'wb'))\n",
    "pkl.dump(same_user_mat, gzip.open(pkl_dir + '/adjacency_mat.pkl.gz', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: normalise **per playlist**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = pkl.load(open(pkl_dir + '/mlr-aotm2011-C-1-1-1-p-1.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_pl(Y_dev, clf.predict(X_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dev set:')\n",
    "eval_pl(Y_dev, pc.predict(X_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dev set:')\n",
    "gts = [Y_dev[:,k] for k in range(Y_dev.shape[1])]\n",
    "Y_pred = pc.predict(X_dev)\n",
    "preds = [Y_pred[:,k] for k in range(Y_pred.shape[1])]\n",
    "eval_pl(gts, preds, top=100, useLoop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set:')\n",
    "eval_pl(Y_train, pc.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance per user**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "user_of_playlists2 = [t[1] for t in train_playlists + dev_playlists]\n",
    "user_set = sorted(set(user_of_playlists2))\n",
    "Y_pc_train = pc.predict(X_train)\n",
    "Y_pc_dev = pc.predict(X_dev)\n",
    "for u in user_set:\n",
    "    uind = np.where(np.array(user_of_playlists2, dtype=np.object) == u)[0]\n",
    "    if len(uind) < 2: continue\n",
    "    print('--------------------')\n",
    "    print('USER:', u)\n",
    "    print('#playlist: %d' % len(uind))\n",
    "    print('Dev set:')\n",
    "    eval_pl(Y_dev[:, uind], Y_pc_dev[:, uind])\n",
    "    print('\\nTraining set:')\n",
    "    eval_pl(Y_train[:, uind], Y_pc_train[:, uind])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting II: hold a subset of songs in a subset of playlists, use all songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = gen_dataset_subset(playlists=[t[0] for t in train_playlists + dev_playlists], \n",
    "                          song_set=[t[0] for t in train_song_set + dev_song_set], \n",
    "                          song2feature=song2feature, song2genre=song2genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_cols = np.arange(len(train_playlists), Y.shape[1])\n",
    "col_split = len(train_playlists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set all entries corresponding to playlists in dev set to NaN, except those songs in dev playlists that we observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = Y.copy().astype(np.float)  # note: np.nan is float\n",
    "Y_train[:, dev_cols] = np.nan\n",
    "song_indices = {sid: ix for ix, (sid, _) in enumerate(song_set)}\n",
    "assert len(dev_cols) == len(dev_playlists) == len(dev_playlists_obs)\n",
    "num_known = 0\n",
    "for j in range(len(dev_cols)):\n",
    "    rows = [song_indices[sid] for sid in dev_playlists_obs[j]]\n",
    "    Y_train[rows, dev_cols[j]] = 1\n",
    "    num_known += len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isnanmat = np.isnan(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#unknown: {:,} {:,}'.format(np.sum(isnanmat), len(dev_playlists) * len(song_set) - num_known))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#unknown in setting I: {:,}'.format(len(dev_song_set) * Y.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(isnanmat[:, :col_split]))  # number of NaN in training playlists, should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mean = np.mean(X_train, axis=0).reshape((1, -1))\n",
    "X_train_std = np.std(X_train, axis=0).reshape((1, -1)) + 10 ** (-6)\n",
    "X_train -= X_train_mean\n",
    "X_train /= X_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train: %15s %15s' % (X_train.shape, Y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.mean(X_train, axis=0)))\n",
    "print(np.mean( np.std(X_train, axis=0)) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sum(Y_train, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M3. Independent logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "br2 = BinaryRelevance(C=1, n_jobs=4)\n",
    "br2.fit(X_train, np.nan_to_num(Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: normalise **per playlist**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gts = [Y[isnanmat[:, col], col] for col in dev_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dev set:')\n",
    "Y_br2 = br2.predict(X_train)\n",
    "preds = [Y_br2[isnanmat[:, col], col] for col in dev_cols]\n",
    "eval_pl(gts, preds, useLoop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set:')\n",
    "col_split = len(train_playlists)\n",
    "eval_pl(Y_train[:, :col_split].astype(np.bool), br2.predict(X_train)[:, :col_split])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M4. Multilabel p-classification with some playlist fully observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pla = PClassificationMLC(C1=1, C2=1, C3=0.1, weighting='both', similarMat=same_user_mat)\n",
    "pla.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: normalise **per playlist**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dev set:')\n",
    "Y_pla = pla.predict(X_train)\n",
    "preds = [Y_pla[isnanmat[:, col], col] for col in dev_cols]\n",
    "eval_pl(gts, preds, useLoop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set:')\n",
    "col_split = len(train_playlists)\n",
    "eval_pl(Y_train[:, :col_split].astype(np.bool), pla.predict(X_train)[:, :col_split])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check performance per user**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for u in set(user_of_playlists2):\n",
    "    uind = np.where(np.array(user_of_playlists2, dtype=np.object) == u)[0]\n",
    "    uind_train = uind[uind < col_split]\n",
    "    uind_test = uind[uind >= col_split]\n",
    "    #print(uind)\n",
    "    if len(uind_test) < 1: continue\n",
    "    print('--------------------')\n",
    "    print('USER:', u)\n",
    "    print('#train: %d, #test: %d' % (len(uind_train), len(uind_test)))\n",
    "    preds_test = [Y_pla[isnanmat[:, col], col] for col in uind_test]\n",
    "    gts_test = [Y[isnanmat[:, col], col] for col in uind_test]\n",
    "    print('Dev set:')\n",
    "    eval_pl(gts_test, preds_test, useLoop=True)\n",
    "    print('Training set:')\n",
    "    eval_pl(Y[:, uind_train], Y_pla[:, uind_train])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the if the regulariser is effective**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "rows, cols = np.nonzero(same_user_mat)\n",
    "for row, col in zip(rows, cols):\n",
    "    diff = pla.W[row] - pla.W[col]\n",
    "    print('%g' % np.sqrt(np.dot(pla.W[row], pla.W[row])))\n",
    "    print('%g' % np.sqrt(np.dot(pla.W[col], pla.W[col])))\n",
    "    print('%g' % np.sqrt(np.dot(diff, diff)))\n",
    "    print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute matrix $M$ such that $M_{jk} = \\sqrt{(w_j - w_k)^\\top (w_j - w_k)}, \\forall j, k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.dot(pla.W, pla.W.T)\n",
    "B = np.tile(np.diag(A), (A.shape[0], 1))\n",
    "M = np.sqrt(-2 * A + (B + B.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise $M$ by the vector with maximum norm in $W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aa = np.arange(6).reshape(3, 2)\n",
    "#np.einsum('ij,ij->i', aa, aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denorm = np.sqrt(np.einsum('ij,ij->i', pla.W, pla.W))  # compute the norm for each row in W\n",
    "M1 = M / np.max(denorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(M1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = np.nonzero(same_user_mat)\n",
    "M2 = M1[rows, cols]\n",
    "print('Min: %.5f, Max: %.5f, Mean: %.5f, Std: %.5f' % (np.min(M2), np.max(M2), np.mean(M2), np.std(M2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = same_user_mat.copy()\n",
    "np.fill_diagonal(mat, 1)   # remove the diagnoal from consideration\n",
    "rows, cols = np.where(mat == 0)\n",
    "M3 = M1[rows, cols]\n",
    "print('Min: %.5f, Max: %.5f, Mean: %.5f, Std: %.5f' % (np.min(M3), np.max(M3), np.mean(M3), np.std(M3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
