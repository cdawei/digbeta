{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Playlist generation / augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, time\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('src')\n",
    "from PClassificationMLC import PClassificationMLC\n",
    "from BinaryRelevance import BinaryRelevance\n",
    "from evaluate import f1_score_nowarn, calc_F1, calc_precisionK, evaluate_minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/aotm-2011/setting2'\n",
    "fxtrain = os.path.join(data_dir, 'X_train_audio.pkl')\n",
    "fytrain = os.path.join(data_dir, 'Y_train_audio.pkl')\n",
    "fxdev   = os.path.join(data_dir, 'X_dev_audio.pkl')\n",
    "fydev   = os.path.join(data_dir, 'Y_dev_audio.pkl')\n",
    "fxtest  = os.path.join(data_dir, 'X_test_audio.pkl')\n",
    "fytest  = os.path.join(data_dir, 'Y_test_audio.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pkl.load(open(fxtrain, 'rb'))\n",
    "Y_train = pkl.load(open(fytrain, 'rb'))\n",
    "X_dev   = pkl.load(open(fxdev,   'rb'))\n",
    "Y_dev   = pkl.load(open(fydev,   'rb'))\n",
    "X_test  = pkl.load(open(fxtest,  'rb'))\n",
    "Y_test  = pkl.load(open(fytest,  'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train: %15s %15s' % (X_train.shape, Y_train.shape))\n",
    "print('Dev  : %15s %15s' % (X_dev.shape,   Y_dev.shape))\n",
    "print('Test : %15s %15s' % (X_test.shape,  Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = ['LR', 'PC', 'LR-2017']\n",
    "cols = ['F1', 'Precision@K']\n",
    "df = pd.DataFrame(index=rows, columns=cols)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on dev set --BR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1\n",
    "#fname = os.path.join('data', 'aotm2011-params-br/br-aotm2011-C-%s.pkl' % str(C))\n",
    "fname = os.path.join(data_dir, 'br-aotm2011-C-%g.pkl' % C)\n",
    "br = pkl.load(open(fname, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate F1: threshold for logistic regression is 0 for logits, 0.5 for probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "F1 = evaluate_minibatch(br, calc_F1, X_dev, Y_dev, threshold=0, batch_size=1500, verbose=1)\n",
    "avgF1 = np.mean(F1)\n",
    "F1_all.append(avgF1)\n",
    "print('\\nF1: %g' % avgF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`C: 0.1, Threshold: 0.05, F1: 0.00254648`  \n",
    "`C:   1, Threshold: 0.05, F1: 0.0121401`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Precision@K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pak = evaluate_minibatch(br, calc_precisionK, X_dev, Y_dev, threshold=None, batch_size=1500, verbose=1)\n",
    "print('\\nPrecision@K: %g' % np.mean(pak))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`C: 0.1, Precision@K: 0.0884917`  \n",
    "`C:   1, Precision@K: 0.0943461`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test set -- BR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_C = 1\n",
    "best_TH = 0.05\n",
    "fname = os.path.join('data', 'aotm2011-params-br/br-aotm2011-C-%s.pkl' % str(best_C))\n",
    "best_br = pkl.load(open(fname, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_test_br = evaluate_minibatch(best_br, calc_F1, X_test, Y_test, threshold=best_TH, batch_size=1500, verbose=1)\n",
    "print('\\nTest F1: %g' % np.mean(F1_test_br))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pak_test_br = evaluate_minibatch(best_br, calc_precisionK, X_test, Y_test,threshold=None,batch_size=1500,verbose=1)\n",
    "print('\\nTest Precision@K: %g' % np.mean(pak_test_br))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['LR', 'F1'] = np.mean(F1_test_br)\n",
    "df.loc['LR', 'Precision@K'] = np.mean(pak_test_br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on dev set -- PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "C_set = [0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000, 3000, 10000, 30000]\n",
    "p_set = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "metrics_pc = [ ]\n",
    "print('%15s %15s %15s %15s %15s' % ('C', 'p', 'Threshold', 'F1', 'Precision@K'))\n",
    "for C in C_set:\n",
    "    for p in p_set:\n",
    "        #fname = os.path.join('data', 'aotm2011-params-pc/pc-aotm2011-C-%g-p-%g.pkl' % (C, p))\n",
    "        fname = os.path.join(data_dir, 'pc-aotm2011-C-%g-p-%g.pkl' % (C, p))\n",
    "        if not os.path.exists(fname): continue\n",
    "        pc_dict = pkl.load(open(fname, 'rb'))\n",
    "        print('%15s %15s %15s %15s %15s' % ('%g'%pc_dict['C'], '%g'%pc_dict['p'], \\\n",
    "                                            '%g'%pc_dict['Threshold'], '%g'%pc_dict['F1'], \\\n",
    "                                            '%g'%pc_dict['Precision@K']))\n",
    "        metrics_pc.append((pc_dict['C'], pc_dict['p'], pc_dict['Threshold'],pc_dict['F1'],pc_dict['Precision@K']))\n",
    "        clf = PClassificationMLC()\n",
    "        clf.load_params(fname)\n",
    "        th = pc_dict['Threshold']\n",
    "        F1 = evaluate_minibatch(clf, calc_F1, X_test, Y_test, threshold=th, batch_size=1500, verbose=1)\n",
    "        print('\\nTest F1: %g' % np.mean(F1))\n",
    "        pak = evaluate_minibatch(clf, calc_precisionK, X_test, Y_test, threshold=None, batch_size=1500, verbose=1)\n",
    "        print('\\nTest Precision@K: %g' % np.mean(pak))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyix = 3  # F1\n",
    "sorted_metrics_pc = sorted(metrics_pc, key=lambda x: x[keyix], reverse=True)\n",
    "print('Best hyper-param:\\n(C, p, Threshold, F1, Precision@K):', sorted_metrics_pc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test set -- PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_C = 30000 #10000 #300   #3000\n",
    "best_p = 2 #2 #3     #6\n",
    "best_TH = 0.1 #0.15 #0.1\n",
    "#fname = os.path.join('data', 'aotm2011-params-pc/pc-aotm2011-C-%g-p-%g.pkl' % (best_C, best_p))\n",
    "fname = os.path.join(data_dir, 'pc-aotm2011-C-%g-p-%g.pkl' % (best_C, best_p))\n",
    "best_pc = PClassificationMLC()\n",
    "best_pc.load_params(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestdict = pkl.load(open(fname, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bestdict['cost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_test_pc = evaluate_minibatch(best_pc, calc_F1, X_test, Y_test, threshold=best_TH, batch_size=1500, verbose=1)\n",
    "print('\\nTest F1: %g' % np.mean(F1_test_pc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pak = evaluate_minibatch(best_pc, calc_precisionK, X_train, Y_train, threshold=None, batch_size=1500, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nTrain P@K: %g' % np.mean(pak))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = best_pc.decision_function(X_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ex_idx = 2\n",
    "\n",
    "plt.hist(preds[test_ex_idx], bins=50)\n",
    "\n",
    "y_true = Y_train[test_ex_idx].toarray()\n",
    "\n",
    "pos_idx = np.where(y_true)[1]\n",
    "print('prediction of true positives')\n",
    "print(preds[test_ex_idx][pos_idx])\n",
    "print('top predictions')\n",
    "np.sort(preds[test_ex_idx])[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pak_test_pc = evaluate_minibatch(best_pc, calc_precisionK, X_test, Y_test,threshold=None,batch_size=1500,verbose=1)\n",
    "print('\\nTest Precision@K: %g' % np.mean(pak_test_pc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['PC', 'F1'] = np.mean(F1_test_pc)\n",
    "df.loc['PC', 'Precision@K'] = np.mean(pak_test_pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['LR-2017', 'F1'] = 0.031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_str = df.to_latex(float_format=lambda x: '$%.4f$' % x, na_rep='-', multirow=False, escape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\\\begin{table}[!h]')\n",
    "print('\\centering')\n",
    "print('\\\\caption{Performance on test set}')\n",
    "print('\\\\label{tab:perf}')    \n",
    "print(tab_str)\n",
    "print('\\\\end{table}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
