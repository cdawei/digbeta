{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Dev/Test split using AotM-2011 Playlists & MSD Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "import gzip\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import lil_matrix, issparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/aotm-2011'\n",
    "#faotm = os.path.join(data_dir, 'aotm2011-subset.pkl')\n",
    "faotm = os.path.join(data_dir, 'aotm2011-user-playlist.pkl')\n",
    "ffeature = 'data/msd/songID2Features.pkl.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load playlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_playlists = pkl.load(open(faotm, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#user    : 14182\n",
      "#playlist: 84710\n"
     ]
    }
   ],
   "source": [
    "print('#user    :', len(user_playlists))\n",
    "print('#playlist:', np.sum([len(user_playlists[u]) for u in user_playlists]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = sorted(user_playlists.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SOEURWZ12A58A7B772',\n",
       " 'SOGVEJN12A6D4F9D83',\n",
       " 'SOUNSZA12A8C13A204',\n",
       " 'SOXLLMC12A8C13E499',\n",
       " 'SOQPSMN12AB0184EDA',\n",
       " 'SOKAWWA12A6701FD46']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_playlists[users[0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average playlist length: 10.1\n"
     ]
    }
   ],
   "source": [
    "pl_lengths = [len(pl) for u in user_playlists for pl in user_playlists[u]]\n",
    "#plt.hist(pl_lengths, bins=100)\n",
    "print('Average playlist length: %.1f' % np.mean(pl_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84710"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array(pl_lengths) >= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "857782"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pl_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "857782"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([len(set(pl)) for u in user_playlists for pl in user_playlists[u]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 45, 10.1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.min(pl_lengths), np.max(pl_lengths), float('%.1f' % np.mean(pl_lengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load song features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `song_id` --> `feature array` mapping: map a song to the audio features of one of its corresponding tracks in MSD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2Features = pkl.load(gzip.open(ffeature, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of songs, which is the set of labels in this formulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#song_set = sorted(song2Features.keys())  # use MSD songs as label space\n",
    "# use the intersection of MSD and AotM as label space\n",
    "song_set = sorted({sid for u in user_playlists for pl in user_playlists[u] for sid in pl})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114428"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(song_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Song-User Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transpose the user-song matrix to song-user matrix: songs as rows, users as columns, split rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_songuser_mat(user_playlists, song_set, features_MSD):\n",
    "    \"\"\"\n",
    "    Create labelled dataset: rows are songs, columns are users.\n",
    "    \n",
    "    Input:\n",
    "        - user_playlists: dictionary that maps users to a set of playlists\n",
    "        - song_set: a set of songIDs\n",
    "        - features_MSD: dictionary that maps songIDs to features from MSD\n",
    "    Output:\n",
    "        - (Feature, Label) pair (X, Y)\n",
    "          X: #songs by #features\n",
    "          Y: #songs by #users\n",
    "    \"\"\"\n",
    "    user_set = sorted(user_playlists.keys())\n",
    "    song_indices = {sid: ix for ix, sid in enumerate(song_set)}\n",
    "    N = len(song_set)\n",
    "    K = len(user_set)\n",
    "    \n",
    "    X = np.array([features_MSD[sid] for sid in song_set])\n",
    "    Y = lil_matrix((N, K), dtype=np.bool)\n",
    "    \n",
    "    for k in range(K):\n",
    "        if (k+1) % 1000 == 0:\n",
    "            sys.stdout.write('\\r%d / %d' % (k+1, K))\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        u = user_set[k]\n",
    "        songs = sorted({sid for pl in user_playlists[u] for sid in pl})\n",
    "        indices = [song_indices[sid] for sid in songs]\n",
    "        Y[indices, k] = 1\n",
    "\n",
    "    return X, Y.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdir = os.path.join(data_dir, 'song_user')\n",
    "fx      = os.path.join(fdir, 'X.pkl')\n",
    "fy      = os.path.join(fdir, 'Y.pkl')\n",
    "fxtrain = os.path.join(fdir, 'X_train.pkl')\n",
    "fytrain = os.path.join(fdir, 'Y_train.pkl')\n",
    "fxdev   = os.path.join(fdir, 'X_dev.pkl')\n",
    "fydev   = os.path.join(fdir, 'Y_dev.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14000 / 14182"
     ]
    }
   ],
   "source": [
    "if np.all([os.path.exists(fname) for fname in [fx, fy, fxtrain, fytrain, fxdev, fydev]]):\n",
    "    X       = pkl.load(open(fx,      'rb'))\n",
    "    Y       = pkl.load(open(fy,      'rb'))\n",
    "    X_train = pkl.load(open(fxtrain, 'rb'))\n",
    "    Y_train = pkl.load(open(fytrain, 'rb'))\n",
    "    X_dev   = pkl.load(open(fxdev,   'rb'))\n",
    "    Y_dev   = pkl.load(open(fydev,   'rb'))\n",
    "else:\n",
    "    # generate dataset\n",
    "    X, Y = gen_songuser_mat(user_playlists=user_playlists, song_set=song_set, features_MSD=song2Features)\n",
    "    \n",
    "    # data split: approximately 80/20 for training/dev\n",
    "    X_train, X_dev, Y_train, Y_dev = train_test_split(X, Y, test_size=0.2, random_state=123456789)\n",
    "    \n",
    "    # feature normalisation\n",
    "    X_train_mean = np.mean(X_train, axis=0).reshape((1, -1))\n",
    "    X_train_std = np.std(X_train, axis=0).reshape((1, -1)) + 10 ** (-6)\n",
    "    X_train -= X_train_mean\n",
    "    X_train /= X_train_std\n",
    "    X_dev   -= X_train_mean\n",
    "    X_dev   /= X_train_std\n",
    "    \n",
    "    # save to files\n",
    "    pkl.dump(X,       open(fx, 'wb'))\n",
    "    pkl.dump(Y,       open(fy, 'wb'))\n",
    "    pkl.dump(X_train, open(fxtrain, 'wb'))\n",
    "    pkl.dump(Y_train, open(fytrain, 'wb'))\n",
    "    pkl.dump(X_dev,   open(fxdev,   'wb'))\n",
    "    pkl.dump(Y_dev,   open(fydev,   'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All  :   (114428, 202) (114428, 14182)\n",
      "Train:    (91542, 202)  (91542, 14182)\n",
      "Dev  :    (22886, 202)  (22886, 14182)\n"
     ]
    }
   ],
   "source": [
    "print('All  : %15s %15s' % (X.shape, Y.shape))\n",
    "print('Train: %15s %15s' % (X_train.shape, Y_train.shape))\n",
    "print('Dev  : %15s %15s' % (X_dev.shape,   Y_dev.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.744088163228033e-15"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.mean(X_train, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.019815081566402215"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.std(X_train, axis=0)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0005716163590059731"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.mean(X_dev, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0354526080921741"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.std(X_dev, axis=0)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playlist-Song Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The playlist-song matrix: playlists as rows, songs as columns, split rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For users with more than 10 playlists, we split their playlists approximately 70/10/20 for train/dev/test.\n",
    "- For each playlist in dev/test set, we remove the last 20% and try to predict them.\n",
    "- All remaining playlists are used for training as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(label_indices.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_training_set(playlists, label_indices, features):\n",
    "    \"\"\"\n",
    "        Create the labelled dataset: rows are playlists, columns are songs\n",
    "        \n",
    "        Input:\n",
    "            - playlists: which playlists to create features for\n",
    "            - label_indices: a dictionary that maps a songID to the index of the corresponding label\n",
    "            - features: a dictionary that maps a songID to its feature vector\n",
    "            \n",
    "        Output:\n",
    "            - (Feature, Label) pair (X, Y), with # num playlists rows\n",
    "              X comprises the features for each seed song (the 1st in playlist)\n",
    "              Y comprises the indicators of whether the given song is present in the respective playlist\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(playlists)\n",
    "    K = len(label_indices)\n",
    "\n",
    "    X = [ ]\n",
    "    Y = lil_matrix((N, K), dtype=np.int8)\n",
    "    \n",
    "    cnt = 0\n",
    "    for i in range(len(playlists)):\n",
    "        cnt += 1\n",
    "        if cnt % 1000 == 0:\n",
    "            sys.stdout.write('\\r%d / %d' % (cnt, len(playlists)))\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        playlist = playlists[i]\n",
    "        seed     = playlist[0]\n",
    "\n",
    "        X.append(features[seed])\n",
    "        #indices = [label_indices[s] for s in playlist]\n",
    "        indices = [label_indices[s] for s in playlist if s in label_indices]\n",
    "        Y[i, indices] = 1\n",
    "\n",
    "    return np.array(X), Y.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_dict = {1: 0, 2: 1, 3: 2}\n",
    "#[test_dict[s] for s in [1, 2, 5] if s in test_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdir = os.path.join(data_dir, 'setting1')\n",
    "fxtrain = os.path.join(fdir, 'X_train_audio.pkl')\n",
    "fytrain = os.path.join(fdir, 'Y_train_audio.pkl')\n",
    "fxdev   = os.path.join(fdir, 'X_dev_audio.pkl')\n",
    "fydev   = os.path.join(fdir, 'Y_dev_audio.pkl')\n",
    "fxtest  = os.path.join(fdir, 'X_test_audio.pkl')\n",
    "fytest  = os.path.join(fdir, 'Y_test_audio.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.all([os.path.exists(fname) for fname in [fxtrain, fytrain, fxdev, fydev, fxtest, fytest]]):\n",
    "    X_train = pkl.load(open(fxtrain, 'rb'))\n",
    "    Y_train = pkl.load(open(fytrain, 'rb'))\n",
    "    X_dev   = pkl.load(open(fxdev,   'rb'))\n",
    "    Y_dev   = pkl.load(open(fydev,   'rb'))\n",
    "    X_test  = pkl.load(open(fxtest,  'rb'))\n",
    "    Y_test  = pkl.load(open(fytest,  'rb'))\n",
    "else:\n",
    "    # generate dataset\n",
    "    X, Y = gen_training_set(playlists=playlists, label_indices=label_indices, features=song2Features)\n",
    "    \n",
    "    # data split: approximately 70/10/20 for training/dev/test\n",
    "    # by fixing random seed, the same playlists will be in the test set each time\n",
    "    X_train, X_other, Y_train, Y_other = train_test_split(X, Y, test_size=0.3, random_state=123456789)\n",
    "    X_dev,   X_test,  Y_dev,   Y_test  = train_test_split(X_other, Y_other, test_size=0.65, random_state=987654321)\n",
    "    \n",
    "    # feature normalisation\n",
    "    X_train_mean = np.mean(X_train, axis=0).reshape((1, -1))\n",
    "    X_train_std = np.std(X_train, axis=0).reshape((1, -1)) + 10 ** (-6)\n",
    "    X_train -= X_train_mean\n",
    "    X_train /= X_train_std\n",
    "    X_dev   -= X_train_mean\n",
    "    X_dev   /= X_train_std\n",
    "    X_test  -= X_train_mean\n",
    "    X_test  /= X_train_std\n",
    "    \n",
    "    # save to files\n",
    "    pkl.dump(X_train, open(fxtrain, 'wb'))\n",
    "    pkl.dump(Y_train, open(fytrain, 'wb'))\n",
    "    pkl.dump(X_dev,   open(fxdev,   'wb'))\n",
    "    pkl.dump(Y_dev,   open(fydev,   'wb'))\n",
    "    pkl.dump(X_test,  open(fxtest,  'wb'))\n",
    "    pkl.dump(Y_test,  open(fytest,  'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train: %15s %15s' % (X_train.shape, Y_train.shape))\n",
    "print('Dev  : %15s %15s' % (X_dev.shape,   Y_dev.shape))\n",
    "print('Test : %15s %15s' % (X_test.shape,  Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.mean(X_train, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.std(X_train, axis=0)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.mean(X_dev, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.std(X_dev, axis=0)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.mean(X_test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.std(X_test, axis=0)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of playlists as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_training_set2(playlists, features):\n",
    "    \"\"\"\n",
    "        Create the labelled dataset: rows are songs, columns are playlists\n",
    "        \n",
    "        Input:\n",
    "            - playlists: which playlists to create features for\n",
    "            - features: a dictionary that maps a songID to its feature vector\n",
    "            \n",
    "        Output:\n",
    "            - (Feature, Label) pair (X, Y), with # num playlists rows\n",
    "              X comprises the features for each song\n",
    "              Y comprises the indicators of whether the given song is present in the respective playlist\n",
    "    \"\"\"\n",
    "    \n",
    "    song_set = sorted({sid for pl in playlists for sid in pl})\n",
    "    songInPlaylist = {sid: [] for sid in song_set}\n",
    "    N = len(song_set)\n",
    "    K = len(playlists)\n",
    "\n",
    "    for j in range(K):\n",
    "        pl = playlists[j]\n",
    "        for sid in pl:\n",
    "            songInPlaylist[sid].append(j)\n",
    "    \n",
    "    X = [ ]\n",
    "    Y = lil_matrix((N, K), dtype=np.int8)\n",
    "    for i in range(N):\n",
    "        if (i+1) % 1000 == 0:\n",
    "            sys.stdout.write('\\r%d / %d' % (i+1, N))\n",
    "            sys.stdout.flush()\n",
    "        sid = song_set[i]\n",
    "        X.append(features[sid])\n",
    "        indices = songInPlaylist[sid]\n",
    "        Y[i, indices] = 1\n",
    "        \n",
    "    return np.array(X), Y.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdir = os.path.join(data_dir, 'setting2')\n",
    "fxtrain = os.path.join(fdir, 'X_train_audio.pkl')\n",
    "fytrain = os.path.join(fdir, 'Y_train_audio.pkl')\n",
    "fxdev   = os.path.join(fdir, 'X_dev_audio.pkl')\n",
    "fydev   = os.path.join(fdir, 'Y_dev_audio.pkl')\n",
    "fxtest  = os.path.join(fdir, 'X_test_audio.pkl')\n",
    "fytest  = os.path.join(fdir, 'Y_test_audio.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.all([os.path.exists(fname) for fname in [fxtrain, fytrain, fxdev, fydev, fxtest, fytest]]):\n",
    "    X_train = pkl.load(open(fxtrain, 'rb'))\n",
    "    Y_train = pkl.load(open(fytrain, 'rb'))\n",
    "    X_dev   = pkl.load(open(fxdev,   'rb'))\n",
    "    Y_dev   = pkl.load(open(fydev,   'rb'))\n",
    "    X_test  = pkl.load(open(fxtest,  'rb'))\n",
    "    Y_test  = pkl.load(open(fytest,  'rb'))\n",
    "else:\n",
    "    # generate dataset\n",
    "    X, Y = gen_training_set2(playlists=playlists, features=song2Features)\n",
    "\n",
    "    # data split: approximately 70/10/20 for training/dev/test\n",
    "    # by fixing random seed, the same playlists will be in the test set each time\n",
    "    X_train, X_other, Y_train, Y_other = train_test_split(X, Y, test_size=0.3, random_state=59)\n",
    "    X_dev,   X_test,  Y_dev,   Y_test  = train_test_split(X_other, Y_other, test_size=0.65, random_state=71)\n",
    "\n",
    "    # feature normalisation\n",
    "    X_train_mean = np.mean(X_train, axis=0).reshape((1, -1))\n",
    "    X_train_std = np.std(X_train, axis=0).reshape((1, -1)) + 10 ** (-6)\n",
    "    X_train -= X_train_mean\n",
    "    X_train /= X_train_std\n",
    "    X_dev   -= X_train_mean\n",
    "    X_dev   /= X_train_std\n",
    "    X_test  -= X_train_mean\n",
    "    X_test  /= X_train_std\n",
    "\n",
    "    # save to files\n",
    "    pkl.dump(X_train, open(fxtrain, 'wb'))\n",
    "    pkl.dump(Y_train, open(fytrain, 'wb'))\n",
    "    pkl.dump(X_dev,   open(fxdev,   'wb'))\n",
    "    pkl.dump(Y_dev,   open(fydev,   'wb'))\n",
    "    pkl.dump(X_test,  open(fxtest,  'wb'))\n",
    "    pkl.dump(Y_test,  open(fytest,  'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train: %15s %15s' % (X_train.shape, Y_train.shape))\n",
    "print('Dev  : %15s %15s' % (X_dev.shape,   Y_dev.shape))\n",
    "print('Test : %15s %15s' % (X_test.shape,  Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.mean(X_train, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.std(X_train, axis=0)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.mean(X_dev, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.std(X_dev, axis=0)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.mean(X_test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.std(X_test, axis=0)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Song in playlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_set = sorted({sid for pl in playlists for sid in pl})\n",
    "songInPlaylist = {sid: [] for sid in song_set}\n",
    "K = len(playlists)\n",
    "for j in range(K):\n",
    "    pl = playlists[j]\n",
    "    for sid in pl: songInPlaylist[sid].append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_train, S_other, dummy_train, dummy_other = train_test_split(song_set, np.arange(len(song_set)), \n",
    "                                                              test_size=0.3, random_state=59)\n",
    "S_dev, S_test, dummy_dev, dummy_test = train_test_split(S_other, dummy_other, test_size=0.65, random_state=71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(S_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(S_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 0\n",
    "sid = S_train[ix]\n",
    "#np.equal(song2Features[sid], X_train[ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songInPlaylist[song_set[dummy_train[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = pd.read_csv('data/f1.txt', names=['F1', '0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1['F1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pak = pd.read_csv('data/pak.txt', names=['PaK', '0'])\n",
    "pak = pak['PaK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = np.min([np.min(f1), np.min(pak)]) - 0.00005\n",
    "xmax = np.max([np.max(f1), np.max(pak)]) + 0.00005\n",
    "plt.xlim([xmin, xmax])\n",
    "plt.ylim([xmin, xmax])\n",
    "plt.plot([xmin, xmax], [xmin, xmax], ls='--', c='g')\n",
    "plt.scatter(f1, pak)\n",
    "plt.xlabel('F1')\n",
    "plt.ylabel('Precision@K')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
