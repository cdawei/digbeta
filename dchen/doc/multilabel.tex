\documentclass[9pt]{extarticle}
\usepackage[a4paper,top=0.79in,left=0.79in,bottom=0.79in,right=0.79in]{geometry} % A4 paper margins in LibreOffice
\usepackage[numbers,compress]{natbib}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{bbm}
%\usepackage{ulem}
\usepackage{stmaryrd}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[sc]{mathpazo}
\linespread{1.05}       % Palladio needs more leading (space between lines)
\usepackage[T1]{fontenc}
\usepackage{footmisc}   % \footref, refer the same footnote at different places
\usepackage{subcaption} % sub-figures
\usepackage{setspace}   % set space between lines
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{xcolor}
\usepackage{graphicx}
\graphicspath{{fig/}}   % Location of the graphics files

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\eat}[1]{}
\newcommand{\given}{\mid}
\newcommand{\llb}{\llbracket}
\newcommand{\rrb}{\rrbracket}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\h}{\mathbf{h}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\1}{\mathbf{1}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\p}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\q}{\mathbf{q}}
\newcommand{\LCal}{\mathcal{L}}
\newcommand{\SCal}{\mathcal{S}}
\newcommand{\XCal}{\mathcal{X}}
\newcommand{\YCal}{\mathcal{Y}}
\newcommand{\alphat}{\widetilde{\alpha}}
\newcommand{\betat}{\widetilde{\beta}}
\newcommand{\gammat}{\widetilde{\gamma}}
\newcommand{\phit}{\widetilde{\phi}}
\newcommand{\alphabm}{\bm{\alpha}}
\newcommand{\betabm}{\bm{\beta}}
\newcommand{\nubm}{\bm{\nu}}
\newcommand{\xibm}{\bm{\xi}}
% madeness: suPer-script in Brackets
\newcommand{\pb}[1]{^{({#1})}}

\newcommand{\eg}{e.g.\ }
\newcommand{\ie}{i.e.\ }
\newcommand{\downto}{\,\textbf{downto}\,}
\newcommand{\blue}[1]{{\color{blue}{#1}}}

\setlength{\columnsep}{1.5em} % spacing between columns

\title{Multi-label Classification, Bipartite Ranking and Playlist Generation}

\author{Dawei Chen}

\date{\today}

\begin{document}

\maketitle

\section{Multi-label classification}
\label{sec:mlc}

%1. Brief summary of reference
\paragraph{Summary}
\citet{dembczynski:2010} formalised the multi-label classification problem, 
and claimed that if Hamming loss or rank loss is used,
multi-label classification methods, in theory, could not benefit from modelling label dependence.
On the other hand, modelling correlation between labels was necessary if one chose to use the subset 0/1 loss.
Further, a probabilistic classifier chains (PCC), which generalised the classifier chains (CC) from a probabilistic perspective,
was proposed to modelling label correlation. 

Theoretically, the order of labels does not affect the model, 
in practice, however, using different order of labels will result in different model parameters (we do not have infinity data).
To alleviate this issue, an ensemble of PCC (EPCC) was proposed, which made a prediction by averaging over predictions by a number of PCCs, 
each model was trained using a randomly chosen permutation of the labels.
PCC (and EPCC) was empirically shown to outperform a number of baselines that did not model label correlations when label dependence existed in data.


\noindent
\paragraph{Definition}
Let $\LCal = \{\lambda_1,\dots,\lambda_l\}$ be a finite set of class labels,
and example $(\x,\y) \in \XCal \times \YCal$, 
where $\YCal \in \{0,1\}^m$ is the set of all possible labels,
and $\y=y_{1:m}$ is a binary vector where $y_i = 1$ \emph{iff} $\lambda_i$ is a label of $\x$.
A multi-label classifier is a mapping $\h: \XCal \to \YCal$.

\noindent
\paragraph{Label dependence}
Suppose examples are independent and identically distributed (iid) according to a joint probability distribution $\p(\X,\Y)$ on $\XCal \times \YCal$,
where $\X$ is a random variable and $\Y=Y_{1:l}$ is a random vector,
Let $\p\pb{i}(Y_i |\x)$ be the marginal distribution of $Y_i$, then
\begin{equation*}
\p\pb{i}(Y_i=b |\x) = \sum_{\y \in \YCal:y_i = b} \p(\Y = \y |\x),
\end{equation*}
where $\p(\Y = \y |\x)$ is the posterior distribution given observation $\x$.
We note that the labels are not independent if 
\begin{equation*}
\p(\Y |\x) \ne \prod_{i=1}^l \p\pb{i}(Y_i |\x),
\end{equation*}
and the degree of dependence could be quantified in terms of measures such as cross entropy and KL divergence.

\noindent
\paragraph{Learning}
Given a loss function $\ell(\cdot)$, 
we can learn a multi-label classifier by find a model $\h^*$ that minimise the expected loss over the joint distribution $\p(\X,\Y)$:
\begin{equation*}
\h^* 
= \argmin_{\h} \, \E_{\X\Y} \, \ell(\Y,\h(\X))
= \argmin_{\h} \, \E_{\X} \, \E_{\Y|\X} \, \ell(\Y,\h(X))
= \argmin_{\h} \, \sum_{\x} \x \p(\x) \, \E_{\Y|\X} \, \ell(\Y,\h(\x)),
\end{equation*}
thanks to the summation, fix $\x$, we have
\begin{equation*}
\h^*(\x) = \argmin_{\y} \, \E_{\Y|\X} \, \ell(\Y,\y).
\end{equation*}
Frequently used loss functions in the context of multi-label classification including Hamming loss, rank loss and subset 0/1 loss~\cite{dembczynski:2010},
here we focus on a rank loss (taking care of ties):
\begin{equation}
\label{eq:loss_rank}
\ell(\y, \h(\x)) = \sum_{(i,j): y_i > y_j} \left( \llb h_i < h_j \rrb + \frac{1}{2} \llb h_i = h_j \rrb \right).
\end{equation}
\emph{Theorem 3.1 in~\cite{dembczynski:2010} here.}

\noindent
\paragraph{Probabilistic classifier chains}
Given a query $\x$, the posterior probability of a label $\y$ can be computed using the product rule of probability:
\begin{equation*}
\p(\y |\x) = \p(y_1) \cdot \prod_{i=2}^l \p(y_i |\x, y_{1:i-1}),
\end{equation*}
and we further define a function:
\begin{equation*}
f_i = 
\begin{cases}
\p(y_i = 1 |\x), & i = 1 \\
\p(y_i = 1 |\x, y_{1:i-1}), & 1 < i \le l
\end{cases}
\end{equation*}
then we have
\begin{equation*}
\p(\y |\x) = f_1 \cdot \prod_{i=2}^l f_i,
\end{equation*}
where $f_i$ uses $\x$ and $y_{1:i-1}$ as the input features. 
Theoretically, the results of the product rule does not depend on the order of variables, 
however, in practice, different order of variables will result in different model parameters (\ie the order of features depend on the order of variables). \\
\emph{Greedy approach -- classifier chain; assuming Markov property, we can use the Viterbi algorithm; with Neural net, we can build an order agnostic model.}


\section{Bipartite ranking}
\label{sec:birank}

%1. Brief summary of reference
\paragraph{Summary}
\citet{li:2014} proposed a new algorithm (\ie \emph{TopPush}) for bipartite ranking to optimise the ranking accuracy at the top.
This algorithm has a linear time complexity at each iteration of the optimisation process.

The key observation was that the loss used in~\cite{agarwal:2011} (when indicator function is replaced with a convex surrogate)
can be equivalently transformed to a new form 
which can be optimised in linear time (w.r.t the size of training set).


\paragraph{Definition} 
Bipartite ranking is to learn a real-valued ranking function that places positive examples above negative examples~\cite{li:2014}.
Formally, given training examples $S = S_+ \cup S_-$ with $m$ positive examples $S_+ = \{\x_i^+\}_{i=1}^m$ and $n$ negative examples $S_- = \{\x_i^-\}_{i=1}^n$, 
bipartite ranking aims to learn a ranking function $f: \XCal \to \R$ that is likely ranks positive examples higher than negative examples.

\paragraph{Loss function}
AUC is a widely used as an evaluate metric for bipartite ranking, and it turns out that AUC can be optimised by minimising a loss defined as~\cite{cortes:2004}
\begin{equation}
\label{eq:loss_auc}
\ell_\text{rank}(f; S) = \frac{1}{mn} \sum_{i=1}^m \sum_{j=1}^n \llb f(\x_i^+) \le f(\x_j^-) \rrb,
\end{equation}
and this loss can be easily optimised (\eg by gradient descent) if we replace the indicator function with a convex surrogate such as the truncated quadratic loss 
$\ell(z) = (1+z)_+^2$, the exponential loss $\ell(z) = e^z$ and logistic loss $\ell(z) = \log(1+e^z)$.
One drawback of this loss function is enumerating all the positive-negative pairs, which is computationally expensive for large dataset. \\
\emph{Theorem 3.1 in~\cite{dembczynski:2010} for this loss function here.}

Alternatively, one may interested in optimising the ranking accuracy only at the top, 
or equivalently, we would like to minimize the number of positive examples that ranked below the highest-ranking negative instance~\cite{agarwal:2011,li:2014}:
\begin{equation}
\label{eq:loss_inf}
\begin{aligned}
\ell_{\infty}(f; S) 
&= \max_{1 \le j \le n} \frac{1}{m} \sum_{i=1}^m \, \llb f(\x_i^+) < f(\x_j^-) \rrb \\
&= \frac{1}{m} \sum_{i=1}^m \max_{1 \le j \le n} \llb f(\x_i^+) < f(\x_j^-) \rrb,
\end{aligned}
\end{equation}
by replace the indicator function in (\ref{eq:loss_inf}) with a convex surrogate $\ell(\cdot)$, we have
\begin{equation}
\label{eq:loss_inf1} 
\begin{aligned}
\tilde{\ell}_{\infty}(f; S) 
&= \frac{1}{m} \sum_{i=1}^m \max_{1 \le j \le n} \ell\left( f(\x_j^-) - f(\x_i^+) \right) \\
&= \frac{1}{m} \sum_{i=1}^m \ell\left( \max_{1 \le j \le n} f(\x_j^-) - f(\x_i^+) \right),
\end{aligned}
\end{equation}
which can be optimised more efficiently than (\ref{eq:loss_auc})~\cite{li:2014}.

\paragraph{Dual formulation}
Consider a linear ranking function $f(\x) = \w^\top \x$ and loss function~\ref{eq:loss_inf1}.
Table~\ref{tab:symbol} summarises some notation we will use.
\begin{table}[!h]
\caption{Glossary of commonly used symbols}
\label{tab:symbol}
\renewcommand{\arraystretch}{1.5} % tweak the space between rows
\setlength{\tabcolsep}{1pt} % tweak the space between columns
\centering
\begin{tabular}{llll}
\hline \hline
\multicolumn{3}{l}{\textbf{Symbol}} & \textbf{Quantity} \\ \hline 
$d$              &  $\in$  &  $\Z^+$  & The number of features for each example \\
$\w$             &  $\in$  &  $\R^d$  & The vector of model parameters \\
$\mathbf{1}_m$   &  $\in$  &  $\R^m$  & The $m$ dimensional vector of $1$'s \\
$\X^+$           &  $\in$  &  $\R^{m \times d}\quad$  & Matrix of features of positive examples \\
$\X^-$           &  $\in$  &  $\R^{n \times d}$       & Matrix of features of negative examples \\
$\alphabm$       &  $\in$  &  $\R^m$  &  Dual variables for positive examples \\
$\betabm, \nubm$ &  $\in$  &  $\R^n$  &  Dual variables for negative examples \\ \hline
\end{tabular}
\end{table}

We can learn the model parameters $\w$ by risk minimisation with L2 regularisation:
\begin{equation}
\label{eq:minrisk}
\min_{\w} \, \frac{\lambda}{2} \w^\top \w + \frac{1}{m} \sum_{i=1}^m \ell\left( \max_{1 \le j \le n} \w^\top \x_j^- - \w^\top \x_i^+ \right),
\end{equation}
where $\lambda > 0$ is a regularisation constant.
Problem (\ref{eq:minrisk}) is hard to optimise in general due to the maximum term in loss function, one widely used trick is to form its dual problem.
Let 
\begin{equation*}
\begin{aligned}
f_0 (\w, \xi) &= \frac{\lambda}{2} \w^\top \w + \frac{1}{m} \sum_{i=1}^m \ell\left( \xi - \w^\top \x_i^+ \right), \\
f_j (\w, \xi) &= \w^\top \x_j^- - \xi, \ j \in \{1,\dots,n\}.
\end{aligned}
\end{equation*}
Then problem (\ref{eq:minrisk}) is equivalent to
\begin{equation}
\label{eq:minrisk_lg}
\begin{aligned}
\min_{\w, \xi} \quad & f_0 (\w, \xi) \\
s.t. \quad & f_j (\w, \xi) \le 0, \ j \in \{1,\dots,n\}.
\end{aligned}
\end{equation}
For $\nu_j \ge 0, \, j \in \{1,\dots,n\}$, the \emph{Lagrangian} of (\ref{eq:minrisk_lg}) is
\begin{equation}
\label{eq:minrisk_lg1}
\begin{aligned}
L(\w, \xi, \nubm) 
&= f_0 (\w, \xi) + \sum_{j=1}^n \nu_j \cdot f_j(\w, \xi) \\
&= \frac{\lambda}{2} \w^\top \w + \frac{1}{m} \sum_{i=1}^m \ell\left( \xi - \w^\top \x_i^+ \right) + \sum_{j=1}^n \nu_j \cdot \left( \w^\top \x_j^- - \xi \right)
\end{aligned}
\end{equation}
Note that the conjugate of the conjugate of a convex function is itself, \ie $f(\z) = f^{**}(\z) = \sup_{\y} \left( \z^\top \y - f^*(\y) \right)$, we have
\begin{equation}
\label{eq:lg_part1}
\begin{aligned}
\frac{1}{m} \sum_{i=1}^m \ell\left( \xi - \w^\top \x_i^+ \right)
&= \frac{1}{m} \sum_{i=1}^m \sup_{\alpha_i} \left( (\xi - \w^\top \x_i^+) \cdot \alpha_i - \ell^*(\alpha_i) \right) \\
&= \sup_{\alphabm} \left[ \frac{1}{m} \sum_{i=1}^m (\xi - \w^\top \x_i^+) \cdot \alpha_i - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right] \\
&= \sup_{\alphabm} \left[ \frac{\xi}{m} \1_m^\top \alphabm - \frac{1}{m} \alphabm^\top \X^+ \w - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right] \\
\end{aligned}
\end{equation}
where $\ell^*(\cdot)$ is the conjugate of $\ell(\cdot)$.
Further, 
\begin{equation}
\label{eq:lg_part2}
\sum_{j=1}^n \nu_j \cdot \left( \w^\top \x_j^- - \xi \right) = \nubm^\top \X^- \w - \xi \1_n^\top \nubm
\end{equation}
Then by (\ref{eq:minrisk_lg1}), (\ref{eq:lg_part1}) and (\ref{eq:lg_part2}), we have
\begin{align*}
L(\w, \xi, \alphabm, \nubm) 
&= \frac{\lambda}{2} \w^\top \w + 
   \sup_{\alphabm} \left[ \frac{\xi}{m} \1_m^\top \alphabm - \frac{1}{m} \alphabm^\top \X^+ \w - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right] +
   \nubm^\top \X^- \w - \xi \1_n^\top \nubm \\
&= \sup_{\alphabm} \left[ 
   \frac{\lambda}{2} \w^\top \w + 
   \frac{\xi}{m} \1_m^\top \alphabm - \frac{1}{m} \alphabm^\top \X^+ \w - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) +
   \nubm^\top \X^- \w - \xi \1_n^\top \nubm \right] \\
&= \sup_{\alphabm} \left[ g(\w, \xi) - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right]
\end{align*}
where
$$g(\w, \xi) = \frac{\lambda}{2} \w^\top \w + \frac{\xi}{m} \1_m^\top \alphabm - \frac{1}{m} \alphabm^\top \X^+ \w + \nubm^\top \X^- \w - \xi \1_n^\top \nubm$$
The \emph{Lagrangian dual function} of (\ref{eq:minrisk_lg}) is
\begin{equation}
\label{eq:lg_dual_func}
\begin{aligned}
\inf_{\w, \xi} \, L(\w, \xi, \alphabm, \nubm) 
&= \inf_{\w, \xi}  \, \sup_{\alphabm} \left[ g(\w, \xi) - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right] \\
&= \sup_{\alphabm} \, \inf_{\w, \xi} \left[ g(\w, \xi) - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right] ~~ \text{(assuming strong duality)} \\
&= \max_{\alphabm} \, \min_{\w, \xi} \left[ g(\w, \xi) - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right] ~~ \text{(Equation (\ref{eq:minrisk}) is L2 regularised~\cite{shalev:2007})} \\
&= \max_{\alphabm} \left[ \min_{\w, \xi} g(\w, \xi) - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right]
\end{aligned}
\end{equation}
To solve the (unconstrained) inner minimisation, let
\begin{align*}
\frac{\partial g}{\partial \w}  &= \lambda \w - \frac{1}{m} \left( \alphabm^\top \X^+ \right)^\top + \left( \nubm^\top \X^- \right)^\top = 0 \\
\frac{\partial g}{\partial \xi} &= \frac{1}{m} \1_m^\top \alphabm - \1_n^\top \nubm = 0
\end{align*}
Then we have
\begin{equation}
\label{eq:sol1}
\widetilde\w 
= \frac{1}{\lambda m} \left( \alphabm^\top \X^+ - m \nubm^\top \X^- \right)^\top 
= \frac{1}{\lambda m} \left( \alphabm^\top \X^+ - \betabm^\top \X^- \right)^\top  
\end{equation}
and
\begin{equation}
\label{eq:sol2}
\1_m^\top \alphabm = m \1_n^\top \nubm = \1_n^\top \betabm
\end{equation}
where $\betabm = m \nubm \succeq 0$, and by (\ref{eq:sol1}) and (\ref{eq:sol2}), we have
\begin{equation}
\label{eq:min_func}
\begin{aligned}
\min_{\w, \xi} g(\w, \xi) 
&= \frac{\lambda}{2} \widetilde\w^\top \widetilde\w + \frac{\xi}{m} \1_m^\top \alphabm -
   \frac{1}{m} \alphabm^\top \X^+ \widetilde\w + \nubm^\top \X^- \widetilde\w - \xi \1_n^\top \nubm \\
&= \frac{\lambda}{2} \widetilde\w^\top \widetilde\w + 
   \frac{\xi}{m} \left( \1_m^\top \alphabm - m \1_n^\top \nubm \right) - 
   \frac{1}{m} \left( \alphabm^\top \X^+ - m \nubm^\top \X^- \right) \widetilde\w \\
&= \frac{\lambda}{2} \widetilde\w^\top \widetilde\w + 0 - \lambda \widetilde\w^\top \widetilde\w \\
&= -\frac{\lambda}{2} \widetilde\w^\top \widetilde\w \\
&= -\frac{1}{2 \lambda m^2} \left\| \alphabm^\top \X^+ - \betabm^\top \X^- \right\|^2
\end{aligned}
\end{equation}
Lastly, by (\ref{eq:lg_dual_func}) and (\ref{eq:min_func}), the \emph{Lagrangian dual problem} of (\ref{eq:minrisk_lg}) is
\begin{align*}
\max_{\nubm} \, \inf_{\w, \xi} \, L(\w, \xi, \alphabm, \nubm) 
= \max_{\alphabm, \nubm} \, \left[ \min_{\w, \xi} g(\w, \xi) - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right]
= \max_{\alphabm, \betabm} \, \left[ -\frac{1}{2 \lambda m^2} \left\| \alphabm^\top \X^+ - \betabm^\top \X^- \right\|^2 - 
  \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right]
\end{align*}
subject to $\1_m^\top \alphabm = m \1_n^\top \nubm$ and $\nubm \succeq 0$,
or equivalently 
\begin{equation}
\label{eq:minrisk_dual}
\begin{aligned}
\min_{\alphabm, \betabm} \quad & \frac{1}{2 \lambda m} \left\| \alphabm^\top \X^+ - \betabm^\top \X^- \right\|^2 + \sum_{i=1}^m \ell^*(\alpha_i) \\
s.t. \quad & \1_m^\top \alphabm = \1_n^\top \betabm \\
& \betabm \succeq 0.
\end{aligned}
\end{equation}


\section{Playlist generation as multi-label classification}
\label{sec:playlist}

%2. Formal problem statement (e.g. input, output)
%\subsection{Problem formulation}

Given $N$ playlists where songs in each playlist are from a music library with $K$ songs $\{s_i\}_{i=1}^K$,
we derive a training set $\SCal = \left\{ \left( \x\pb{n}, \y\pb{n} \right) \right\}_{n=1}^N$ where $\x\pb{n} \in \R^D$ is a feature vector of the query 
induced by the $n$-th playlist (\eg the feature vector of the first song in the $n$-th playlist),
$\y\pb{n} \in \{0,1\}^K$ is a binary indicator such that 
$$
y_i\pb{n} = 
\begin{cases}
1, & \text{song $s_i$ is in the $n$-the playlist} \\
0, & \text{otherwise}
\end{cases}
$$

The empirical risk of a predictor $\f$ (with parameters $\w$) on training set $\SCal$ is
\begin{equation}
\label{eq:risk_pl}
R_{\LCal}(\f; \SCal) = \frac{1}{N} \sum_{n=1}^N \LCal\left(\f(\x\pb{n}), \y\pb{n}\right),
\end{equation}
where $\LCal(\cdot)$ is a loss function for multi-label learning such as Hamming loss, rank loss, subset 0/1 loss etc.

For training example $(\x, \y)$, let $K_+ = \{i |y_i = 1\}$ and $K_- = \{j |y_j = 0\}$, 
we consider three options for $\LCal(\cdot)$ here:
\begin{enumerate}
\item Hamming loss: 
      \begin{equation}
      \label{eq:loss_hamm_pl0}
      \LCal_\text{Hamm}(\f(\x), \y) = \frac{1}{K} \sum_{i=1}^K \; \llb f_i \ne y_i \rrb
      \end{equation}
\item Rank loss: 
      \begin{equation}
      \label{eq:loss_rank_pl0}
      \LCal_\text{Rank}(\f(\x), \y) = \frac{1}{K_+ \cdot K_-} \sum_{i \in K_+} \sum_{j \in K_-} 
                                      \left( \llb f_i < f_j \rrb + \frac{1}{2} \llb f_i = f_j \rrb \right)
      \end{equation}

\item Top-push loss:
      $$\LCal_\infty(\f(\x), \y) = \frac{1}{K_+} \sum_{i \in K_+} \max_{j \in K_-} \llb f_i < f_j \rrb,$$
      let $\ell(\cdot)$ be a convex surrogate of the indicator function, we have
      \begin{equation}
      \label{eq:loss_inf_pl}
      \widetilde{\LCal}_\infty(\f(\x), \y) = \frac{1}{K_+} \sum_{i \in K_+} \ell\left( \max_{j \in K_-} f_j - f_i \right).
      \end{equation}
\end{enumerate}

To learn the parameters of predictor $\f$, we can minimise the empirical risk (\ref{eq:risk_pl}) with L2 regularisation:
\begin{equation}
\label{eq:minrisk_l2}
\min_{\w} \, \frac{1}{2} \w^\top \w + R_{\LCal}(\f; \SCal).
\end{equation}


\subsection{Baselines}

%\paragraph{First song as seed +  Independent logistic regression}
%Suppose the feature of a query induced by a playlist is simply the feature of the first song in the playlist 
%(\ie use the first song in a playlist as the \emph{seed}).

Given a loss function $\LCal(\cdot)$, assuming L2 regularisation, the optimisation objective to learn weights $\w$ is
\begin{equation}
\label{eq:obj}
J(\w) = \frac{\lambda}{2}\w^\top \w + \frac{1}{N} \sum_{n=1}^N \LCal(\x^n, \y^n; \w),
\end{equation}
where $\w = [\w_1^\top, \cdots, \w_K^\top]^\top$ is the flattened weight vector.

We further assume the predicted score of a label has a \emph{linear} form, \ie $f_k(\x) = \w_k^\top \x, \, k \in \{1,\cdots,K\}$.



\subsubsection{Rank loss}
\label{sssec:rank}

Given a convex surrogate of the indicator function $\ell(\cdot)$, 
the rank loss of an example $(\x, \y)$ is
\begin{equation*}
\LCal(\x, \y; \w) = \frac{1}{K_+} \sum_{i: y_i = 1} \frac{1}{K_-} \sum_{j: y_j = 0} \ell(\w_i^\top \x - \w_j^\top \x),
\end{equation*}
where $K_+$ and $K_-$ is the number of positive and negative labels in $\y$ respectively, 
and $\ell(\cdot)$ used here is (log loss)
\begin{equation*}
\ell(v) = \log(1 + \exp(-v)).
\end{equation*}

Then the objective with rank loss is
\begin{equation}
\label{eq:obj_rank}
J(\w) = \frac{\lambda}{2} \w^\top \w + \frac{1}{N} \sum_{n=1}^N \frac{1}{K_+^n \cdot K_-^n} \sum_{i:y_i^n=1} \sum_{j:y_j^n=0} 
        \log \left( 1 + \exp \left( - \left( \w_i^\top \x^n - \w_j^\top \x^n \right) \right) \right).
\end{equation}
where $K_+^n$ and $K_-^n$ is the number of positive and negative labels in the $n$-th example respectively.

%The gradient of the weight vector $\w_i$ for positive labels and $\w_j$ for negative labels are
%\begin{align}
%%\label{eq:grad_rank_pos}
%\frac{\partial J(\w)}{\partial \w_i} & = \lambda \cdot \w_i + \frac{1}{N} \sum_{n=1}^N \frac{1}{K_+^n \cdot K_-^n} \sum_{j:y_j=0} 
%                                         \frac{-\x\pb{n}} {1 + \exp(\w_i^\top \x\pb{n} - \w_j^\top \x\pb{n})} \\
%\frac{\partial J(\w)}{\partial \w_j} & = \lambda \cdot \w_j + \frac{1}{N} \sum_{n=1}^N \frac{1}{K_+^n \cdot K_-^n} \sum_{i:y_i=1} 
%                                         \frac{\x\pb{n}} {1 + \exp(\w_i^\top \x\pb{n} - \w_j^\top \x\pb{n})}.
%\end{align}

To compute the derivative of weight vector $\w_k, \, k \in \{1,\cdots,K\}$, we note that $\w^\top \w = \sum_{k=1}^K \w_k^\top \w_k$ and 
\begin{equation}
\label{eq:grad_of_obj}
\frac{\partial J(\w)} {\partial \w_k} = \lambda \w_k + \frac{1}{N} \sum_{n=1}^N \frac{\partial \LCal(\x^n, \y^n; \w)} {\partial \w_k}.
\end{equation}
We further note that 
\begin{equation}
\label{eq:grad_decomp}
\frac{\partial \LCal(\x, \y; \w)} {\partial \w_k} =
\begin{cases}
\frac{\partial \LCal_+(\x, \y; \w)} {\partial \w_k} = \frac{1}{K_+ K_-} \underset{j:y_j=0}{\sum} \, \frac{-\x} {1 + \exp(\w_k^\top \x - \w_j^\top \x)}, 
    & \text{if} \ y_k=1 \\
\frac{\partial \LCal_-(\x, \y; \w)} {\partial \w_k} = \frac{1}{K_+ K_-} \underset{i:y_i=1}{\sum} \, \frac{\x} {1 + \exp(\w_i^\top \x - \w_k^\top \x)},
    & \text{if} \ y_k=0
\end{cases}
\end{equation}
which can be summarised as
\begin{equation}
\label{eq:grad_of_loss}
\frac{\partial \LCal(\x^n, \y^n; \w)} {\partial \w_k} =
\frac{\partial \LCal_+(\x^n, \y^n; \w)} {\partial \w_k} \llb y_k^n=1 \rrb +
\frac{\partial \LCal_-(\x^n, \y^n; \w)} {\partial \w_k} \llb y_k^n=0 \rrb.
\end{equation}
%
By Eq.~(\ref{eq:grad_of_obj}), (\ref{eq:grad_decomp}) and~(\ref{eq:grad_of_loss}), we have
\begin{equation}
\label{eq:grad_k}
\frac{\partial J(\w)} {\partial \w_k} = \lambda \w_k + \sum_{n=1}^N \frac{\x^n}{N K_+^n K_-^n} \left(
\underset{j:y_j^n=0}{\sum} \, \frac{-\llb y_k^n=1 \rrb} {1 + \exp(\w_k^\top \x^n - \w_j^\top \x^n)} +
\underset{i:y_i^n=1}{\sum} \, \frac{ \llb y_k^n=0 \rrb} {1 + \exp(\w_i^\top \x^n - \w_k^\top \x^n)} \right).
\end{equation}
%
We can rewrite Eq.~(\ref{eq:grad_k}) as
$$
\frac{\partial J(\w)} {\partial \w_k} = \lambda \w_k + \sum_{n=1}^N (a_n + b_n) \x^n
$$
where
\begin{align*}
a_n &= \frac{1}{N K_+^n K_-^n} \underset{j:y_j^n=0}{\sum} \, \frac{-\llb y_k^n=1 \rrb} {1 + \exp(\w_k^\top \x^n - \w_j^\top \x^n)} \\
b_n &= \frac{1}{N K_+^n K_-^n} \underset{i:y_i^n=1}{\sum} \, \frac{ \llb y_k^n=0 \rrb} {1 + \exp(\w_i^\top \x^n - \w_k^\top \x^n)}
\end{align*}



\subsubsection{p-norm push loss}
\label{sssec:pnorm}

The p-norm push loss of example $(\x, \y)$ given weights $\w$ is
\begin{equation}
\label{eq:loss_pnorm}
\LCal(\x, \y; \w) = \frac{1}{K_+} \sum_{i:y_i=1} \ell_+(\w_i^\top \x) + \frac{1}{K_-} \sum_{j:y_j=0} \ell_-(\w_j^\top \x),
\end{equation}
where $K_+$ and $K_-$ are defined as before, 
and for constant $p \gg 1$,
\begin{equation}
\begin{aligned}
\ell_+(v) & = \exp(-v), \\
\ell_-(v) & = \frac{1}{p} \exp(pv).
\end{aligned}
\end{equation}
When $p \to +\infty$, the above loss is equivalent to the top-push loss (with exponential surrogate).

Then the objective with p-norm push loss is
\begin{equation}
\label{eq:obj_pnorm_push}
J(\w) = \frac{\lambda}{2} \w^\top \w + \frac{1}{N} \sum_{n=1}^N \left( 
        \frac{1}{K_+^n} \sum_{i:y_i^n=1} \exp(-\w_i^\top \x^n) + 
        \frac{1}{K_-^n} \sum_{j:y_j^n=0} \frac{1}{p} \exp(p \cdot \w_j^\top \x^n) \right), 
\end{equation}
where $K_+^n$ and $K_-^n$ are defined as before.


%The gradient of the weight vector $\w_i$ for positive labels and $\w_j$ for negative labels are
%\begin{align}
%\label{eq:grad_rank_pos}
%\frac{\partial J(\w)}{\partial \w_i} & = \lambda \cdot \w_i + \frac{1}{N} \sum_{n=1}^N \frac{-\x\pb{n}}{K_+^n} \exp(-\w_i^\top \x\pb{n}), \\
%\frac{\partial J(\w)}{\partial \w_j} & = \lambda \cdot \w_j + \frac{1}{N} \sum_{n=1}^N \frac{\x\pb{n}}{K_-^n} \exp(p \cdot \w_j^\top \x\pb{n}).
%\end{align}

Similar to Section~\ref{sssec:rank}, we can compute the derivative of weight vector $\w_k, \, k \in \{1,\cdots,K\}$ as follows:
$$
\frac{\partial J(\w)} {\partial \w_k} = \lambda \w_k + \frac{1}{N} \sum_{n=1}^N \frac{\partial \LCal(\x^n, \y^n; \w)} {\partial \w_k}.
$$
where
\begin{equation}
\frac{\partial \LCal(\x^n, \y^n; \w)} {\partial \w_k} =
\begin{cases}
\frac{-\x^n}{K_+} \exp(-\w_k^\top \x^n),  & \text{if} \ y_k^n=1 \\
\frac{ \x^n}{K_-} \exp(p\w_k^\top \x^n),  & \text{if} \ y_k^n=0
\end{cases}
\end{equation}
as a result,
\begin{equation}
\label{eq:grad_pnorm}
\frac{\partial J(\w)} {\partial \w_k} = \lambda \w_k + \sum_{n=1}^N (a_n + b_n) \x^n,
\end{equation}
where
\begin{align*}
a_n &= \frac{-\llb y_k^n=1 \rrb} {N K_+^n} \exp( -\w_k^\top \x^n), \\
b_n &= \frac{ \llb y_k^n=0 \rrb} {N K_-^n} \exp(p \w_k^\top \x^n).
\end{align*}



\subsubsection{Top-push loss}
\label{sssec:tpush}

The top push loss of example $(\x, \y)$ given weights $\w$ is
\begin{equation}
\label{eq:tpush_loss}
\LCal(\x, \y; \w) = \frac{1}{K_+} \sum_{i:y_i=1} \llb \w_i^\top \x \le \underset{j:y_j=0}{\max} \, \w_j^\top \x \rrb,
\end{equation}
where $K_+$ is the number of positive labels in $\y$.

Let $\ell(\cdot)$ be a convex surrogate of the indicator function, our optimisation objective is
\begin{equation}
\label{eq:tpush_obj}
J(\w) = \frac{\lambda}{2} \w^\top \w + \frac{1}{N} \sum_{n=1}^N 
        \frac{1}{K_+^n} \sum_{i:y_i^n=1} \ell \left( \w_i^\top \x^n - \underset{j:y_j^n=0}{\max} \, \w_j^\top \x^n \right),
\end{equation}
where $K_+^n$ is the number of positive labels in $\y^n$.
The objective~(\ref{eq:tpush_obj}) is hard to optimise due to the inner maximisation,
we therefore resort to optimise its \emph{dual}.


\paragraph{Dual formulation}
Let
\begin{align*}
f_0(\w, \xibm)     &= \frac{\lambda}{2} \w^\top \w + \frac{1}{N} \sum_{n=1}^N \frac{1}{K_+^n} \sum_{i:y_i^n=1} \ell \left( \w_i^\top \x^n - \xi_n \right), \\
f_{n,j}(\w, \xibm) &= \w_j^\top \x^n - \xi_n, \ n \in \{1,\dots,N\}, \, j \in \{j: y_j^n = 0\}
\end{align*}
where $\xibm \in \R^{N}$ is a vector of $N$ slack variables.
%
Our optimisation problem can be rewritten as
\begin{equation}
\label{eq:tpush_opt}
\begin{aligned}
\min_{\w, \xibm} \ & f_0(\w, \xibm) \\
s.t.             \ & f_{n,j}(\w, \xibm) \le 0, \ n \in \{1,\dots,N\}, \, j \in \{j: y_j^n = 0\}
\end{aligned}
\end{equation}
%
For $\nubm_n \succeq 0, \, n \in \{1,\dots,N\}$, the \emph{Lagrangian} of problem (\ref{eq:tpush_opt}) is
\begin{equation}
\label{eq:tpush_lg}
\begin{aligned}
L(\w, \xibm, \nubm_{1:N}) 
&= f_0(\w, \xibm) + \sum_{n=1}^N \sum_{j:y_j^n=0} \nu_{n,j} \cdot f_{n,j}(\w, \xibm) \\
&= \frac{\lambda}{2} \w^\top \w + \left[ \frac{1}{N} \sum_{n=1}^N \frac{1}{K_+^n} \sum_{i:y_i^n=1} \ell \left( \w_i^\top \x^n - \xi_n \right) \right] +
   \sum_{n=1}^N \sum_{j:y_j^n=0} \nu_{n,j} \left( \w_j^\top \x^n - \xi_n \right)
\end{aligned}
\end{equation}
%
Note that the conjugate of the conjugate of a convex function is itself, \ie $f(\z) = f^{**}(\z) = \sup_{\y} \left( \z^\top \y - f^*(\y) \right)$, we have
\begin{equation}
\label{eq:conjugate}
\ell( \w_i^\top \x^n - \xi_n) = \sup_{\alpha_{n,i}} \left[ \left( \w_i^\top \x^n - \xi_n \right) \alpha_{n,i} - \ell^*(\alpha_{n,i}) \right],
\end{equation}
where $\ell^*(\cdot)$ is the conjugate of $\ell(\cdot)$.

By (\ref{eq:tpush_lg}) and (\ref{eq:conjugate}), the \emph{Lagrangian} becomes
\begin{align*}
L(\w, \xibm, \alphabm_{1:N}, \nubm_{1:N})
&= \frac{\lambda}{2} \w^\top \w + \left[ \frac{1}{N} \sum_{n=1}^N \frac{1}{K_+^n} \sum_{i:y_i^n=1} 
   \sup_{\alpha_{n,i}} \left[ \left( \w_i^\top \x^n - \xi_n \right) \alpha_{n,i} - \ell^*(\alpha_{n,i}) \right] \right] +
   \sum_{n=1}^N \sum_{j:y_j^n=0} \nu_{n,j} \left( \w_j^\top \x^n - \xi_n \right) \\
&= \underset{\alphabm_{1:N}}{\sup} \left[
   \frac{\lambda}{2} \w^\top \w + \sum_{n=1}^N \frac{1}{N K_+^n} \sum_{i:y_i^n=1} 
   \left[ \left( \w_i^\top \x^n - \xi_n \right) \alpha_{n,i} - \ell^*(\alpha_{n,i}) \right] +
   \sum_{n=1}^N \sum_{j:y_j^n=0} \nu_{n,j} \left( \w_j^\top \x^n - \xi_n \right) \right] \\
&= \underset{\alphabm_{1:N}}{\sup} \left[ 
   g(\w, \xibm, \alphabm_{1:N}, \nubm_{1:N}) -
   \sum_{n=1}^N \frac{1}{N K_+^n} \sum_{i:y_i^n=1} \ell^*(\alpha_{n,i}) \right],
\end{align*}
where 
\begin{align*}
g(\w, \xibm, \alphabm_{1:N}, \nubm_{1:N})
&= \frac{\lambda}{2} \w^\top \w + \sum_{n=1}^N \frac{1}{N K_+^n} \sum_{i:y_i^n=1} \left( \w_i^\top \x^n - \xi_n \right) \alpha_{n,i}  + 
   \sum_{n=1}^N \sum_{j:y_j^n=0} \nu_{n,j} \left( \w_j^\top \x^n - \xi_n \right), \\
&= \frac{\lambda}{2} \w^\top \w + \sum_{n=1}^N \left[ \frac{1}{N K_+^n} \sum_{i:y_i^n=1} \left( \w_i^\top \x^n - \xi_n \right) \alpha_{n,i}  + 
   \sum_{j:y_j^n=0} \nu_{n,j} \left( \w_j^\top \x^n - \xi_n \right) \right], \\
&= \frac{\lambda}{2} \w^\top \w + \sum_{n=1}^N t(\x^n, \y^n, \w, \xi_n, \alphabm_n, \nubm_n),
\end{align*}
and
\begin{equation}
\label{eq:tmp_func}
t(\x^n, \y^n, \w, \xi_n, \alphabm_n, \nubm_n) 
= \frac{1}{N K_+^n} \sum_{i:y_i^n=1} \left( \w_i^\top \x^n - \xi_n \right) \alpha_{n,i}  + 
  \sum_{j:y_j^n=0} \nu_{n,j} \left( \w_j^\top \x^n - \xi_n \right).
\end{equation}
%
The \emph{Lagrangian dual function} of problem (\ref{eq:tpush_opt}) is
\begin{equation}
\label{eq:tpush_dual_func}
\begin{aligned}
\inf_{\w, \xibm} \, L(\w, \xibm, \alphabm_{1:N}, \nubm_{1:N})
&= \inf_{\w, \xibm} \, \underset{\alphabm_{1:N}}{\sup} \left[ g(\w, \xibm, \alphabm_{1:N}, \nubm_{1:N}) -
   \sum_{n=1}^N \frac{1}{N K_+^n} \sum_{i:y_i^n=1} \ell^*(\alpha_{n,i}) \right] \\
&= \underset{\alphabm_{1:N}}{\sup} \, \inf_{\w, \xibm} \left[ g(\w, \xibm, \alphabm_{1:N}, \nubm_{1:N}) -
   \sum_{n=1}^N \frac{1}{N K_+^n} \sum_{i:y_i^n=1} \ell^*(\alpha_{n,i}) \right] \quad 
   \text{(assuming \emph{strong duality})} \\
&= \underset{\alphabm_{1:N}}{\max} \, \min_{\w, \xibm} \left[ g(\w, \xibm, \alphabm_{1:N}, \nubm_{1:N}) -
   \sum_{n=1}^N \frac{1}{N K_+^n} \sum_{i:y_i^n=1} \ell^*(\alpha_{n,i}) \right] \quad
   \text{(Eq.~\ref{eq:tpush_obj} is L2 regularised~\cite{shalev:2007})} \\
&= \underset{\alphabm_{1:N}}{\max} \left[ \min_{\w, \xibm} \, g(\w, \xibm, \alphabm_{1:N}, \nubm_{1:N}) -
   \sum_{n=1}^N \frac{1}{N K_+^n} \sum_{i:y_i^n=1} \ell^*(\alpha_{n,i}) \right].
\end{aligned}
\end{equation}
%
To solve the (unconstrained) inner minimisation, 
note that $\w^\top \w = \sum_{k=1}^K \w_k^\top \w_k$,
let the derivatives of $\w_k, \, k \in \{1,\cdots,K\}$ (weights for the $k$-th label) and $\xi_n, \, n \in \{1,\cdots,N\}$ be 0, \ie
\begin{equation}
\label{eq:grad_eq_zero}
\begin{aligned}
\frac{\partial g}{\partial \w_k} 
&= \lambda \w_k + \sum_{n=1}^N \frac{\partial t(\x^n, \y^n, \w, \xi_n, \alphabm_n, \nubm_n)} {\partial \w_k} = 0, \\
\frac{\partial g}{\partial \xi_n} 
&= \frac{\partial t(\x^n, \y^n, \w, \xi_n, \alphabm_n, \nubm_n)} {\partial \xi_n} = 0,
\end{aligned}
\end{equation}
where 
\begin{equation}
\label{eq:grad_tw}
\frac{\partial t(\x^n, \y^n, \w, \xi_n, \alphabm_n, \nubm_n)} {\partial \w_k} =
\begin{cases}
\frac{1}{N K_+^n} \alpha_{n,k} \x^n, \ & \text{if} \ y_k^n = 1 \\
\nu_{n,k} \x^n, \ & \text{if} \ y_k^n = 0
\end{cases}
\end{equation}
and 
\begin{equation}
\label{eq:grad_txi}
\frac{\partial t(\x^n, \y^n, \w, \xi_n, \alphabm_n, \nubm_n)} {\partial \xi_n} 
= \frac{1}{N K_+^n} \sum_{i:y_i^n=1} (-\alpha_{n,i}) + \sum_{j:y_j^n=0} (-\nu_{n,j})
\end{equation}
%
By Eq.~(\ref{eq:grad_eq_zero}), (\ref{eq:grad_tw}) and (\ref{eq:grad_txi}), we have
\begin{equation}
\begin{aligned}
\frac{\partial g}{\partial \w_k} 
&= \lambda \w_k + \sum_{n=1}^N \left( \frac{\llb y_k^n=1 \rrb}{N K_+^n} \alpha_{n,k} \x^n + \llb y_k^n=0 \rrb \nu_{n,k} \x^n \right) = 0\\
\frac{\partial g}{\partial \xi_n} 
&= \frac{1}{N K_+^n} \sum_{i:y_i^n=1} (-\alpha_{n,i}) + \sum_{j:y_j^n=0} (-\nu_{n,j}) = 0
\end{aligned}
\end{equation}
%
or equivalently
\begin{equation}
\label{eq:sol_wk}
\widetilde\w_k = -\frac{1}{\lambda} \sum_{n=1}^N \left( \frac{\llb y_k^n=1 \rrb \alpha_{n,k}}{N K_+^n} + \llb y_k^n=0 \rrb \nu_{n,k} \right) \x^n, \ k \in \{1,\cdots,K\}
\end{equation}
\begin{equation}
\label{eq:sol_xin}
\frac{1}{N K_+^n} \sum_{i:y_i^n=1} \alpha_{n,i} + \sum_{j:y_j^n=0} \nu_{n,j} = 0, \ n \in \{1,\cdots,N\}
\end{equation}
%
Note that we can rewrite Eq.~(\ref{eq:sol_xin}) as
\begin{equation}
\label{eq:sol_xin2}
\sum_{k=1}^K \left( \frac{\llb y_k^n=1 \rrb \alpha_{n,k}} {N K_+^n} + \llb y_k^n=0 \rrb \nu_{n,k} \right) = 0, \ n \in \{1,\cdots,N\}
\end{equation}
%
Let 
\begin{equation}
\label{eq:new_var}
\gamma_{n,k} = \frac{1}{N K_+^n} \llb y_k^n=1 \rrb \alpha_{n,k} + \llb y_k^n=0 \rrb \nu_{n,k}.
\end{equation}
%
Eq.~(\ref{eq:sol_wk}) and (\ref{eq:sol_xin2}) become
\begin{equation}
\label{eq:sol}
\begin{aligned}
\widetilde\w_k = -\frac{1}{\lambda} \sum_{n=1}^N \gamma_{n,k} \x^n, & \quad k \in \{1,\cdots,K\} \\
\sum_{k=1}^K \gamma_{n,k} = 0, & \quad n \in \{1,\cdots,N\}
\end{aligned}
\end{equation}
%
Thus, by Eq.~(\ref{eq:tmp_func}), we have
\begin{align*}
t(\x^n, \y^n, \w, \xi_n, \alphabm_n, \nubm_n) 
&= \frac{1}{N K_+^n} \sum_{i:y_i^n=1} \left( \w_i^\top \x^n - \xi_n \right) \alpha_{n,i} + \sum_{j:y_j^n=0} \nu_{n,j} \left( \w_j^\top \x^n - \xi_n \right) \\
&= \frac{1}{N K_+^n} \sum_{i:y_i^n=1} \alpha_{n,i} \w_i^\top \x^n - \frac{1}{N K_+^n} \sum_{i:y_i^n=1} \alpha_{n,i} \xi_n +
   \sum_{j:y_j^n=0} \nu_{n,j} \w_j^\top \x^n - \sum_{j:y_j^n=0} \nu_{n,j} \xi_n \\
&= \frac{1}{N K_+^n} \sum_{i:y_i^n=1} \alpha_{n,i} \w_i^\top \x^n + \sum_{j:y_j^n=0} \nu_{n,j} \w_j^\top \x^n -
   \xi_n \left( \frac{1}{N K_+^n} \sum_{i:y_i^n=1} \alpha_{n,i} + \sum_{j:y_j^n=0} \nu_{n,j} \right) \\
&= \frac{1}{N K_+^n} \sum_{i:y_i^n=1} \alpha_{n,i} \w_i^\top \x^n + \sum_{j:y_j^n=0} \nu_{n,j} \w_j^\top \x^n - 0 \quad 
   \text{(by Eq.~\ref{eq:sol_xin})} \\
&= \sum_{k=1}^K \left( \frac{\llb y_k^n=1 \rrb}{N K_+^n} \alpha_{n,k} \w_k^\top \x^n + \llb y_k^n=0 \rrb \nu_{n,k} \w_k^\top \x^n \right) \\ 
&= \sum_{k=1}^K \left( \frac{\llb y_k^n=1 \rrb \alpha_{n,k}}{N K_+^n} + \llb y_k^n=0 \rrb \nu_{n,k} \right) \w_k^\top \x^n \\
&= \sum_{k=1}^K \gamma_{n,k} \w_k^\top \x^n \quad \text{(by Eq.~\ref{eq:new_var})} \\
\end{align*}
%
As a result,
\begin{equation}
\label{eq:tpush_reduce_g}
\begin{aligned}
\min_{\w, \xibm} \, g(\w, \xibm, \alphabm_{1:N}, \nubm_{1:N}) 
&= \frac{\lambda}{2} \widetilde\w^\top \widetilde\w + \sum_{n=1}^N t(\x^n, \y^n, \widetilde\w, \xi_n, \alphabm_n, \nubm_n) \\
&= \frac{\lambda}{2} \sum_{k=1}^K \widetilde\w_k^\top \widetilde\w_k + 
   \sum_{n=1}^N \sum_{k=1}^K \gamma_{n,k} \widetilde\w_k^\top \x^n \\
&= \frac{\lambda}{2} \sum_{k=1}^K \widetilde\w_k^\top \widetilde\w_k + 
   \sum_{k=1}^K \sum_{n=1}^N \gamma_{n,k} \widetilde\w_k^\top \x^n \\
&= \frac{\lambda}{2} \sum_{k=1}^K \widetilde\w_k^\top \widetilde\w_k + \sum_{k=1}^K \widetilde\w_k^\top \left[ \sum_{n=1}^N \gamma_{n,k} \x^n \right] \\
&= \frac{\lambda}{2} \sum_{k=1}^K \widetilde\w_k^\top \widetilde\w_k + \sum_{k=1}^K \widetilde\w_k^\top (-\lambda \widetilde\w_k) \quad 
   \text{(by Eq.~\ref{eq:sol_wk})} \\
&= -\frac{\lambda}{2} \sum_{k=1}^K \widetilde\w_k^\top \widetilde\w_k
\end{aligned}
\end{equation}
%
Lastly, by Eq.~(\ref{eq:tpush_dual_func}), (\ref{eq:sol}) and (\ref{eq:tpush_reduce_g}), the \emph{Lagrangian dual problem} of (\ref{eq:tpush_opt}) is
\begin{align*}
\underset{\nubm_{1:N}}{\max} \, \inf_{\w, \xibm} \, L(\w, \xibm, \alphabm_{1:N}, \nubm_{1:N})
&= \underset{\alphabm_{1:N}, \, \nubm_{1:N}}{\max} \left[ \min_{\w, \xibm} \, g(\w, \xibm, \alphabm_{1:N}, \nubm_{1:N}) -
   \sum_{n=1}^N \frac{1}{N K_+^n} \sum_{i:y_i^n=1} \ell^*(\alpha_{n,i}) \right] \\
&= \underset{\alphabm_{1:N}, \, \nubm_{1:N}}{\max} \left[ \min_{\w, \xibm} \, g(\w, \xibm, \alphabm_{1:N}, \nubm_{1:N}) -
   \sum_{n=1}^N \sum_{k=1}^K \frac{\llb y_k^n = 1 \rrb}{N K_+^n} \ell^*(\alpha_{n,i}) \right] \\
&= \underset{\alphabm_{1:N}, \, \nubm_{1:N}}{\max} \left[ 
   -\frac{1}{2 \lambda} \sum_{k=1}^K \left( \sum_{n=1}^N \gamma_{n,k} \x^n \right)^\top \left( \sum_{n=1}^N \gamma_{n,k} \x^n \right)
   -\sum_{n=1}^N \sum_{k=1}^K \frac{\llb y_k^n = 1 \rrb}{N K_+^n} \ell^*(\alpha_{n,i}) \right]
\end{align*}
subject to constraints (\ref{eq:sol_xin2}) and $\nubm_n \succeq 0, \, n \in \{1,\dots,N\}$,
or equivalently
\begin{equation}
\label{eq:tpush_dual}
\begin{aligned}
\underset{\alphabm_{1:N}, \, \nubm_{1:N}}{\min} \ &
    \sum_{k=1}^K \left[ 
    \frac{1}{2 \lambda} \left( \sum_{n=1}^N \gamma_{n,k} \x^n \right)^\top \left( \sum_{n=1}^N \gamma_{n,k} \x^n \right) +
    \sum_{n=1}^N \frac{\llb y_k^n = 1 \rrb}{N K_+^n} \ell^*(\alpha_{n,i}) \right] \\
s.t. \ \quad & \sum_{k=1}^K \gamma_{n,k} = 0, \ n \in \{1,\dots,N\} \\
             & \nubm_n \succeq 0, \ n \in \{1,\dots,N\}
\end{aligned}
\end{equation}
where variables $\gamma_{n,k}$ are defined in Eq.~(\ref{eq:new_var}).
\\
Suppose we use the logistic loss $\ell(v) = \log(1 + e^{-v})$, the conjugate of $\ell(v)$ is
$$
\ell^*(u) 
= \sup_v \left[ uv - \ell(v) \right] 
= \sup_v \left[ uv - \log(1 + e^{-v}) \right]
$$
let 
$$
\frac{\partial \left[ uv - \log(1 + e^{-v}) \right]} {\partial v} 
= u - \frac{-e^{-v}} {1 + e^{-v}}
= 0
$$
we have $u = \frac{-1}{1 + e^v} \in [-1, 0)$ and $v = \log(1+u) - \log(-u)$.
Thus
$$
\ell^*(u) = u \left[ \log(1+u) - \log(-u) \right] - \log(\frac{1}{1+u}) = - u\log(-u) + (1+u) \log(1+u)
$$




\newpage
\thispagestyle{empty}

\section*{Interesting problems}

\subsection*{What precision metric that bipartite ranking is optimising?}

We can derive a precision metric from the loss function of bipartite ranking.
Given a training set with binary labels $\{\x\pb{n}, y\pb{n}\}_{n=1}^N$, 
if the rank loss is used, assuming a linear score function $f(\x) = \w^\top \x$, 
then the empirical loss on training set is 
\begin{equation*}
\frac{1}{N_+ \cdot N_-} \sum_{i:y\pb{i}=1} \sum_{j:y\pb{j}=0} \llb \w^\top\x\pb{i} \le \w^\top\x\pb{j} \rrb,
\end{equation*}
where $N_+$ and $N_-$ is the number of positive and negative examples training set respectively.
This function will results in a penalty if any positive example has a lower score than any negative example,
which leads to the following precision metric:
\begin{equation}
\label{eq:precision_rank}
P_\text{rank} = \frac{\text{\#True Postive}}{\text{\#Positive}} 
              = \frac{1}{N_+ \cdot N_-} \sum_{i:y\pb{i}=1} \sum_{j:y\pb{j}=0} \llb \w^\top\x\pb{i} > \w^\top\x\pb{j} \rrb,
\end{equation}
\ie the fraction of correctly predicted positive-negative example pairs among all positive-negative example pairs.

\paragraph{Comments}
{\it
\begin{itemize}
\item You need to define precision more precisely. What does ``\# positive'' mean? Precision is usually hard to write down as a single summation, since the fraction you are involves a numerator and denominator that both depend on the predictions.
\item Precision@K is a highly non-trivial measure to directly optimise. Even logistic regression doesn't optimise it directly. Rather, it solves a more general problem of estimating P(y = 1 | x), from which one can find a good threshold to make predictions with high precision@K.
\item You can view AUC as an average of the balanced error, where one uses all possible classification thresholds. This is something of ``folk-lore'', and (regrettably) the only explicit reference I know is Appendix A.5 of~\cite{menon2015learning}.
\end{itemize}
}


\subsection*{What is the difference between logistic regression for binary classification and bipartite ranking?}

It is known that logistic regression is optimising the logistic loss derived from the maximum likelihood principle
$-\log \left( \p(y=1 |\x)^y \cdot (1 - \p(y=1 | \x)^{(1-y)} \right) = -y\log\p(y=1 | \x) - (1-y) \log(1-\p(y=1 | \x))$,
where $\p(y=1 | \x) = \sigma(\w^\top \x)$ is the probability that $\x$ is a positive example.
Logistic loss is a convex surrogate of the 0/1 loss for binary classification.
We can see from logistic loss that there will be a penalty if we predict a low probability for positive examples and 
a high probability for negative examples,
\ie logistic regression is trying to predict high probabilities for positive examples and low probabilities for negative examples,
which means it (indirectly) optimises the accuracy metric,
\begin{equation}
\label{eq:accuracy}
Acc = \frac{\text{\#True Postive} + \text{\#True Negative}}{\text{\#Example}} 
    = \frac{1}{N} \left( \sum_{i:y\pb{i}=1} \llb \w^\top \x\pb{i} \ge P_{Th} \rrb + \sum_{j:y\pb{j}=0} \llb \w^\top \x\pb{j} < P_{Th} \rrb \right),
\end{equation}
where $P_{Th}$ is the threshold probability.

An simple experiment to check this is filling Table~\ref{tab:precision} by training a logistic regression and a bipartite ranking algorithm (ranking loss and a convex surrogate) on a binary classification dataset.
\begin{table}[!h]
\centering
\begin{tabular}{c|cc} \hline \hline
Precision Metric                        & Logistic Regression & Bipartite Ranking \\
$Acc$~\ref{eq:accuracy}                 & \textsc{Best}       & ?                 \\
$P_\text{rank}$~\ref{eq:precision_rank} & ?                   & \textsc{Best}     \\ \hline
\end{tabular}
\caption{Evaluation results that compares logistic regression with bipartite ranking.}
\label{tab:precision}
\end{table}

\paragraph{Comments}
{\it
\begin{itemize}
\item You might get some insight on the relation between logistic regression and bipartite ranking from~\cite{kotlowski2011bipartite}.
\item It seems natural to consider structured SVM approaches to optimising precision and/or multilabel measures, as per SVMPerf.
\end{itemize}
}


\subsection*{How to use top-push loss for multi-label classification?}

For each multi-label example $(\x, \y)$, if we construct a training set $\{(\x, y_k)\}_{k=1}^K$ ($K$ is the total number of labels),
and aggregate over all multi-label training examples, we can then use top-push in multi-label classification. 
In particular, given training set $\{\x\pb{n}, \y\pb{n}\}_{n=1}^N$, the empirical risk is
\begin{equation*}
R(\w) = \frac{1}{N} \sum_{n=1}^N \LCal(\x\pb{n}, \y\pb{n}; \w),
\end{equation*}
where $\w = [\w_1^\top, \cdots, \w_K^\top]^\top$ is the flattened weights vector, and
\begin{equation}
%\label{eq:tpush_loss}
\LCal(\x, \y; \w) = \frac{1}{K_+} \sum_{i:y_i=1} \llb \w_i^\top \x \le \underset{j:y_j=0}{\max} \w_j^\top \w \rrb,
\end{equation}
and $K_+$ is the number of positive labels in $\y$.

%\begin{equation*}
%R(\w) = \frac{1}{N} \sum_{n=1}^N \frac{1}{K_+^n} \sum_{i:y_i^n=1} \llb \w_i^\top \x \le \underset{j:y_j^n=0}{\max} \w_j^\top \x \rrb,
%\end{equation*}
%where $K_+^n$ is the number of positive labels in $\y\pb{n}$.

Assuming L2 regularisation, the optimisation objective is
\begin{equation*}
J(\w) = \frac{\lambda}{2}\w^\top \w + R(\w).
\end{equation*}
To optimise this objective, we first replace the indicator function in $R(\w)$ with one of its convex surrogate, 
\eg the log loss $\log(1+e^{-z})$ where $z = \w_l^\top \x$ is the ranking score for a label.
We further derive the \emph{dual} of the objective to overcome the inner maximisation in $R(\w)$.
Finally, we do gradient descent to optimise the weights $\w$.

\paragraph{Comments}
{\it
\begin{itemize}
\item Does the overall dual decompose into a sum of duals, one for each label?
\end{itemize}
}



\subsection*{Active learning for structured prediction}

Popular models for multi-label classification and structured prediction such as the probabilistic classifier chains~\cite{dembczynski:2010},
and Seq2Seq models~\cite{Vinyals:2017} are limited by sequential inference. 
Inspired by structured SVM~\cite{taskar2004max,tsochantaridis2004support}, 
a value network was proposed to directly approximate the loss between the predicted and ground truth labels~\cite{gygli17a}.
To train the value network, it was shown that examples generated by inference as well as adversarial examples work best empirically. 

Obtain labelled examples is a typical example of the well-known exploration/exploitation trade-off, where there exist rich literatures in active learning and bandit algorithms, it is interesting to explore these strategies for training value networks.



%4. Evaluation measure (e.g. Precision@k, Average precision, Reciprocal precision)
\subsection{Evaluation measure}
Evaluation measure such as Precision@k, Average precision, Reciprocal precision can be used.
\\ \emph{describe details of the above measures}

%\bibliographystyle{ieeetr}
%\bibliographystyle{apalike}
\bibliographystyle{plainnat}
\bibliography{ref_mlc}

\end{document}
