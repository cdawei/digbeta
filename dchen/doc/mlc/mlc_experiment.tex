\section{Experiment}
\label{sec:experiment}

The settings that we focus in this work can be described as:
\begin{itemize}
\item The total number of labels is large,
\item the number of positive labels of examples, on average, is very small compared with the total number of labels.
\end{itemize}
and we are interested in modelling these positive labels.

\subsection{Multi-label classification}

We cast multi-label classification as ranking labels and experiment the proposed methods on several multi-label datasets~\cite{mulan2011}, 
that falls in the interested setting described above, the statistics of these datasets are shown in Table~\ref{tab:dataset}.

\begin{table}[!h]
\centering
%\setlength{\tabcolsep}{3pt} % tweak the space between columns
\caption{Statistics of multi-label dataset}
\label{tab:dataset}
\begin{tabular}{l*{4}{r}} \hline \hline
                                  & \texttt{bibtex} & \texttt{bookmarks} & \texttt{delicious} & \texttt{mediamill} \\ \hline 
\#Examples (train)                & 4,880           & 60,000             & 12,910             & 29,804         \\
\#Examples (test)                 & 2,515           & 27,856             & 3,181              & 12,373         \\
\#Features                        & 1,836           & 2,150              & 500                & 120            \\
\#Labels                          & 159             & 208                & 983                & 101            \\
\#Average Positive Labels (train) & 2.380 (1.50\%)  & 2.027 (0.97\%)     & 19.062 (1.94\%)    & 4.537 (4.49\%) \\
\#Average Positive Labels (test)  & 2.444 (1.54\%)  & 2.031 (0.98\%)     & 18.934 (1.93\%)    & 4.599 (4.55\%) \\
%Sparsity (train)                  & 1.497\%         & 0.974\%            & 1.939\%            & 4.492\%
%Sparsity (test)                   & 1.537\%         & 0.976\%            & 1.926\%            & 4.553\%
%
%Dataset & \#Examples (train) & \#Examples (test) & \#Features & \#Labels ($K$) & Avg $K_+$ (train) & Avg $K_+$ (test) & $K_+ / K$ \\ \hline
%\texttt{emotions} & 391  & 202  & 6  & 72  \\
%\texttt{yeast}     & 1,500  & 917    & 103  & 14  & 4  & 4  & 29\%  \\
%\texttt{scene}     & 1,211  & 1,196  & 294  & 6   & 1  & 1  & 17\%  \\ 
%\texttt{bibtex}    & 4,880  & 2,515  & 1,836 & 159 & 2  & 2  & 1.3\% \\
%\texttt{bookmarks} & 60,000 & 27,856 & 2,150 & 208 & 2  & 2  & 1\%   \\
%\texttt{delicious} & 12,910 & 3,181  & 500  & 983 & 19 & 18 & 2\%   \\
%\texttt{mediamill} & 29,804 & 12,373 & 120  & 101 & 4  & 4  & 4\%   \\
\hline
\end{tabular}
\end{table}

\noindent
We evaluate the performance of algorithms in terms of four metrics:
\begin{itemize}
\item Precision@3
\item Precision@5
\item Precision@K, where K is the number of positive labels for each example (\ie $K_+^n$)
\item RankingLoss
%\item F1 score
\end{itemize}

\noindent
The algorithms in consideration are:
\begin{itemize}
\item LR: Binary relevance (\ie independent logistic regression), regularisation constant $C = 1$.
\item LR-CV: Binary relevance (\ie independent Logistic regression), $C$ is tuned by cross validation.
\item TP: Multi-label classification with top-push loss, regularisation constant $C=1$.
\item TP-CV: Multi-label classification with top-push loss, $C$ is tuned by cross validation.
\item TP-NW: Multi-label classification with top-push loss, without normalise the loss by the number of positive labels (\ie $K_+$)
             in (\ref{eq:tpush_loss}), and $C$ is tuned by cross validation.
\item SPEN: Structured prediction energy networks~\cite{belanger2016structured}.
\item DVN: Deep value networks~\cite{gygli17a}.
\end{itemize}
The set of values for regularisation constant in cross validation is 
$\{ 10^{z}, 3 \times 10^{z} \}$ where $z \in \{-6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6\}$,
and values of $r$ (E.q.~\ref{eq:tpush_loss_approx}) are in $\{0.5, 1, 2, 4\}$.
The number of folds for cross validation is $5$.


\noindent
The performance of these algorithms are shown in Table~\ref{tab:perf}.

\input{tab_mulan}


\subsection{Music recommendation}

Another application of interest is music recommendation, in particular, recommending a small subset of songs from a large music library.
This problem is different from playlist generation%~\ref{} 
as the target here is a set instead of a list.

Table~\ref{tab:dataset_music} shows the preliminary results on the AotM-2011 music playlist dataset~\cite{mcfee2012},
where we only use the precomputed audio features (from MSD) of the first song in a playlist as the feature of a query.

\begin{table}[!h]
\centering
\caption{Statistics of music recommendation dataset}
\label{tab:dataset_music}
\begin{tabular}{l*{2}{r}} \hline \hline
                                  & \texttt{AotM-2011} & \texttt{30Music}  \\ \hline 
\#Examples (train)                & 63,849             & \\
\#Examples (test)                 & 31,449             & \\
\#Features (genres)               & 91                 & \\
\#Labels (songs)                  & 119,466            & \\
\#Average Positive Labels (train) & 9.365 (0.008\%)    & \\
\#Average Positive Labels (test)  & 9.337 (0.008\%)    & \\
\hline
\end{tabular}
\end{table}


\input{tab_music}
