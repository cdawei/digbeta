\section{Experiment}
\label{sec:experiment}

The settings that we focus in this work can be described as:
\begin{itemize}
\item The total number of labels is large,
\item the number of positive labels of examples, on average, is very small compared with the total number of labels.
\end{itemize}
and we are interested in modelling these positive labels.

\subsection{Multi-label classification}

We cast multi-label classification as ranking labels and experiment the proposed methods on several multi-label datasets~\cite{mulan2011}, 
that falls in the interested setting described above, the statistics of these datasets are shown in Table~\ref{tab:dataset}.

\begin{table}[!h]
\centering
%\setlength{\tabcolsep}{3pt} % tweak the space between columns
\caption{Statistics of multi-label dataset}
\label{tab:dataset}
\begin{tabular}{l*{4}{r}} \hline \hline
                                  & \texttt{bibtex} & \texttt{bookmarks} & \texttt{delicious} & \texttt{mediamill} \\ \hline 
\#Examples (train)                & 4,880           & 60,000             & 12,910             & 29,804         \\
\#Examples (test)                 & 2,515           & 27,856             & 3,181              & 12,373         \\
\#Features                        & 1,836           & 2,150              & 500                & 120            \\
\#Labels                          & 159             & 208                & 983                & 101            \\
\#Average Positive Labels (train) & 2.380 (1.50\%)  & 2.027 (0.97\%)     & 19.062 (1.94\%)    & 4.537 (4.49\%) \\
\#Average Positive Labels (test)  & 2.444 (1.54\%)  & 2.031 (0.98\%)     & 18.934 (1.93\%)    & 4.599 (4.55\%) \\
%Sparsity (train)                  & 1.497\%         & 0.974\%            & 1.939\%            & 4.492\%
%Sparsity (test)                   & 1.537\%         & 0.976\%            & 1.926\%            & 4.553\%
%
%Dataset & \#Examples (train) & \#Examples (test) & \#Features & \#Labels ($K$) & Avg $K_+$ (train) & Avg $K_+$ (test) & $K_+ / K$ \\ \hline
%\texttt{emotions} & 391  & 202  & 6  & 72  \\
%\texttt{yeast}     & 1,500  & 917    & 103  & 14  & 4  & 4  & 29\%  \\
%\texttt{scene}     & 1,211  & 1,196  & 294  & 6   & 1  & 1  & 17\%  \\ 
%\texttt{bibtex}    & 4,880  & 2,515  & 1,836 & 159 & 2  & 2  & 1.3\% \\
%\texttt{bookmarks} & 60,000 & 27,856 & 2,150 & 208 & 2  & 2  & 1\%   \\
%\texttt{delicious} & 12,910 & 3,181  & 500  & 983 & 19 & 18 & 2\%   \\
%\texttt{mediamill} & 29,804 & 12,373 & 120  & 101 & 4  & 4  & 4\%   \\
\hline
\end{tabular}
\end{table}

\noindent
We evaluate the performance of algorithms in terms of four metrics:
\begin{itemize}
\item Precision@3
\item Precision@5
\item Precision@K, where K is the number of positive labels for each example (\ie $K_+^n$)
\item RankingLoss
%\item F1 score
\end{itemize}

\noindent
The algorithms in consideration are:
\begin{itemize}
\item LR: Binary relevance (\ie independent logistic regression), regularisation constant $C = 1$.
\item LR-CV: Binary relevance (\ie independent Logistic regression), $C$ is tuned by cross validation.
\item TP: Multi-label classification with top-push loss, regularisation constant $C=1$.
\item TP-CV: Multi-label classification with top-push loss, $C$ is tuned by cross validation.
\item TP-NW: Multi-label classification with top-push loss, without normalise the loss by the number of positive labels (\ie $K_+$)
             in (\ref{eq:tpush_loss}), and $C$ is tuned by cross validation.
\end{itemize}
The set of values for regularisation constant in cross validation is 
$\{ 10^{z}, 3 \times 10^{z} \}$ where $z \in \{-6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6\}$,
and values of $r$ (E.q.~\ref{eq:tpush_loss_approx}) are in $\{0.5, 1, 2, 4\}$.
The number of folds for cross validation is $5$.


\noindent
The performance of these algorithms are shown in Table~\ref{tab:perf}.

\input{tab_mulan}


\subsection{Music recommendation}

Another application of interest is music recommendation, in particular, recommending a small subset of songs from a large music library.
This problem is different from playlist generation%~\ref{} 
as the target here is a set instead of a list.

To begin with, let $\{\x^i, \y^i\}_{i=1}^N$ the training set where $\x^i$ is the feature vector of the $i$-th \emph{query},
and $\y^i$ is a binary vector that encode the set of recommended songs for query $\x^i$ (\ie 1 for recommended, 0 otherwise).
Here we make a assumption that the set of recommended songs for a given query is only a small portion of the music library,
\ie $\y^i$ is a sparse vector (which could make independent logistic regression and a naive predictor that recommends all songs won't perform good)

There are many options to specify what a \emph{query} is and its features, for example,
\begin{itemize}
\item a query is a seed song, and its feature is the feature of the seed song,
\item or a query is tuple (user, seed song), and its feature combines both the feature of the user and that of the seed song,
\item we can further add more constraints to the query such that the number of songs we want, 
      and genres or artists that should be covered by the recommendation.
\end{itemize}
where the seed song features can be its genre, age/year of release, artist, lyrics, audio features, album, user rating etc,
and user features can be his/her age, gender, occupation, preference (genre, artist, etc), music purchase history etc.

In summary, to recommend a set of songs from a music library, we learn a predictor with
\begin{itemize}
\item Input: query features
\item Output: a number of options such as a indicator vector, a distribution, a ranking over all songs in the music library.
\end{itemize}
To determine the number of recommended songs $k$, we can either specify $k$ in the query,
or fit a distribution of $k$ from training set, and sample the value of $k$ from this distribution for recommendation.
Further, for a ranking based method, we may require that there should be a margin between the ranking scores of recommended songs and 
those not recommended, thus implicitly specify the value of $k$.

Lastly, instead of modelling the ranks of positive labels as described above, 
we can alternatively formulate the music recommendation problem as a sequential prediction problem,
\ie sequentially predict each label/song given previously predicted labels/songs.

Table~\ref{tab:dataset_music} shows the preliminary results on the AotM-2011 music playlist dataset~\cite{mcfee2012},
where we only use the genre of the first song in a playlist as the feature of a query (one-hot encoding).

\begin{table}[!h]
\centering
\caption{Statistics of music recommendation dataset}
\label{tab:dataset_music}
\begin{tabular}{l*{2}{r}} \hline \hline
                                  & \texttt{AotM-2011} & \texttt{30Music}  \\ \hline 
\#Examples (train)                & 32,758             & \\
\#Examples (test)                 & 16,136             & \\
\#Features (genres)               & 15                 & \\
\#Labels (songs)                  & 84,574             & \\
\#Average Positive Labels (train) & 9.426 (0.01\%)     & \\
\#Average Positive Labels (test)  & 9.454 (0.01\%)     & \\
\hline
\end{tabular}
\end{table}


\input{tab_music}
