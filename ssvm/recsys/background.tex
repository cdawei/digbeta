% !TEX root=main.tex

Fix some set $\PCal$ of points-of-interest (POIs) in a city.
A \emph{trajectory} is any sequence of POIs, possibly containing loops (repeated POIs).
In the \emph{trajectory recommendation} problem, we are given as input a training set of trajectories visited by travellers in the city.
From this, we wish to design a \emph{trajectory recommender}, which accepts a
\emph{trajectory query} $\mathbf{x} = (s, l)$, comprising a start POI $s \in \PCal$, and trip length $l \!>\! 1$ (\ie the desired number of POIs, including $s$),
and produces one or more sequences of POIs that conform to the query.

Formally, let $\XCal \defEq \PCal \times \{ 2, 3, \ldots \}$ be the set of possible queries,
$\YCal \defEq \bigcup_{l = 2}^\infty \PCal^l$ be the set of all possible trajectories,
and for fixed $\x \in \XCal$, $\YCal_{\x} \subset \YCal$ be the set of trajectories that conform to the constraints imposed by $\mathbf{x}$.
Then, the {trajectory recommendation} problem is:

\

{\sc Input}: training set $\{ ( \x^{(i)}, \y^{(i)} ) \}_{i = 1}^n$ %$\{ ( \x^{(i)}, \{ \y^{(i,j)} \}_{j=1}^{n_i} ) \}_{i=1}^{n}$ of historical trajectories

{\sc Output}: a trajectory recommender $r \colon \XCal \to \YCal$ 

\

One way to design a trajectory recommender is to find a scoring function $f \colon \XCal \times \YCal \to \mathbb{R}$, and let
\begin{equation}
	\label{eqn:argmax}
	r( x ) \defEq \argmax_{\mathbf{y} \in \YCal_x}~f(\mathbf{x}, \mathbf{y}).
\end{equation}
%In particular, $\mathbf{y} = (s,~ y_2, \dots, y_l)$ is a trajectory with $l$ POIs. %, which has no sub-tours. %i.e. $y_j \ne y_k$ if $j \ne k$.
%This was the view proposed in~\cite{cikm16paper} where they authors considered an
%objective function that added two components together: a POI score and a transition score.
Several choices of $f$ are possible.
In \citet{cikm16paper}, the authors proposed to use the output of a RankSVM model, combined with a transition score between POIs.
While offering strong performance, this method has a conceptual disadvantage highlighted in the previous section:
it does not model global cohesion, and thus can lead to solutions such as recommending three restaurants in a row.
To overcome this, in \citet{Chen:2017}, the authors proposed a structured SVM solution.
In the case of a structured SVM with linear potentials, the optimisation in Equation \ref{eqn:argmax} can be solved with the classic Viterbi algorithm.

However, we now wish to solve a modified problem:
given a new $\x \in \XCal$, find $\argmax_{\y \in \bar{\YCal}} F( \x, \y )$,
where $\bar{\YCal}$ comprises all sequences in $\YCal$ that are loop-free.

{\color{red!75}
\begin{itemize}
	%\item connect to workshop
	%\item distinguish between next location vs whole trajectory
	%\item define word usage: trajectory, path, walk, sequence, tour, etc.
	\item describe relation to travelling salesman, and say why different
	%\item contributions of this paper
\end{itemize}
}

% Now, our training set of historical trajectories may be written as
% $\{ ( \x^{(i)}, \{ \y^{(i,j)} \}_{j=1}^{n_i} ) \}_{i=1}^{n}$,
% where each $\x^{(i)}$ is a distinct query
% with $\{ \y^{(i,j)} \}_{j=1}^{n_i}$ the corresponding \emph{set} of observed trajectories.
% Note that we expect most queries to have several distinct trajectories;
% minimally,
% for example,
% there may be two nearby POIs that are visited in interchangeable order by different travellers.
% We are also interested in predicting paths $\y$, since it is unlikely a user will want to visit the same location twice.

% Let us suppose that characteristics of a tourist are summarised in some feature space $\XCal$,
% and sequences of a fixed length are represented by a label space $\YCal$.
% Given some affinity model $F \colon \XCal \times \YCal \to \mathbb{R}$ (\eg the output of a structured SVM), the standard inference problem is:
% given a new $\x \in \XCal$, find $\argmax_{\y \in \YCal} F( \x, \y )$.
% In the case of a structured SVM with linear potentials, this can be solved with the classic Viterbi algorithm.


% Consider the following general
% \emph{structured recommendation} problem:
% given an input query $\x \in \XCal$ (representing \eg a location, or some ``seed'' song)
% we wish to recommend one or more \emph{structured outputs} $\y \in \YCal$ (representing \eg a sequence of locations, or songs)
% according to a learned \emph{score function} $f(\x,\y)$.
% To learn $f$,
% we are provided as input a training set
% $\{ ( \x^{(i)}, \{ \y^{(i,j)} \}_{j=1}^{n_i} ) \}_{i=1}^{n}$,
% comprising a collection of inputs $\x^{(i)}$ with an associated \emph{set} of $n_i$ output structures $\{ \y^{(i,j)} \}_{j=1}^{n_i}$.

% For this work, we assume the output $\y$ is a \emph{sequence} of $l$ points, denoted $y_{1:l}$
% where each $y_i$ belongs to some fixed set (\eg places of interest in a city, or a collection of songs).
% We call the resulting specialisation the \emph{sequence recommendation} problem,
% and this shall be our primary interest in this paper.
% In many settings, one further requires the sequences to be \emph{paths}, \ie not contain any repetitions.

% Existing approaches treat the problem as one of determining a score for the intrinsic appeal of each POI.
% For example, a RankSVM model
% which
% learns to predict whether a POI is likely to appear ahead of another POI in a trajectory corresponding to some query~\cite{cikm16paper}.
% Formally,
% from the given set of trajectories
% we derive a training set
% $\{ ( \x^{(i)}, \mathbf{r}^{(i)} ) \}_{i = 1}^n$,
% where for each trajectory query $\x^{(i)}$ there is a list of ranked POI pairs
% $\mathbf{r}^{(i)} \subseteq \PCal \times \PCal$
% such that
% $(p, p') \in \mathbf{r}^{(i)}$
% iff
% POI $p$ appears more times than POI $p'$ in all trajectories associated with $\x^{(i)}$. %according to some notion.
% The training objective is then
% \begin{align}
% \resizebox{0.7\linewidth}{!}{$\displaystyle
% \displaystyle \min_{\w} ~\frac{1}{2} \w^\top \w + C \cdot \sum_{i = 1}^n \sum_{(p, p') \in \mathbf{r}^{(i)}}
% \ell\left( \w^\top ( \Psi( \x^{(i)}, p ) - \Psi( \x^{(i)}, p' ) ) \right),
% $} \label{eq:ranksvm}
% \end{align}
% for
% feature mapping $\Psi$,
% regularisation strength $C$ % > 0$,
% and squared hinge loss $\ell( v ) = \max( 0, 1 - v )^2$.
