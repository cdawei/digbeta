Given a hidden Markov model with parameters $\lambda = (A, B, \pi)$,
$A = \{a_{ij}\}$, $B = \{b_j(v_k)\}$, $\pi = \{\pi_i\}$, 
state sequence $\q = q_{1:T} = q_1 q_2 \cdots q_T, \, q_t \in \{s_i\}_{i=1}^N$,
and observation sequence $O_{1:T} = O_1 O_2 \cdots O_T, \, O_t \in \{v_k\}_{k=1}^M$ and
\begin{equation}
\label{eq:notebasic}
\begin{aligned}
a_{ij}   & \doteq \p(q_{t+1} = s_j |q_{t} = s_i), & 1 \le i,j \le N \\
b_j(v_k) & \doteq \p(v_k |s_j),                   & 1 \le j \le N, \, 1 \le k \le M   \\
\pi_i    & \doteq \p(q_1 = s_i),                  & 1 \le i \le N.
\end{aligned}
\end{equation}

% describe three problems: compute likelihood; identity the best sequence; training HMM using EM

\section{The forward procedure and Viterbi algorithm}
\label{sec:forward}

The likelihood of observation sequence $O_{1:T}$ is
\begin{align*}
\p(O_{1:T} ;\lambda) 
&= \sum_{i} \p(O_{1:T}, q_T = s_i ;\lambda) \\
&= \sum_{i} \left[ \sum_{j} \p(O_{1:T-1}, q_{T-1} = s_j ;\lambda) \cdot \p(q_T = s_i |q_{T-1} = s_j) \right] \p(O_T |q_T = s_i) \\
&= \sum_{i} \left[ \sum_{j} \p(O_{1:T-1}, q_{T-1} = s_j ;\lambda) \cdot a_{ji} \right] b_i(O_T)
\end{align*}
where we use the simplified notation $\p(O_{1:t}, q_t = s_i ;\lambda)$ to denote $\sum_{q_{1:t-1}} \p(O_{1:t}, q_{1:t-1}, q_t = s_i ;\lambda)$,
%\begin{equation}
%\label{eq:notesimp}
%\p(O_{1:t}, q_t = s_i ;\lambda) \doteq \sum_{q_{1:t-1}} \p(O_{1:t}, q_{1:t-1}, q_t = s_i ;\lambda).
%\end{equation}
and let
\begin{equation}
\label{eq:alpha}
\alpha_t(i) = \p(O_{1:t}, q_t = s_i ;\lambda) \doteq \sum_{q_{1:t-1}} \p(O_{1:t}, q_{1:t-1}, q_t = s_i ;\lambda),
\end{equation}
then the likelihood of $O_{1:T}$ is
\begin{alignat}{2}
\p(O_{1:T} ;\lambda) 
&= \sum_{i} \p(O_{1:T}, q_T = s_i ;\lambda) 
 = \blue{\sum_{i} \alpha_T(i)}  \label{eq:likelihood-forward} \\
&= \sum_{i} \left[ \sum_{j} \p(O_{1:T-1}, q_{T-1} = s_j ;\lambda) \right] a_{ji} \cdot b_i(O_T) 
 = \blue{\sum_{i} \left[ \sum_{j} \alpha_{T-1}(j) \cdot a_{ji} \right] b_i(O_T)}. \nonumber
\end{alignat}
We can repeat the above procedure to decompose $\alpha_{T-1}(j)$, in general, we have
\begin{equation}
\label{eq:forward-sum}
\alpha_t(i) = \begin{cases}
               \left[ \sum_{j} \alpha_{t-1}(j) \cdot a_{ji} \right] b_i(O_t), & t=2,\dots,T \\
               \pi_i \cdot b_i(O_t), & t=1.
              \end{cases}
\end{equation}

Computing the $\alpha_t(i)$ values by Eq.~(\ref{eq:forward-sum}) is known as the \emph{forward procedure}, 
and the likelihood of observation sequence $O_{1:T}$ can be computed using Eq.~(\ref{eq:likelihood-forward}).

To compute the highest probability state sequence $q_{1:T}^*$ given observations $O_{1:T}$, we have
\begin{equation*}
q_{1:T}^* = \argmax_{q_{1:T}} \p(O_{1:T}, q_{1:T} ;\lambda).
\end{equation*}
Observe that 
\begin{align*}
\max_{q_{1:T}} \p(O_{1:T}, q_{1:T} ;\lambda) 
&= \max_{i} \left\{ \max_{q_{1:T-1}} \p(O_{1:T}, q_{1:T-1}, q_T = s_i ;\lambda) \right\} \\
&= \max_{i} \left\{ \max_{j} \left\{ \max_{q_{1:T-2}} \p(O_{1:T-1}, q_{1:T-2}, q_{T-1} = s_j ;\lambda) \cdot 
   \p(q_T = s_i |q_{T-1} = s_j) \right\} \p(O_T |q_T = s_i) \right\} \\
&= \max_{i} \left\{ \max_{j} \left\{ \max_{q_{1:T-2}} \p(O_{1:T-1}, q_{1:T-2}, q_{T-1} = s_j ;\lambda) \cdot a_{ji} \right\} b_i(O_T) \right\}
\end{align*}
and let 
\begin{equation}
\label{eq:alphat}
\alphat_{t}(i) = \max_{q_{1:t-1}} \p(O_{1:t}, q_{1:t-1}, q_t = s_i ;\lambda),
\end{equation}
we have
\begin{align*}
\max_{q_{1:T}} \p(O_{1:T}, q_{1:T} ;\lambda) 
&= \max_{i} \left\{ \max_{q_{1:T-1}} \p(O_{1:T}, q_{1:T-1}, q_T = s_i ;\lambda) \right\} 
 = \blue{\max_{i} \alphat_{T}(i)} \\
&= \max_{i} \left\{ \max_{j} \left\{ \max_{q_{1:T-2}} \p(O_{1:T-1}, q_{1:T-2}, q_{T-1} = s_j ;\lambda) \cdot a_{ji} \right\} b_i(O_T) \right\}
 = \blue{\max_{i} \left\{ \max_{j} \left\{ \alphat_{T-1}(j) \cdot a_{ji} \right\} b_i(O_T) \right\}}.
\end{align*}
We can repeat the above procedure to decompose $\alphat_{T-1}(j)$, in general, we have
\begin{equation}
\label{eq:forward-max}
\alphat_t(i) = \begin{cases}
                \left[ \max_{j} \alphat_{t-1}(j) \cdot a_{ji} \right] b_i(O_t), & t=2,\dots,T \\
                \pi_i \cdot b_i(O_t), & t=1.
               \end{cases}
\end{equation}

Computing the $\alphat_t(i)$ values by Eq.~(\ref{eq:forward-max}) is known as the \emph{Viterbi algorithm}, 
and the/a sequence with highest probability can be identified via back-tracking.

Compare Eq.~(\ref{eq:forward-sum}) with Eq.~(\ref{eq:forward-max}), 
the only difference is that Eq.~(\ref{eq:forward-max}) uses \emph{maximisation} instead of \emph{summation} (used in Eq.~\ref{eq:forward-sum}).


\section{The backward procedure}
\label{sec:backward}

Similar to the forward procedure and the Viterbi algorithm, 
we can compute the likelihood of observations $O_{1:T}$ and identity the state sequence with highest probability by computing backwards.
By making use of conditional independences encoded by the HMM:
\begin{alignat}{2}
& q_t \perp q_{t' \notin \{t-1,t\}} \mid q_{t-1} \text{~and~} q_t \perp O_{1:T} \mid q_{t-1}, \, \forall t=2,\dots,T  \label{eq:indep1} \\
& O_t \perp q_{t' \ne t} \mid q_t \text{~and~} O_t \perp O_{t' \ne t} \mid q_t, \, \forall t=1,\dots,T                \label{eq:indep2}
\end{alignat}
The likelihood of $O_{1:T}$ is 
\begin{align*}
\p(O_{1:T} ;\lambda) 
&= \sum_{q_{1:T}} \p(O_{1:T},q_{1:T} ;\lambda) \\
&= \sum_{i} \sum_{q_{2:T}} \p(q_1 = s_i, q_{2:T}, O_{1:T} ;\lambda) \\
&= \sum_{i} \left[ \sum_{q_{2:T}} \p(q_{2:T}, O_{2:T} |q_1 = s_i, \lambda) \right] \p(q_1 = s_i) \cdot \p(O_1 |q_1 = s_i) 
   ~~~ \text{(by Eq.~\ref{eq:indep1} and Eq.~\ref{eq:indep2})} \\
%&= \sum_{q_{2:T}} \sum_{i} \p(q_{2:T}, O_{2:T} |q_1 = s_i, \lambda) \cdot \pi_i \cdot b_i(O_1) \\
&= \sum_{i} \left[ \sum_{j} \sum_{q_{3:T}} \p(q_2 = s_j, q_{3:T}, O_{2:T} |q_1 = s_i, \lambda) \right] \pi_i \cdot b_i(O_1) \\
&= \sum_{i} \left[ \sum_{j} \left[ \sum_{q_{3:T}} \p(q_{3:T}, O_{3:T} |q_2 = s_j, \lambda) \right] \p(q_2 = s_j |q_1 = s_i) \cdot \p(O_2 |q_2 = s_j) 
   \right] \pi_i \cdot b_i(O_1) 
   ~~~ \text{(by Eq.~\ref{eq:indep1} and Eq.~\ref{eq:indep2})}
%&= \sum_{i} \left[ \sum_{j} \left[ \sum_{q_{3:T}} \p(q_{3:T}, O_{3:T} |q_2 = s_j, \lambda) \right] a_{ij} \cdot b_j(O_2) \right] \pi_i \cdot b_i(O_1) 
\end{align*}

%Let $\beta_t(i) = \p(O_{t+1:T} |q_t = s_i, \lambda)$ with notation simplification
%\p(O_{t+1:T} |q_t = s_i, \lambda) \doteq \sum_{q_{t+1:T}} \p(q_{t+1:T}, O_{t+1:T} |q_t = s_i, \lambda),

Let 
\begin{equation}
\label{eq:beta}
\beta_t(i) = \p(O_{t+1:T} |q_t = s_i, \lambda) \doteq \sum_{q_{t+1:T}} \p(q_{t+1:T}, O_{t+1:T} |q_t = s_i, \lambda),
\end{equation}
with notation simplification similar to Eq.~(\ref{eq:alpha}), we have
\begin{alignat}{2}
\p(O_{1:T} ;\lambda) 
&= \sum_{i} \left[ \sum_{q_{2:T}} \p(q_{2:T}, O_{2:T} |q_1 = s_i, \lambda) \right] \pi_i \cdot b_i(O_1) 
 = \blue{\sum_{i} \beta_1(i) \cdot \pi_i \cdot b_i(O_1)}  \label{eq:likelihood-backward} \\
&= \sum_{i} \left[ \sum_{j} \left[ \sum_{q_{3:T}} \p(q_{3:T}, O_{3:T} |q_2 = s_j, \lambda) \right] a_{ij} \cdot b_j(O_2) \right] \pi_i \cdot b_i(O_1) 
 = \blue{\sum_{i} \left[ \sum_{j} \beta_2(j) \cdot a_{ij} \cdot b_j(O_2) \right] \pi_i \cdot b_i(O_1)}  \nonumber
\end{alignat}
We can repeat the above procedure to decompose $\beta_2(j)$, in general, we have
\begin{equation}
\label{eq:backward-sum}
\beta_t(i) = \begin{cases}
              1, & t=T \\
              \sum_{j} \beta_{t+1}(j) \cdot a_{ij} \cdot b_j(O_{t+1}), & t=T\!-\!1 \downto 1.
             \end{cases}
\end{equation}

Computing the $\beta_t(i)$ values by Eq.~(\ref{eq:backward-sum}) is known as the \emph{backward procedure}, 
and the likelihood of observation sequence $O_{1:T}$ can be computed using Eq.~(\ref{eq:likelihood-backward}).

Similarly, to identity the highest probability state sequence $q_{1:T}^*$ given observations $O_{1:T}$, 
we observe that 
\begin{align*}
\max_{q_{1:T}} \p(O_{1:T}, q_{1:T} ;\lambda) 
&= \max_{i} \max_{q_{2:T}} \p(q_1 = s_i, q_{2:T}, O_{1:T} ;\lambda) \\
&= \max_{i} \left\{ \max_{q_{2:T}} \p(q_{2:T}, O_{2:T} |q_1 = s_i, \lambda) \right\} \p(q_1 = s_i) \cdot \p(O_1 |q_1 = s_i) 
   ~~~ \text{(by Eq.~\ref{eq:indep1} and Eq.~\ref{eq:indep2})} \\
&= \max_{i} \left\{ \max_{j} \max_{q_{3:T}} \p(q_2 = s_j, q_{3:T}, O_{2:T} |q_1 = s_i, \lambda) \right\} \pi_i \cdot b_i(O_1) \\
&= \max_{i} \left\{ \max_{j} \left\{ \max_{q_{3:T}} \p(q_{3:T}, O_{3:T} |q_2 = s_j, \lambda) \right\} \p(q_2 = s_j |q_1 = s_i) \cdot \p(O_2 |q_2 = s_j) 
   \right\} \pi_i \cdot b_i(O_1) 
   ~~~ \text{(by Eq.~\ref{eq:indep1} and Eq.~\ref{eq:indep2})}
\end{align*}
Let 
\begin{equation}
\label{eq:betat}
\betat_t(i) = \max_{q_{t+1:T}} \p(q_{t+1:T}, O_{t+1:T} |q_t = s_i, \lambda),
\end{equation}
we have
\begin{align*}
\max_{q_{1:T}} \p(O_{1:T}, q_{1:T} ;\lambda) 
&= \max_{i} \left\{ \max_{q_{2:T}} \p(q_{2:T}, O_{2:T} |q_1 = s_i, \lambda) \right\} \pi_i \cdot b_i(O_1) 
 = \blue{\max_{i} \betat_1(i) \cdot \pi_i \cdot b_i(O_1)} \\
&= \max_{i} \left\{ \max_{j} \left\{ \max_{q_{3:T}} \p(q_{3:T}, O_{3:T} |q_2 = s_j, \lambda) \right\} a_{ij} \cdot b_j(O_2) \right\} \pi_i \cdot b_i(O_1) 
 = \blue{\max_{i} \left\{ \max_{j} \betat_2(j) \cdot a_{ij} \cdot b_j(O_2) \right\} \pi_i \cdot b_i(O_1)}  \nonumber
\end{align*}
We can repeat the above procedure to decompose $\betat_2(j)$, in general, we have
\begin{equation}
\label{eq:backward-max}
\betat_t(i) = \begin{cases}
              1, & t=T \\
              \max_{j} \betat_{t+1}(j) \cdot a_{ij} \cdot b_j(O_{t+1}), & t=T\!-\!1 \downto 1.
             \end{cases}
\end{equation}

Compare Eq.~(\ref{eq:backward-sum}) with Eq.~(\ref{eq:backward-max}), 
the only difference is that Eq.~(\ref{eq:backward-max}) uses \emph{maximisation} instead of \emph{summation} (used in Eq.~\ref{eq:backward-sum}).
It is also interesting to compare Eq.~(\ref{eq:forward-max}) with Eq.~(\ref{eq:backward-max}), 
the latter can be treated as the Viterbi algorithm works backwards, 
and the/a sequence with highest probability can be identified via \emph{forward}-tracking.


\section{The list Viterbi algorithm}
\label{sec:lva}

Let $\gamma_t(i,j)$ denote the probability of being in state $s_i$ at time $t$, and state $s_j$ at time $t\!+\!1$, 
as well as observations $O_{1:T}$, given the model parameters $\lambda$, \ie
\begin{equation}
\label{eq:gamma}
\begin{aligned}
\gamma_t(i,j) 
&= \sum_{q_{1:t-1,t+2:T}} \p(O_{1:T}, q_{1:t-1}, q_t = s_i, q_{t+1} = s_j, q_{t+2:T} ;\lambda) \\
&= \left[ \sum_{q_{1:t-1}} \p(O_{1:t}, q_{1:t-1}, q_t = s_i ;\lambda) \right] \p(q_{t+1} = s_j |q_t = s_i) \cdot \p(O_{t+1} |q_{t+1} = s_j) 
   \left[ \sum_{q_{t+2:T}} \p(O_{t+2:T}, q_{t+2:T} |q_{t+1} = s_j, \lambda) \right] \\
&= \alpha_t(i) \cdot a_{ij} \cdot b_j(O_{t+1}) \cdot \beta_{t+1}(j) 
   ~~~ \text{(by Eq.~\ref{eq:notebasic}, \ref{eq:alpha} and \ref{eq:beta})}.
\end{aligned}
\end{equation}

While $\gamma_t(i,j)$ is the marginal joint probability of all possible observation-state pairs $(O_{1:T}, q_{1:t-1} s_i s_j q_{t+2:T})$ given the model,
we can also compute the maximum joint probability over all possible observation-state pairs $(O_{1:T}, q_{1:t-1} s_i s_j q_{t+2:T})$, \ie
\begin{equation}
\label{eq:gammat}
\begin{aligned}
\gammat_t(i,j) 
&= \max_{q_{1:t-1,t+2:T}} \p(O_{1:T}, q_{1:t-1}, q_t = s_i, q_{t+1} = s_j, q_{t+2:T} ;\lambda) \\
&= \left\{ \max_{q_{1:t-1}} \p(O_{1:t}, q_{1:t-1}, q_t = s_i ;\lambda) \right\} \p(q_{t+1} = s_j |q_t = s_i) \cdot \p(O_{t+1} |q_{t+1} = s_j) 
   \left\{ \max_{q_{t+2:T}} \p(O_{t+2:T}, q_{t+2:T} |q_{t+1} = s_j, \lambda) \right\} \\
&= \alphat_t(i) \cdot a_{ij} \cdot b_j(O_{t+1}) \cdot \betat_{t+1}(j) 
   ~~~ \text{(by Eq.~\ref{eq:notebasic}, \ref{eq:alphat} and \ref{eq:betat})}.
\end{aligned}
\end{equation}
Similarly, we can fix the state only at time $t$, \ie $q_t = s_i$, 
and the maximum joint probability is
\begin{equation}
\label{eq:gammat0}
\begin{aligned}
\gammat_t(i) 
&= \max_{q_{1:t-1,t+1:T}} \p(O_{1:T}, q_{1:t-1}, q_t = s_i, q_{t+1:T} ;\lambda) \\
&= \left\{ \max_{q_{1:t-1}} \p(O_{1:t}, q_{1:t-1}, q_t = s_i ;\lambda) \right\}  
   \left\{ \max_{q_{t+1:T}} \p(O_{t+1:T}, q_{t+1:T} |q_t = s_i, \lambda) \right\} \\
&= \alphat_t(i) \cdot \betat_t(i) 
   ~~~ \text{(by Eq.~\ref{eq:notebasic}, \ref{eq:alphat} and \ref{eq:betat})}.
\end{aligned}
\end{equation}

Note that the joint probability of state sequence $q_{1:T}$ and observations $O_{1:T}$ is
\begin{equation}
\label{eq:jointdef}
\p(O_{1:T}, q_{1:T} ;\lambda) = \p(q_1) \cdot \p(O_1 |q_1) \left[ \prod_{t=1}^{T-1} \p(q_{t+1} |q_t) \cdot \p(O_{t+1} |q_{t+1}) \right],
\end{equation}
and that
\begin{equation}
\label{eq:jointp}
\begin{aligned}
\frac{\prod_{t=1}^{T-1} \gammat_t(q_t, q_{t+1})} {\prod_{t=2}^{T-1} \gammat_t(q_t)} 
&= \frac{\prod_{t=1}^{T-1} \alphat_t(q_t) \cdot \p(q_{t+1} |q_t) \cdot \p(O_{t+1} |q_{t+1}) \cdot \betat_{t+1}(q_{t+1})}
        {\prod_{t=2}^{T-1} \alphat_t(q_t) \cdot \betat_t(q_t)} 
   ~~~ \text{(by Eq.~\ref{eq:gammat} and \ref{eq:gammat0})} \\
&= \left[ \prod_{t=1}^{T-1} \p(q_{t+1} |q_t) \cdot \p(O_{t+1} |q_{t+1}) \right]
   \frac{ \left[ \prod_{t=1}^{T-1} \alphat_t(q_t) \right] \left[ \prod_{t=2}^T \betat_t(q_t) \right] }
        {\prod_{t=2}^{T-1} \alphat_t(q_t) \cdot \betat_t(q_t)} \\
&= \left[ \prod_{t=1}^{T-1} \p(q_{t+1} |q_t) \cdot \p(O_{t+1} |q_{t+1}) \right] \alphat_1(q_1) \cdot \betat_T(q_T) \\
&= \left[ \prod_{t=1}^{T-1} \p(q_{t+1} |q_t) \cdot \p(O_{t+1} |q_{t+1}) \right] \p(q_1) \cdot \p(O_1 |q_1) \cdot 1 
   ~~~ \text{(by Eq.~\ref{eq:forward-max} and \ref{eq:backward-max})} \\
%&= \p(q_1) \cdot \p(O_1 |q_1) \left[ \prod_{t=1}^{T-1} \p(q_{t+1} |q_t) \cdot \p(O_{t+1} |q_{t+1}) \right] \\
&= \p(O_{1:T}, q_{1:t} ;\lambda) ~~~ \text{(by Eq.~\ref{eq:jointdef})}.
\end{aligned}
\end{equation}

From Eq.~(\ref{eq:gammat}) and (\ref{eq:gammat0}), we note that 
\begin{equation}
\label{eq:maxconsist}
\max_{q_{t+1}} \gammat_t(q_t, q_{t+1}) = \gammat_t(q_t) 
~~~ \text{and} ~~~
\max_{q_t} \gammat_t(q_t, q_{t+1}) = \gammat_{t+1}(q_{t+1}),
\end{equation}
which is known as the \emph{max-consistency} property~\cite{nilsson2001sequentially}.
%and we have the following lemma,

\begin{lemma}
\label{lm:1}
Let $E$ be a set of state sequences given by $E = \{\q :q_1 = q_1^*, \dots, q_i = q_i^* \}$, then
\begin{equation*}
\max_{\q \in E} \p(O_{1:T}, q_{1:T}) = \frac{\prod_{t=1}^{i-1} \gammat_t(q_t^*, q_{t+1}^*)} {\prod_{t=2}^{i-1} \gammat_t(q_t^*)}.
\end{equation*}
\end{lemma}

\begin{proof}
By Eq.~(\ref{eq:jointp}) we have
\begin{equation*}
\max_{\q \in E} \p(O_{1:T}, q_{1:T}) 
= \max_{\q \in E} \frac{\prod_{t=1}^{T-1} \gammat_t(q_t, q_{t+1})} {\prod_{t=2}^{T-1} \gammat_t(q_t)}  
= \frac{\prod_{t=1}^{i-1} \gammat_t(q_t^*, q_{t+1}^*)} {\prod_{t=2}^{i-1} \gammat_t(q_t^*)} 
  \max_{q_{i+1:T}} \frac{\prod_{t=i}^{T-1} \gammat_t(q_t, q_{t+1})} {\prod_{t=i}^{T-1} \gammat_t(q_t)},
\end{equation*}
then consider the maximisation term
\begin{align*}
\max_{q_{i+1:T}} \frac{\prod_{t=i}^{T-1} \gammat_t(q_t, q_{t+1})} {\prod_{t=i}^{T-1} \gammat_t(q_t)}
&= \max_{q_{i+1:T-1}} \left\{ \frac{\prod_{t=i}^{T-2} \gammat_t(q_t, q_{t+1})} {\prod_{t=i}^{T-1} \gammat_t(q_t)}
   \max_{q_{T}} \gammat_{T-1}(q_{T-1}, q_T) \right\} \\
&= \max_{q_{i+1:T-1}} \left\{ \frac{\prod_{t=i}^{T-2} \gammat_t(q_t, q_{t+1})} {\prod_{t=i}^{T-1} \gammat_t(q_t)}
   \gammat_{T-1}(q_{T-1}) \right\} 
   ~~~ \text{(by Eq.~\ref{eq:maxconsist})} \\
&= \max_{q_{i+1:T-1}} \left\{ \frac{\prod_{t=i}^{T-2} \gammat_t(q_t, q_{t+1})} {\prod_{t=i}^{T-2} \gammat_t(q_t)} \right\}
 = \max_{q_{i+1:T-2}} \left\{ \frac{\prod_{t=i}^{T-3} \gammat_t(q_t, q_{t+1})} {\prod_{t=i}^{T-2} \gammat_t(q_t)}
   \max_{q_{T-1}} \gammat_{T-2}(q_{T-2}, q_{T-1}) \right\} \\
&= \max_{q_{i+1:T-2}} \left\{ \frac{\prod_{t=i}^{T-3} \gammat_t(q_t, q_{t+1})} {\prod_{t=i}^{T-2} \gammat_t(q_t)}
   \gammat_{T-2}(q_{T-2}) \right\}
   ~~~ \text{(by Eq.~\ref{eq:maxconsist})} \\
&= \max_{q_{i+1:T-2}} \left\{ \frac{\prod_{t=i}^{T-3} \gammat_t(q_t, q_{t+1})} {\prod_{t=i}^{T-3} \gammat_t(q_t)} \right\}
 = \max_{q_{i+1:T-3}} \left\{ \frac{\prod_{t=i}^{T-4} \gammat_t(q_t, q_{t+1})} {\prod_{t=i}^{T-3} \gammat_t(q_t)}
   \max_{q_{T-2}} \gammat_{T-3}(q_{T-3}, q_{T-2}) \right\} \\
&= \cdots \\
&= \max_{q_{i+1:i+2}} \left\{ \frac{\prod_{t=i}^{i+1} \gammat_t(q_t, q_{t+1})} {\prod_{t=i}^{i+1} \gammat_t(q_t)} \right\} \\
&= \max_{q_{i+1:i+2}} \left\{ \frac{\gammat_i(q_i^*, q_{i+1}) \cdot \gammat_{i+1}(q_{i+1}, q_{i+2})} 
                                   {\gammat_i(q_i^*) \cdot \gammat_{i+1}(q_{i+1})} \right\}
 = \max_{q_{i+1}} \left\{ \frac{\gammat_i(q_i^*, q_{i+1})} 
                               {\gammat_i(q_i^*) \cdot \gammat_{i+1}(q_{i+1})} 
                          \max_{q_{i+2}} \gammat_{i+1}(q_{i+1}, q_{i+2}) \right\} \\
&= \max_{q_{i+1}} \left\{ \frac{\gammat_i(q_i^*, q_{i+1})} 
                               {\gammat_i(q_i^*) \cdot \gammat_{i+1}(q_{i+1})} \gammat_{i+1}(q_{i+1}) \right\}
   ~~~ \text{(by Eq.~\ref{eq:maxconsist})} \\
&= \max_{q_{i+1}} \left\{ \frac{\gammat_i(q_i^*, q_{i+1})} {\gammat_i(q_i^*)} \right\} \\
&= 1 ~~~ \text{(by Eq.~\ref{eq:maxconsist})}
\end{align*}
and the result follows.
\end{proof}

\subsection{Find the second best sequence}
\label{ssec:2ndbest}

Suppose $\q^* = q_{1:T}^*$ is the state sequence with highest probability,
to find the second best sequence, we have
\begin{align*}
&\max_{\q \ne \q^*} \p(q_{1:T}, O_{1:T}; \lambda) \\
&= \max_{q_{1:T} \ne q_{1:T}^*} \p(q_{1:T}, q_{T+1} = s_{T+1}^*, O_{1:T+1}) \\
&= \max \left\{
    \max_{q_{1:T-1}, \, q_T \ne q_T^*} \p(q_{1:T-1}, q_T \ne q_T^*, q_{T+1} = s_{T+1}^*, O_{1:T}), \,
    \max_{q_{1:T-1} \ne q_{1:T-1}^*, \, q_T = q_T^*} \p(q_{1:T-1}, q_T = q_T^*, q_{T+1} = s_{T+1}^*, O_{1:T+1}) 
   \right\} \\
&= \max \left\{
   \max_{s_i \ne q_T^*} \left\{ \max_{q_{1:T-1}} \p(q_{1:T-1}, q_T = s_i, O_{1:T}) \right\} 
        \p(s_{T+1}^* |s_i) \p(O_{T+1} | s_{T+1}^*), \,
   \max_{q_{1:T-1} \ne q_{1:T-1}^*} \p(q_{1:T-1}, q_T = q_T^*, O_{1:T}) \p(s_{T+1}^* |q_{T}^*) \p(O_{T+1} |q_{T+1}^*)
   \right\} \\
&= \max \left\{
   \max_{s_i \ne q_T^*} \alphat_{T}(i)
        \p(s_{T+1}^* |s_i) \p(O_{T+1} | s_{T+1}^*), \,
   \max_{q_{1:T-1} \ne q_{1:T-1}^*} \p(q_{1:T-1}, q_T = q_T^*, O_{1:T}) \p(s_{T+1}^* |q_{T}^*) \p(O_{T+1} |q_{T+1}^*)
   \right\} 
\end{align*}
%Let $\delta_T = \max_{\q \ne \q^*} \p(q_{1:T}, O_{1:T}; \lambda)$

Alternatively, we have
\begin{align*}
&\max_{q_{1:T} \ne q_{1:T}^*} \p(q_{1:T}, O_{1:T}; \lambda) \\
&= \max \left\{
    \max_{q_1 \ne q_1^*, \, q_{2:T}} \p(q_1 \ne q_1^*, q_{2:T}, O_{1:T}), \,
    \max_{q_1 = q_1^*, \, q_{2:T} \ne q_{2:T}^*} \p(q_1 = q_1^*, q_{2:T}, O_{1:T+1}) 
   \right\} \\
&= \max \left\{
    \max_{s_i \ne q_1^*} \left\{ \max_{q_{2:T}} \p(q_{2:T}, O_{2:T} |q_1 = s_i) \p(O_1 |q_1 = s_i) \right\}, \,
    \p(q_1^*, O_1) \max_{q_{2:T} \ne q_{2:T}^*} \p(q_{2:T}, O_{2:T} |q_1 = q_1^*) 
   \right\}
\end{align*}

