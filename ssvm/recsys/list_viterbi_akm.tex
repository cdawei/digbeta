% !TEX root=main.tex

% \begin{itemize}
%     %\item use list viterbi to go down top-k best sequences until we find one without loops
%     %\item two versions of serial list viterbi are equivalent
%     %\item parallel list viterbi
% \end{itemize}

The Viterbi algorithm %for standard inference in a structured prediction model.
%This
finds the best scoring sequence in Equation \ref{eqn:argmax}, which may have loops.
To find the best sequence without loops in Equation \ref{eqn:argmax-path}, one can apply a \emph{list Viterbi algorithm}
to find not just the single best sequence,
but rather the top $K$ best sequences.
By definition, the first such sequence that is loop free must be the highest scoring sequence that does not have loops.
We detail existing approaches to solve the list Viterbi problem.

%
\subsection{Parallel and serial list Viterbi algorithms}

At a high level, there are two approaches to extend the Viterbi algorithm to the top-$K$ setting.
The first approach is to keep track, at each state, of the top $K$ sub-sequences that end at this state; these are known as \emph{parallel list Viterbi} algorithms.
Such algorithms date back to at least \citet{Forney:1973},
but have the disadvantage of imposing a non-trivial memory burden.
Further, they are not applicable as-is in our setting:
we do \emph{not} know in advance what value of $K$ is suitable,
since we do not know the position of the best loop-free sequence.

The second approach is to more fundamentally modify how one selects paths; these are known as \emph{serial list Viterbi} algorithms.
There are at least two such well-known proposals in the context of hidden Markov models (HMMs).
In the signal processing community, \citet{seshadri1994list} proposed an algorithm that keeps track of the ``next-best'' sequence terminating at each state in the current list of best sequences.
In the AI community, \citet{nilsson2001sequentially}
%(building on the work of \citet{Nilsson:1998} for general graphical models)
proposed an algorithm that cleverly partitions the search space into subsets of sequences that share a prefix with the current list of best sequences.
While derived in different communities, these two algorithms are in fact only superficially different, as we now see.

%
\subsection{Relating serial list Viterbi algorithms}

The connection between the two list Viterbi algorithms is easiest to see when finding the second-best sequence for an HMM.
Suppose we have an HMM with states $\SSf_{t}$, observations $\OSf_{t}$, transitions $a_{ij} = \Pr( \SSf_{t+1} = j \mid \SSf_t = i )$, and emissions $b_{ik} = \Pr( \OSf_{t} = k \mid \SSf_t = i )$.
Suppose $s^*_{1:T}$ is the most likely length $T$ sequence given observations $\OSf_{1:T}$, and
$\delta_{j, t}$ is the value of the best sequence up till position $t$ ending at state $j$ as computed by the Viterbi algorithm.
%$\delta_{j, t} \defEq \max_{\SSf_{1:t - 1}} \Pr( \SSf_{1:t - 1}, \SSf_t = j, \OSf_{1:t} )$

Our interest is in finding the second-best sequence %given observations $\OSf_{1:T}$,
with value
$ M \defEq \max_{\SSf_{1:T} \neq s^*_{1 : T}}{\Pr( \SSf_{1:T}, \OSf_{1:T} )}. $
\citet{seshadri1994list} proposed to find $M$ via a Viterbi-like recurrence,
\begin{equation}
    \label{eqn:att-recurrence}
    \begin{aligned}
        M &= \bar{\delta}_{T+1} \\
        %(\forall t > 0) \,
        \bar{\delta}_t &= 
        \indicator{t > 0} \cdot
        \max
        \begin{cases}
        \max_{i \neq s^*_{t-1}} \delta(i, t-1) \cdot a( i, s^*_{t} ) \cdot b( s^*_{t}, \OSf_{t} ) \\
        \bar{\delta}_{t - 1} \cdot a( s^*_{t - 1}, s^*_{t} ) \cdot b( s^*_{t}, \OSf_{t} ).
        \end{cases}% \\
        %\bar{\delta}_0 &= 0.
    \end{aligned}    
\end{equation}
Intuitively, $\bar{\delta}_t$ finds the value of the second-best sequence that merges with the best sequence by at least time $t$.

\setlength{\belowdisplayskip}{1pt} \setlength{\belowdisplayshortskip}{0pt}
\setlength{\abovedisplayskip}{1pt} \setlength{\abovedisplayshortskip}{0pt}

\citet{nilsson2001sequentially} proposed to find $M$ via another way,
\begin{align*}
	M &= \max_t \widehat{\rho}_t \\
	\widehat{\rho}_{t} &\defEq \max_{i \neq s^*_{t}} \max_{S_{t+1:T}} {\Pr( \SSf_{1:t-1} = s^*_{1:t-1}, \SSf_t = i, \SSf_{t+1:T}, \OSf_{1:T} )}.
\end{align*}
Intuitively, $\widehat{\rho}_t$ finds the value of the second-best sequence that first deviates from the best sequence exactly at time $t$.
One can compute $\widehat{\rho}_{t}$ using $\eta_{i, j, t} \defEq \max_{\SSf \colon S_{t - 1} = i, \SSf_{t} = j} \Pr( \SSf_{1:T}, \OSf_{1:T} )$ \citep{nilsson2001sequentially},
which in turn can be computed from the ``backward'' analogue of $\delta$.

To connect the two approaches, observe that by unrolling the recurrence in Equation \ref{eqn:att-recurrence}, and by definition of $\delta$, we have
\begin{align*}
	M &= \max_t \widehat{\mu}_t \\
	\widehat{\mu}_{t} &\defEq \prod_{k = 1}^t a( s^*_{k-1}, s^*_{k} ) \cdot b( s^*_{k}, \OSf_{k} ) \cdot \max_{i \neq s^*_{t}} \delta(i, t) \cdot a( i, s^*_{t+1} ) \cdot b( s^*_{t+1}, \OSf_{t+1} ) \\
	&= \max_{i \neq s^*_{t}} \max_{S_{1:t-1}} {\Pr( \SSf_{1:t-1}, \SSf_t = i, \SSf_{t+1:T} = s^*_{t+1:T}, \OSf_{1:T} )};
\end{align*}
i.e., the same quantities are computed, except that the former fixes the suffix of the candidate sequence, while the latter fixes the prefix.
A similar analysis holds in the case of finding the $K$th best sequence.

The complexity of either of the above algorithms is $\mathscr{O}( l \cdot |\PCal|^2 + l \cdot |\PCal| \cdot K + l \cdot K \cdot \log( l \cdot K ) )$ \citep{nilsson2001sequentially}.
This is tractable, but we emphasise that the smallest $K$ guaranteeing we obtain a path is unknown \emph{a-priori}.
% informally, we expect the required $K$ to be larger as the length $l$ increases,
% since here the search space grows exponentially.
