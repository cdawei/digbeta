% !TEX root=main.tex

% \begin{itemize}
%     %\item use list viterbi to go down top-k best sequences until we find one without loops
%     %\item two versions of serial list viterbi are equivalent
%     %\item parallel list viterbi
% \end{itemize}

Recall that one uses the Viterbi algorithm for standard inference in a structured prediction model.
This finds the single highest scoring sequence; unfortunately, it may have loops.
To find the best sequence without loops, one can apply a \emph{list Viterbi algorithm}
to find not just the single best sequence,
but rather the top $K$ best sequences.
By definition, the first such sequence that is loop free must be the highest scoring sequence that does not have loops.

%
%\subsection{List Viterbi algorithms}

At a high level, there are two ways to extend the Viterbi algorithm to the top-$K$ setting.
The first is to keep track, at each state, of the top $K$ paths that end at this state; these are known as \emph{parallel list Viterbi} algorithms.
Parallel list Viterbi algorithms date back to at least \citet{Forney:1973}.
However, they impose a memory burden; further, in our setting it is unknown how many paths will be required, as we do not know how many of the top $K$ sequences have loops.

The second is to more fundamentally modify how one selects paths; these are known as \emph{serial list Viterbi} algorithms.
Amongst serial list Viterbi algorithms, there are at least two well-known proposals.
In the signal processing community, \citet{seshadri1994list} proposed an algorithm that keeps track of the ``next best'' sequence terminating at each state in the current list of best sequences.
In the AI community, \citet{nilsson2001sequentially} (building on the work of \citet{Nilsson:1998} for general graphical models) proposed an algorithm that cleverly partitions the search space into subsets of sequences that share a prefix with the current list of best sequences.

While derived in different communities, the two approaches are in fact only superficially different.
This is easiest to see in the case of finding the second best sequence.
Suppose we have a hidden Markov model with states $\SSf_{t}$, observations $\OSf_{t}$, transition probabilities $a_{ij} = \Pr( \SSf_{t+1} = j \mid \SSf_t = i )$, and emission probabilities $b_{ik} = \Pr( \OSf_{t} = k \mid \SSf_t = i )$.
Suppose $s^*$ is the most likely sequence given observations $\OSf_{1:T}$,
and our interest is in finding the second best sequence.
To do, \citet{seshadri1994list} proposed the recurrence
\begin{align*}
	\max_{\SSf_{1:T} \neq s^*_{1 : T}}{\Pr( \SSf_{1:T}, \OSf_{1:T} )} &= \bar{\delta}_{T+1} \\
    (\forall t \in \{ 1, \ldots, T + 1 \}) \, \bar{\delta}_t &= 
    \max
    \begin{cases}
    \max_{i \neq s^*_{t-1}} \delta(i, t-1) \cdot a( i, s^*_{t} ) \cdot b( s^*_{t}, \OSf_{t} ) \\
    \bar{\delta}_{t - 1} \cdot a( s^*_{t - 1}, s^*_{t} ) \cdot b( s^*_{t}, \OSf_{t} )
    \end{cases} \\
    \bar{\delta}_0 &= 0.
\end{align*}
Intuitively, $\bar{\delta}_t$ finds the value of the second best sequence that merges with the best sequence by at least time $t$.
On the other hand, \citet{nilsson2001sequentially} proposed to find
\begin{align*}
	\max_{\SSf_{1:T} \neq s^*_{1 : T}}{\Pr( \SSf_{1:T}, \OSf_{1:T} )} &= \max_t \widehat{\rho}_t \\
	\widehat{\rho}_{t} &\defEq \max_{i \neq s^*_{t}} \max_{S_{t+1:T}} {\Pr( \SSf_{1:t-1} = s^*_{1:t-1}, \SSf_t = i, \SSf_{t+1:T}, \OSf_{1:T} )}.
\end{align*}
Intuitively, $\widehat{\rho}_t$ finds the value of the second best sequence that first deviates from the best sequence exactly at time $t$.

To connect the two approaches, observe that we can unroll the previous recurrence to give
\begin{align*}
	\max_{\SSf_{1:T} \neq s^*_{1 : T}}{\Pr( \SSf_{1:T}, \OSf_{1:T} )} &= \max_t \widehat{\mu}_t \\
	\widehat{\mu}_{t} &\defEq \prod_{k = 1}^t a( s^*_{k-1}, s^*_{k} ) \cdot b( s^*_{k}, \OSf_{k} ) \cdot \max_{i \neq s^*_{t}} \delta(i, t) \cdot a( i, s^*_{t+1} ) \cdot b( s^*_{t+1}, \OSf_{t+1} ) \\
	&= \max_{i \neq s^*_{t}} \max_{S_{1:t-1}} {\Pr( \SSf_{1:t}, \SSf_t = i, \SSf_{t+1:T} = s^*_{t+1:T}, \OSf_{1:T} )}.
\end{align*}
That is, the quantities being computed are the same, except that in the former we fix the suffix of the candidate sequence, while in the latter we fix the prefix.
