R1:

> Recommendations without personalization is more akin to a more general prediction tasks

We agree that personalisation is very important for recommendation when there are sufficient per user data. However, there are only 1 to 2 trajectories per user on average in our datasets, as shown in Table 1, which makes personalised recommendation impractical. That's why we decided to pursue recommending trajectories for an average user.

> would you expect that your method continues to outperform other baselines for longer trajectory length?

We agree that the average trajectory length is short in both datasets, but long trajectories do exist in our dataset (length > 5). When we look at the individual recommendations for these longer trajectories, our proposed model do outperform other baselines. We will mention this fact in our analysis of results.

> In Section 4.3 you mention that you use the ``best of top 10'' strategy. It seems like this only applies to your model. If that's the case, doesn't this give an unfair advantage to your model? i.e., at test time you wouldn't have this information.

The ``best of top 10'' strategy only applies to our proposed models. Our arguments are the proposed models can be treated as ranking trajectories, and we take the top-ranked items (i.e., trajectories) for evaluation. Baselines can be treated as ranking individual POIs according to some metric (i.e., a random number for "Random", POI popularity for "PoiPopularity" and  POI and query features for "PoiRank"), and we also take the top-ranked items (i.e., POIs) for evaluation.

> Are your methods the only ones to have access to the starting POI?

All methods shown in Table 2 have access to the starting POI; we will clarify this.

> playlist generation using recurrent neural network references

Thanks for the recommended papers; we'll cite them, and mention that, as stated, they require much more training data than we have available.

> Line 212: It's not clear what \bar{y}^{(i)} is

It should be \bar{y}; we'll fix this.


R2:

> limited novelty from a structured prediction perspective

Our focus in this paper is in using tools from structured prediction to solve a challenging and interesting problem in recommending sequences. Thus, we think it is of interest to the recommender system research community.

> For the multiple ground truths, why not just duplicate the examples as (x^(1),y^(11)) and (x^(1),y^(12))?

Duplicating the examples as proposed is exactly how "SP" and "SPpath" methods (Sec 4.2) deal with the multiple ground truths. The difference between this and the proposed "SR" and "SRpath" methods (Sec 4.2) is that the former can cause conflict when doing loss-augmented inference, e.g., the most violated label for example (x^(1),y^(11)) can be y^(12), and similarly, the result of loss-augmented inference for (x^(1),y^(12)) can be y^(11); but the latter explicitly requires that the result of loss-augmented inference for example (x^(1), {y^(11), y^(12)}) can be neither y^(11) nor y^(12), as described in (2).

> The restriction that \bar{y} is not in the observed labels (line 421) is not necessary

For the vanilla SSVM, this restriction is indeed not necessary, however, this restriction is essential for the proposed "SR" model, which makes sure the result of loss-augmented inference is not in the set of observed labels (as explained right above).

> loss-augmented inference remains hard despite the decomposition (in general).

We are aware that loss-augmented inference can be hard despite the decomposition in general, but our statement is in the context that the underlying structure is a sequence. In this case, efficient loss-augmented inference is indeed possible.

> Lines 220-228: it is worth mentioning that cutting plane is not the only optimization scheme suggested for problem (1).

We are aware of sub-gradient and Frank-Wolfe methods for training SSVM and will mention them; our decision to use cutting plane methods is due to the easier accessibility of cutting plane toolkit.

> For hard instances of the ILP, have you considered LP relaxation + some rounding heuristic?

We can solve all our hard instances in reasonable time using the state-of-art ILP solver. We'll mention that LP relaxation with rounding heuristics is an option in general.

> What do we lose if we represent the outputs in the u space (edge indicators) instead of y space (points along the path)? Will it not be simpler and save some of the complications

We agree that the outputs/recommendations can be represented in u space (edge indicators). However, to get the actual recommendation (i.e., a trajectory), it is necessary to perform a decoding in the u space. 

> It seems to me like the features (Psi) can be expressed in that space

Pairwise features (Psi_{j,k}) are indeed representing features of an edge, so they are corresponding to the edge indicators in u space by definition (i.e., Psi_{j,k} are features of edge u_{j,k}).

> The assumption on the form of the score function (line 330) as singletons and pairwise terms should be made clear much earlier 

Thanks for the suggestion; we'll clarify.

> Section 3.2 doesn’t add much over Section 2, and should probably be discarded.

Thanks. We admit and appreciate this suggestion.


R4:

> not clear what the relationship is between Viterbi and the output/predictions from the structured SVM

The output of SSVM prediction is a sequence that satisfies the given query (i.e., the "seed" item). The Viterbi algorithms and its extension (Sec 2.3) were used for the loss-augmented inference when training SSVM and also for test (i.e., prediction) using the trained SSVM. We'll clarify this.

> little discussion on how the feature mapping is done in the experiments.

We described features used in our experiment in the supplementary material (i.e., Table 1 and Table 2 in section A.2.). The "Random" baseline doesn't use any feature, "PoiPopularity" uses only the popularity of POIs, and "PoiRank" uses all features that were used in the proposed methods except the pairwise features, i.e., only uses features described in Table 1 in section A.2. We'll draw more attention to this in the main body.

> more details about the RankSVM model

The details of RankSVM is in the cited reference of (Chen et al., 2016). We'll add a brief summary of the method in Sec 4.2.

> more details about how the training and testing sets were constructed

Trajectories are first grouped by queries, we then perform leave-one-out cross validation over these groups, i.e., holding all trajectories that satisfy a specific query for test, and use all other trajectories for training.

> For baselines, I’m surprised the authors didn’t include some sort of markov chain model 

Markov chain models for sequence prediction are actually a weak baseline; (Chen et al., 2016) show that it is comprehensively outperformed by RankSVM. That's why we didn't include it in this paper. We will make a note of this fact.

> Table 2: What is k for these results?

k is 10 in Table 2 as described in Sec 4.3.

> Why not show results on different values of k?

Thanks. We'll fix this.

> I would like to see this on datasets that are either much larger in scale (i.e. travel itineraries from travel websites such as TripAdvisor?)
> or at least a benchmark on a dataset that is more widely known.

Thanks. We admit and appreciate this suggestion.
