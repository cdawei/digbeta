%!TEX root = main.tex

\secmoveup
\section{Introduction}
\label{sec:intro}
\textmoveup

Content recommendation has been the subject of a rich body of literature~\citep{Goldberg:1992,Sarwar:2001,Koren:2010},
with established techniques seeing widespread adoption in industry~\citep{Linden:2003,Agarwal:2013,Amatriain:2015,Gomez-Uribe:2015}.
The success of these methods is explained by both the explosion in availability of user's explicit and implicit preferences for content,
as well as the design of methods that can suitably exploit these to make useful recommendations~\citep{Koren:2009}.

%LX: uh, 'unstructured' isn't a very well known way to refer to classic recsys
%For the most part, recommendation models have been limited to the case of static, \emph{unstructured} content.
Classical recommendation systems have focused on a fixed set of individual items such as movies, dubbed here as recommending \emph{unstructured} content.
While this setting has considerable value,
in many important scenarios one needs to recommend content that possesses some \emph{structure}.
%In its simplest form, this structure may be manifest by
%A wide-reaching
An example of this is where the desired content is naturally organized in
\emph{sequences} or {\em graphs}.
For example, consider %the problem of
recommending a trajectory of points of interest in a city to a visitor~\citep{lu2010photo2trip,lu2012personalized,ijcai15,cikm16paper}, a playlist of songs~\citep{McFee:2011,chen2012playlist,hidasi2015session,choi2016towards},
a chemical compound~\cite{dehaspe1998finding} or a few linked websites for e-commerce~\cite{antikacioglu2015recommendation}.
%
%a playlist of songs to users based on their listening history~\citep{McFee:2011,chen2012playlist,hidasi2015session,choi2016towards}.
%LX - do we want to broaden the definition and down-play music playlist?

A na\"{i}ve approach to sequence recommendation is to ignore the structure.
%and learn a standard recommendation model.
% In the playlist example, we could learn a user's preference for individual songs,
% and then create a playlist based on the top ranked songs.
In the trajectory example, we could learn a user's preference for individual locations,
and create a trajectory based on the top ranked locations.
However, such an approach may be sub-optimal:
%This however completely ignores the fact that while a user's
for example,
in the trajectory recommendation problem, it is unlikely a user will want to visit three restaurants in a row.
Similarly,
while a user's two favourite songs might belong
the metal and country genres respectively,
it is questionable that a playlist featuring these songs in succession will be as enjoyable to the user.

The above raises the question of how one can effectively learn from such sequential content.
In this paper, we show how to cast sequence recommendation as a \emph{structured prediction} problem,
which allows us to leverage the toolkit of structured SVMs~\citep{tsochantaridis2005large}.
However, a vanilla application of such methods does not suffice,
as they do not account for the fact that each input can have multiple ground truths,
and that repeated elements in predicted sequences are undesirable.
We show how to overcome this by
incorporating multiple correct sequences into a structured prediction formulation,
%suitably normalising the loss function for the model,
%LX - what's the right word here?  i feel normalising is only summarizing part of what is done??
and by two novel applications of the list Viterbi algorithm that sequentially finds the list of top-scoring sequences.
%LX - marketing listViterbi
%modifying the inference and prediction steps using a variant of the Viterbi algorithm.
Specifically, our contributions are as follows:
\begin{itemize}[noitemsep,leftmargin=12pt]\itemmoveup
	\item We formalise the problem of sequence recommendation (\S\ref{sec:seqrec-defn}), and show how trajectory recommendation can be seen as a special case (\S\ref{sec:trajrec}).
	%cast it as a structured prediction task (\S\ref{sec:recseq}), and .

	\item We show how sequence recommendation may be attacked using structured SVMs (\S\ref{sec:recseq}).
	We propose one improvement of structured SVMs to the recommendation problem, so as to account for the existence of multiple ground truths for each input (\S\ref{ssec:sr}).
	%Following \citep{joachims2009cutting}, we propose both $n$-slack and 1-slack versions of the structured recommender.%\footnote{This new structured recommender can in principle be applied to any problem where loss augmented inference can be efficiently computed. We focus on sequence recommendation in this paper.}

    \item We propose two novel applications of the list Viterbi algorithm -- an extension of the classic Viterbi algorithm that sequentially finds the list of highest scored sequences under some model --
to exclude multiple ground truths for model learning (\S\ref{ssec:training}),
and to predict sequences without repeated elements, i.e. {\em path}s in state-space (\S\ref{ssec:testing}).
	%\item We show how one can avoid recommending sequences with loops or repetitions via integer linear programming and list Viterbi;
%an extension of the classic Viterbi algorithm that returns a list of the highest scored sequences under some model;
%%	We show that these algorithms may be incorporated during both the training %(via loss augmented inference)
%%	and prediction steps of our structured recommendation (\S\ref{ssec:training}, \S\ref{ssec:testing}).
	%LX - now we have only four bullet points, i'm happier with this -- 5 is too many!
	%LX - in addition, i do not think we can claim ILP as a contribution. it's in Lim'2015

	\item We present experiments on real-world trajectory recommendation problems, and demonstrate our structured recommendation approaches improve over existing non-structured baselines (\S\ref{sec:experiment}).\itemmoveup
\end{itemize}

%We begin with an overview of related work, before presenting our model.
%We begin with an overview of the sequence recommendation problem, before presenting our model.
