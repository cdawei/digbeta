% !TEX root = ./main.tex

\secmoveup
\section{Trajectory Recommendation as a Structured Prediction Problem}
\label{sec:trajrec}
\textmoveup

We present the trajectory recommendation problem,
which shall serve as our canonical application of sequential recommendation.
We first review the problem as it is typically treated in the literature,
and then show how it may be viewed as a structured prediction problem.

\secmoveup
\subsection{Trajectory recommendation: standard view}

Travel recommendation problems involve a set of points-of-interest (POIs) $\mathcal{P}$ in a city.
The \emph{trajectory recommendation} problem is: given a \emph{trajectory query} $\mathbf{x} = (s, l)$,
comprising a start POI $s \in \mathcal{P}$ and trip length
\footnote{Instead of specifying the number of desired POIs, 
we can constrain the trajectory with a total time budget $T$. 
In this case, the number of POIs $l$ can be treated as a \emph{hidden} variable, 
with additional constraint $\sum_{k=1}^l t_k \le T$ where $t_k$ is the time spent at POI $y_k$.}
$l > 1$ (including the start location $s$),
we want to recommend one or more sequences of POIs (or \emph{trajectory}) $\mathbf{y}^*$ that maximises some notion of utility.
To learn a suitable $f$ for trajectory recommendation,
we are provided as input a training set
%$(\x\pb{i}, \{ \y\pb{ij} \}_{j=1:n^i})$, $i=1:n$,
%comprising pairs of queries and corresponding
of trajectories visited by travellers in the city.

% AKM: not sure these are suitable here
%
% This problem is related to automatic playlist generation,
% where we recommend a sequence of songs given a specified song (a.k.a. the seed) and the number of new songs.
% Formally, given a library of songs and a query $\mathbf{x} = (s, K)$, where $s$ is the seed and $K$ is the number of songs in playlist,
% we produce a list with $K$ songs (without duplication) by maximising the likelihood~\cite{chen2012playlist},
% \begin{equation*}
% %\max_{(y_1,\dots,y_K)} \prod_{k=2}^K \mathbb{P}(y_{k-1} \given y_k),~ y_1 = s ~\text{and}~ y_j \ne y_k,~ j \ne k.
% \mathbf{y}^* = \argmax_{\mathbf{y} \in \mathcal{P}_\mathbf{x}}~ \mathbb{P}(\mathbf{y} \given \mathbf{x}),~ \mathbf{y} = (y_1=s,\dots,y_K)
% ~\text{and}~ y_j \ne y_k ~\text{if}~ j \ne k.
% \end{equation*}

% Another similar problem is choosing a small set of photos from a large photo library and compiling them into a slideshow or movie.

Existing approaches treat the problem as one of determining a score for the intrinsic appeal of each POI,
based on implicit preference data in the training set.
Perhaps the simplest such approach is based on learning a model to predict whether a particular POI occurs, or not, in the trajectory corresponding to a query.
Formally, from the given set of trajectories
we derive a training set $\{ ( \x^{(i)}, p, y^{(i)} ) \}_{i = 1}^n$,
where each POI $p$ is augmented with a label $y^{(i)} \in \{ \pm 1 \}$ denoting whether or not it appeared in a trajectory corresponding to the query $\x^{(i)}$. 
Formally, the objective used for training is
$$ \min_{\w} \frac{1}{2} \w^T \w + C \cdot \sum_{i = 1}^n \sum_{p \in \mathcal{P}} \ell\left( y^{(i)} \cdot ( \w^T \Phi( \x^{(i)}, p ) ) \right) $$
where
$\Phi( \cdot, \cdot )$ denotes a query-POI feature mapping,
$C > 0$ is some regularisation strength,
and $\ell( v )$ is any suitable margin loss, such as the logistic loss $\ell( v ) = \log( 1 + e^{-v} )$.
One can use the resulting model to produce a score for any POI $p$ given a new query $\x$,
and then pick the top-$l$ scoring POIs to produce a trajectory.

As another example, \citet{cikm16paper} proposed the use of the RankSVM model,
which %is fed as input %pairs $(p_i, p_j)$ of POIs such that $p_i$ appears ahead of $p_j$ in some trajectory.
learns a model to predict whether a POI is likely to appear ahead of another POI in a trajectory corresponding to some query.
Formally, the objective used for training is
%$$ \min_{\w} \frac{1}{2} \w^T \w + C \cdot \sum_{\mathbf{x} \in \mathcal{Q}} \sum_{p_i \succeq_{\mathbf{x}} p_j} \max\left( 0, 1 - \w^T ( \Phi( \x, p_i ), \Phi( \x, p_j ) ) \right)^2 $$

\resizebox{0.99\linewidth}{!}{
$\displaystyle \min_{\w} \frac{1}{2} \w^T \w + C \cdot \sum_{i = 1}^n \sum_{(p, p') \in \mathcal{R}( \x^{(i)} )} 
\ell\left( \w^T ( \Phi( \x^{(i)}, p ) - \Phi( \x^{(i)}, p' ) ) \right), $
}

where $\Phi, C$ are as before,
$\ell( v ) = \max( 0, 1 - v )^2$ is the squared hinge loss,
and
%$p \succeq_{\mathbf{x}} p'$ is denoted to mean that the
$\mathcal{R}( \x )$
denotes all pairs of POIs $(p, p')$ such that
POI $p$ is ranked above the POI $p'$ in the trajectories associated with $\x$ according to some notion.
For example, this may be computed by counting the number of times a POI appears in the trajectories associated with a query, 
and using this to determine which of two POIs is more suitable for a query.

While such approaches can certainly learn to recommend useful trajectories,
we observe that their modelling power is inherently limited:
they are not capable of ensuring the \emph{global} cohesion of a trajectory,
as they inherently rely on either pointwise or pairwise preferences for POIs.
For example, as argued in Section~\ref{sec:intro},
we might find three restaurants to be the highest scoring POIs in a city;
however, this it is unlikely that these three POIs form a trajectory that most travellers will enjoy.

This motivates an approach that directly ensures such global cohesion.
To do this, we show how the problem can be cast as one of sequential recommendation, 
thus allowing us to leverage the tools developed in the previous section.


%
\subsection{Trajectory recommendation: structured view}

More abstractly, we can cast trajectory recommendation as the following problem:
given some $\x$, and suitable function $f(\cdot,\cdot)$, we wish to find
\begin{equation*}
\mathbf{y}^* = \argmax_{\mathbf{y} \in \mathcal{Y}}~f(\mathbf{x}, \mathbf{y}),
\end{equation*}
where $\mathcal{Y}$ is the set of all possible trajectories with POIs in $\mathcal{P}$ that conform to the constraints imposed by the query $\mathbf{x}$.
In particular,
$\mathbf{y} = (y_1 = s,~ y_2, \dots, y_l)$ is a trajectory with $l$ POIs, which has no sub-tours i.e. $y_j \ne y_k$ if $j \ne k$.

Now, our training set of historical trajectories may be written as
$\{ ( \x\pb{i}, \{ \y\pb{ij} \}_{j=1}^{n_i} ) \}_{i=1}^{n}$,
where each $\x\pb{i}$ is a distinct query,
while the corresponding $\{ \y\pb{ij} \}_{j=1}^{n_i}$ is the set of trajectories observed for that query.
Note that we expect most queries to have several distinct trajectories;
minimally,
for example,
there may two nearby POIs that are visited in interchangeable order by different travellers.

Comparing the above to the discussion in the previous section, it is clear that
we can cast trajectory recommendation as a special case of the structured recommendation problem.
Consequently, we may approach it using structured prediction methods such as the SSVM,
as well as the extensions proposed to account for multiple ground truths and eliminate loops during training.

% AKM: repetitive
%
% standard SSVM
% We can learn a recommender by training a SSVM on the set of observed trajectories $\{\mathbf{x}^{(i')}, \mathbf{y}^{(i')}\}_{i'=1}^{N'}$,
% However, we ignore the fact that for the same query, we normally observed more than one trajectory,
% we would like to exploit this fact to better modelling the observed trajectories.

% AKM: repetitive
%
% \subsection{Query Aggregation}
% \label{sec:query}

% To modelling the fact that a given query has multiple observed trajectories, 
% we firstly group trajectories according to queries, in other words,
% we now have a dataset $\{\mathbf{x}^{(i)}, \{\mathbf{y}^{(ij)}\}_{j=1}^{N_i}\}_{i=1}^N$
% with $N$ queries and queries $\mathbf{x}^{(i)}$ has $N_i$ trajectories observed.


% \subsection{Recommendation with Multiset SSVM}
% \label{sec:trajrec-ssvm}

% We can learn to recommend trajectories by training a multiset SSVM described in Section~\ref{sec:ssvm-ms}
