%!TEX root = main.tex
\secmoveup
\section{A structured prediction approach to sequence recommendation}
\label{sec:recseq}
\textmoveup

We now show how to solve sequence recommendation problems using structured prediction.
We first provide background on structured SVMs (SSVM) (Sec~\ref{ssec:ssvm}),
then present a model for structured recommendation (Sec~\ref{ssec:sr}),
followed by algorithms for its learning (Sec~\ref{ssec:training}) 
and inference (Sec~\ref{ssec:testing}).


\secmoveup
\subsection{SSVM for structured prediction}
\label{ssec:ssvm}
\textmoveup

A structured prediction task involves predicting some structured label $\y \in \mathcal{Y}$ for an input $\x \in \mathcal{X}$,
typically via a score function $f(\x,\y)$ that determines the affinity of an (input, label) pair.
A popular model is the Structured Support Vector Machine (SSVM)~\cite{joachims2009predicting,tsochantaridis2005large}, comprising three core ingredients.

\emph{Score function}. In SSVMs, we specify that the score function $f(\x, \y)$ takes a linear form, \ie 
%is a function that scores the compatibility between features $\mathbf{x}$ and a specific label $\mathbf{y}$,
%in the case of structured SVM (SSVM), the compatibility function $f(\mathbf{x}, \mathbf{y})$ for structured SVM is this linear form,
%\begin{equation*}
$f(\x, \y) = \w^\top \Psi(\x, \y)$,
%\end{equation*}
where $\w$ is a weight vector, and $\Psi(\x, \y)$ is a \emph{joint feature map}.
%between the input $\x$ and label sequence $\y$.

The design of the feature map $\Psi$ is problem specific.
For sequence prediction problems,
where $\y = y_{1 : l}$,
%For many problems,
%we may assume that it
a popular choice
is a vector whose elements represent unary
terms for each element in the label $y_{1:l}$, and pairwise interactions between
adjacent
elements in the label
%For sequence data, %in particular, 
%we also
% We may further
% assume that the pairwise interactions are between
% adjacent elements,
i.e. $y_j$ and $y_{j+1}$ for $j=1 : l \!-\! 1$.
Subsequently, $f(\x,\y)$ decomposes into a weighted sum of
each of these features: %with the corresponding feature weight:
\begin{equation}
\label{eq:jointfeature}
%\resizebox{0.9\linewidth}{!}{$\displaystyle
f(\x, \y) = \w^\top \Psi(\x, \y) = 
\sum_{j=1}^l \w_j^\top \psi_j(\x, y_j) + \sum_{j=1}^{l-1} \w_{j,j+1}^\top \psi_{j,j+1}(\x, y_{j}, y_{j+1}).
%$}
\end{equation}
Here, $\psi_j$ is a feature map between the input and one output label element $y_j$, with a corresponding weight vector $\w_j$,
and $\psi_{j,j+1}$ is a pairwise feature vector that captures the interactions between consecutive labels $y_j$ and $y_{j+1}$,
with a corresponding weight vector $\w_{j,j+1}$.

%To learn the parameters, we train the structured SVM by optimising a quadratic program (QP),
\emph{Objective function}.
To learn a suitable weight vector $\w$, SSVMs solve: %the following optimisation problem:
\begin{equation}
\label{eq:nslack}
\resizebox{0.93\linewidth}{!}{$\displaystyle
\begin{aligned}
\min_{\w, \, \bm{\xi} \ge 0} ~\frac{1}{2} \w^\top \w + \frac{C}{n} \sum_{i=1}^n \xi_i,  ~~s.t.~  \forall i, 
  \w^\top \Psi(\x^{(i)}, \y^{(i)}) - \w^\top \Psi(\x^{(i)}, \bar{\y}) \ge
  \Delta(\y^{(i)}, \bar{\y}) - \xi_i, \, \forall \bar\y \in \mathcal{Y}.
\end{aligned}
$}
\end{equation}

Here, 
$\mathcal{Y}$ is the set of all possible sequences
and $\Delta(\y\pb{i}, \bar\y)$ is the loss function between $\bar\y$ and the ground truth $\y\pb{i}$,
and slack variable $\xi_i$ is the \emph{hinge loss} for the prediction of the $i$-th example~\cite{tsochantaridis2005large}.
To solve problem (\ref{eq:nslack}),
% one option is to simply enumerate all constraints, and feed the problem into a standard solver.
% However, this approach is impractical as there is a constraint for every possible label $\bar{\y}$.
%recently developed learning schemes such as 
cutting-plane algorithms~\cite{joachims2009predicting}, % cutting-plane
gradient-based algorithms~\cite{ratliff2006subgradient} % sub-gradient
and conditional gradient methods~\cite{lacoste2013block} % Frank-Wolfe
%can efficiently optimise the objective and
are widely used in practice~\cite{muller2014methods}.
%Instead, we can resort to a cutting-plane algorithm which repeatedly solves the quadratic program (\ref{eq:nslack})
%with a growing set of constraints~\cite{joachims2009predicting}.
%In each iteration, a new constraint is formed by solving the loss-augmented inference,
%which helps shrink the feasible region of the problem.


\emph{Loss-augmented inference}.
We can rewrite the constraints in (\ref{eq:nslack}) as 
$\w^\top \Psi(\x\pb{i}, \y\pb{i}) + \xi_i \ge 
\max_{\bar\y \in \mathcal{Y}} \left\{ \Delta(\y\pb{i}, \bar\y) + \w^\top \Psi(\x\pb{i}, \bar\y) \right\}$,
and the maximisation at the right side is known as the loss-augmented inference.
%refers to the inner maximisation in Eq.~(\ref{eq:hingeloss}).
%This formulation is called "$n$-slack" as we have one slack variable for each example in training set.
%Here the inner maximisation is known as the \emph{loss-augmented inference}.
When the underlying graph of SSVM is a tree (which is the case for sequence recommendation),
this may be done efficiently if the loss function $\Delta(\cdot,\cdot)$ is decomposable
with respect to individual and pairs of label elements,
\eg using the Viterbi algorithm.
% in a way similar to Equation~\eqref{eq:jointfeature}.

\secmoveup
\subsection{SSVM for structured recommendation: the SP and SR models}
\label{ssec:sr}
\textmoveup

We now present two possible means of applying SSVMs to sequence recommendation.

\emph{The SP model}.
Recall that structured recommendation
involves observing \emph{multiple} ground truth outputs for each input, \ie
%If we observed more than one labels for a particular set of features,
%The classic SSVM described in Section~\ref{ssec:ssvm} can be generalised to capture this setting:
for input $\x\pb{i}$, there is a set of ground truths $\{\y\pb{ij}\}_{j=1}^{n_i}$.
%where $n_i$ is the number of labels for $\x^{(i)}$.
% describe duplicating examples
One na\"{i}ve approach to the problem
is simply creating 
$n_i$ examples $\{(\x\pb{i}, \y\pb{ij})\}_{j=1}^{n_i}$,
and feeding this to the classic SSVM. %described in Section~\ref{ssec:ssvm} to capture this setting 
We call this the \emph{structured prediction} (\emph{SP}) model.

The SP model is appealing due to its simplicity.
However, it is sub-optimal:
%The problem with this approach is in the result of loss-augmented inference, 
%\ie
%%
%% DW: no need to exclude \yik
%%the result of loss-augmented inference on $(\xii, \yij)$ could be any label $\yik \in \{\yij\}_{j=1}^{n_i} \setminus \{\yij\}$,
%%but then we would incorrectly penalise predicting $\yik$ for $\xii$. 
%% 
the result of loss-augmented inference on $(\x\pb{i}, \y\pb{ij})$ could be any label $\y' \in \{\y\pb{ij}\}_{j=1}^{n_i}\}$,
but then we would incorrectly penalise predicting $\y'$ for $\x\pb{i}$. 

\emph{The SR model}.
To overcome the limitation of the SP model,
we propose the following \emph{structured recommendation} (\emph{SR}) model that extends the SSVM to explicitly incorporate multiple ground truths: %for each example.
%We can learn an \emph{SR} model by optimising %a quadratic program similar to (\ref{eq:nslack}),
\begin{equation}
\label{eq:nslack_ml}
\resizebox{0.93\linewidth}{!}{$
\begin{aligned}
\min_{\w, \, \bm{\xi} \ge 0} ~ \frac{1}{2} \w^\top \w + \frac{C}{N} \sum_{i=1}^n \sum_{j=1}^{n_i} \xi_{ij}, ~~s.t.~ \forall i, \, \forall j, 
  \w^\top \Psi(\x^{(i)}, \y^{(ij)}) - \w^\top \Psi(\x^{(i)}, \bar{\y}^{(i)}) \ge
  \Delta(\y^{(ij)}, \bar{\y}^{(i)}) - \xi_{ij}.
\end{aligned}
$}
\end{equation}
where $N = \sum_i n_i$ and $\bar{\y}^{(i)} \in \mathcal{Y} \setminus \{\y^{(ij)}\}_{j=1}^{n_i}$.
Intuitively, the objective in (\ref{eq:nslack_ml}) is similar to a ranking objective, as the constraints enforce
that the positively labeled sequences (the known items that the user likes) are scored
higher than all other unseen sequences.
Such objectives have proven useful in classic unstructured recommendation~\cite{bpr09}.
%Recent work on positive and unlabelled data have
%theoretically shown the close relationship between positive and unlabelled learning and two class classification.

Compared to the SP model (\ref{eq:nslack}), the key distinction is that (\ref{eq:nslack_ml})
explicitly aggregates all the ground truth sequences for each input when generating the constraints,
\ie the loss-augmented inference becomes
\begin{equation}
\label{eq:loss_aug_inf}
%%\resizebox{0.9\linewidth}{!}{$
\max_{\bar{\y}^{(i)} \in \ \mathcal{Y} \setminus \{\y^{(ij)}\}_{j=1}^{n_i}}
     \left( \Delta(\y^{(ij)}, \bar{\y}^{(i)}) + \w^\top \Psi(\x^{(i)}, \bar\y^{(i)}) \right).
%%$}
\end{equation}
In this way, we do not have apparently contradictory constraints where
two ground truth sequences are each required to have larger score than the other.

As a remark, we note that instead of using $N$ slack variables as that in (\ref{eq:nslack_ml}),
we can use \emph{one} slack variable to represent the sum of the $N$ hinge losses. % in (\ref{eq:nslack_ml}).
%which leads to a $1$-slack formulation
%This $1$-slack formulation can be learned more efficiently than the $N$-slack formulation (\ref{eq:nslack_ml}) if training using cutting-plane algorithm.
This 1-slack formulation can be learned more efficiently than (\ref{eq:nslack_ml})
if training using cutting-plane algorithms. %the above can be learned more efficiently than (\ref{eq:nslack_ml}).

To describe how to use the above models for sequence recommendation, we must precisely describe how one performs training and prediction using them.
This is the focus of the next two sections.


%
\subsection{SP and SR model training}
\label{ssec:training}

In training the SP and SR models, the main challenge is in performing loss-augmented inference~(\ref{eq:loss_aug_inf}).
As noted above, the SP model can be trained as per the vanilla SSVM.
The SR model, however, requires that we modify the training procedure to account for the existence of multiple ground truths.
In particular, (\ref{eq:loss_aug_inf}) needs to be solved by \emph{excluding the sequences} $\{\y\pb{ij}\}_{j=1}^{n_i}$.

%% DW: do we need to describe this here?
%%
%%For both models, a further, subtler desiderata is to
%%\emph{avoid sequences that have loops} in loss-augmented inference.
%%The motivation is that, as mentioned in \S\ref{sec:trajrec}, repeated elements are often undesirable in sequence recommendation 
%%\eg a user will likely not want to visit the same location twice.

%%We now see how both these problems may be addressed with an extension of the Viterbi algorithm.
We now see how this problem may be addressed with an extension of the Viterbi algorithm.


%
\textbf{Eliminating multiple ground truths with list Viterbi}.
Recall that standard loss-augmented inference for an SSVM (for sequence) may be done with the classic Viterbi algorithm.
To modify this algorithm to exclude a known set of sequences (\ie $\{\y^{(ij)}\}_{j=1}^{n_i}$),
we may employ
several extensions of the Viterbi algorithm that aim to find more than one high-scoring sequences.
The \emph{parallel list Viterbi} algorithm~\cite{seshadri1994list} finds the top $k$ sequences
by keeping $k$ backtrack pointers for each possible state in each position of the sequence.
However, it is difficult to apply in our scenario,
since we do not know $k$ beforehand.
This is because it is generally impossible to foresee
the rank (according to the score) of the first path among all valid sequences. %% (that may have repeated points). 
%% DW: what is a valid sequence? no loops or not observed?

We resort to the \emph{serial list Viterbi} algorithm~\cite{seshadri1994list,nilsson2001sequentially}.
This algorithm sequentially find the $k$-th best sequence given the best, 2nd best, \dots, $(k \!-\! 1)$-th best sequences.
The key insight here is that the 2nd best sequence deviates from the best sequence
for one continuous segment, and then finally merges back to the best sequence without deviating again
-- otherwise replacing one of the continuous segments with those from the best sequence will increase the score.
Subsequently, the $k$th best sequence can be the 2nd best sequence relative to the $(k \!-\! 1)$-th sequence
at the point of final merge, or the 2nd or 3rd best sequence to the $(k \!-\! 2)$-th sequence at the point of final merge, \ldots, and so on.
The serial list Viterbi allows exclusion of sequences with repeats,
as well as
excluding an arbitrary number of known sequences.
The list Viterbi algorithm is fully detailed in the supplement.

Note that the serial list Viterbi algorithm
can be used for loss-augmented inference with Hamming loss, the most common loss function for sequence prediction tasks,
since it only requires the loss function be decomposable with respect to the label elements.

%
\textbf{Eliminating loops with list Viterbi}.
%Having seen the viability of the list Viterbi algorithm for removing known sequences from consideration during inference,
The list Viterbi algorithm is also applicable to enforce that loss-augmented inference only considers sequences that are \emph{paths}.
This may be done by simply expanding the set of disallowed sequences and following the algorithm as above.
This idea can be applied to both the SP and SR models, as it is independent of whether or not we exclude multiple ground truths.
We call these extended models \textsc{SPpath} and \textsc{SRpath} respectively.


%
\subsection{SP and SR model prediction}
\label{ssec:testing}

For both the SP and SR models, prediction requires that we compute $\argmax_{\y \in \mathcal{Y}_{\mathrm{path}}} f( \x, \y )$;
subtly, we compute the maximum over $\mathcal{Y}_{\mathrm{path}} \subseteq \mathcal{Y}$, which comprises all elements of $\mathcal{Y}$ that are {paths}.
Observing that this requires excluding a set of sequences from consideration, a natural idea is to apply the list Viterbi algorithm of the previous section for this task.
However, while list Viterbi algorithms are polynomial time given the list depth $k$~\cite{nilsson2001sequentially},
in reality $k$ is unknown a priori and can be very large for long sequences.
It is fortunately possible to explore %%more efficient 
alternative
approaches to this problem.

%
\textbf{Eliminating loops with ILP}.
Consider the setting of finding the best path $\y^*$ that starts from a specific point $y_1^*$ and traverses exactly $l$ of $m$ candidate points.
Given desired sequence length $l$ among $m$ possible points, the Viterbi algorithm~\cite{tsochantaridis2005large}
will generate the length-$l$ sequence $\y^*$ with the best score. %i.e. $\y^* = \argmax_\y f(\x,\y)$.
One may then use the well-known
Christofides algorithm~\cite{christofides1976} on $\y^*$ to eliminate loops in the sequence.
%is known to generate a solutions within a factor of 3/2 of the optimal solution for traveling salesman problems.
However, the resulting sequence will have less than the desired number of points, and the resulting score will not be optimal in general.

This problem is a variant of the travelling salesman problem (TSP), or the best of ${m \choose l}$ TSPs.
Such a point traversal problem can be solved by incorporating
sub-tour elimination constraints of the TSP~\cite{ijcai15,cikm16paper}.
In particular, %%the integer linear program (ILP) formulation (Eq.~\ref{eq:obj} to \ref{eq:cons4})
%%will solve SR inference for path $\y^*$ of length $l$ by decoding an optimal solution.
it can be solved by decoding an optimal solution of the integer linear program (ILP) Eq.~\ref{eq:obj} to \ref{eq:cons4}.

Given a set of points $\{p_j\}_{j=1}^m$, 
consider binary decision variables $u_{jk}$ that are true if and only if
the transition from $p_j$ to $p_k$ is in the resulting path,
and binary decision variables $z_j$ that are true iff $p_j$ is the last point in $\y^*$.
%Suppose that $l$ is the number of candidate POIs.
For brevity, we index the points such that $y_1^* = p_1$.
Firstly, the desired path should start from $y_1^*$ (Constraint~\ref{eq:cons1}).
In addition, only $l\!-\!1$ transitions between points are permitted as the path length is $l$ (Constraint~\ref{eq:cons2}).
Moreover, any point could be visited at most once (Constraint~\ref{eq:cons3}).
The last constraint, where $v_j \in \mathbf{Z}$ is an auxiliary variable,
enforces that $\y^*$ is a single sequence of points without sub-tours.
%\TODO{LX: i do not understand the last contraint, $v_j$ did not seem to have been defined? are they binary or something else?}
% DW: v_j defined in the first constraint, which is a natural number
We rewrite Eq.~(\ref{eq:jointfeature}) into a linear function of decision variables $u_{jk}$
(dropping the unary term corresponding to $y_1^*$ as it is a constant), which results in ~(\ref{eq:obj}).

\begin{equation}
\label{eq:obj}
\max_{\bu} ~\sum_{k=1}^m \w_k^\top \psi_k(\x, p_k) \sum_{j=1}^m u_{jk} +
            \sum_{j,k=1}^m u_{jk} \w_{jk}^\top \psi_{j, k}(\x, p_j, p_k)
\end{equation}
{$\displaystyle
\begin{minipage}{0.45\linewidth}
\begin{alignat}{2}
s.t. \, 
& \sum_{k=2}^m u_{1k} = 1, \, \sum_{j=2}^m u_{j1} = z_1=0                 \label{eq:cons1} \\
& \sum_{j=1}^m \sum_{k=1}^m u_{jk} = l-1, \, \sum_{j=1}^m u_{jj}=0        \label{eq:cons2}
\end{alignat}
\end{minipage}
\qquad
\begin{minipage}{0.5\linewidth}
\begin{alignat}{3}
\sum_{j=1}^m u_{ji} = z_i &+ \sum_{k=2}^m u_{ik} \le 1, \, i = 2,\cdots,m  \label{eq:cons3} \\
v_j - v_k + 1 &\le (m-1) (1-u_{jk}),                                       \nonumber \\
              & \qquad j,k = 2,\cdots,m                                    \label{eq:cons4}
\end{alignat}
\end{minipage}
$}

% ILP for top-k prediction/inference
%\textbf{Top-$K$ prediction}.
We can also use the above formulation to find the top-$K$ scored paths in a sequential manner. 
In particular, given the $K\!-\!1$ top scored paths $\{\y\pb{k}\}_{k=1}^{K-1}$, 
the $K\!$-th best scored path can be found by solving the above ILP with additional constraints:
%%\begin{equation}
%%\label{eq:topk_ILP}
$\sum_{j=1}^{l-1} u_{y_j, y_{j+1}} \le l-2, ~\forall \y \in \{\y^{(k)}\}_{k=1}^{K-1}$.
%%\end{equation}
% ILP for learning (loss-augmented inference)

\textbf{Eliminating multiple ground truths with ILP?}
As a final remark, a natural question is whether one can use the above ILP 
to exclude observed labels when training an SR model,
\ie solving the loss-augmented inference~(\ref{eq:loss_aug_inf}).
In fact, this may be done
%together with constraints~(\ref{eq:cons1})-(\ref{eq:cons4}),
as long as the loss $\Delta(\y,\bar\y)$ can be represented as a linear function of $u_{jk}$,
\eg the number of mis-predicted POIs disregarding the order $\Delta(\y, \bar\y) = \sum_{j=2}^l (1 - \sum_{k=1}^m u_{k, y_j})$.
However, we note that Hamming loss %%, the most common loss function for sequence prediction tasks,
cannot be used here, as $\Delta(\y,\bar\y) = \sum_{j=1}^l (1 - \llb y_j = \bar y_j \rrb)$
cannot be expressed as a linear function of $u_{jk}$.

\textbf{Practical choices.}
When performing prediction for SP and SR model, we found that state-of-the-art ILP solvers converge to a solution faster 
than the serial list Viterbi algorithm if the sequence length $l$ is large.
We therefore use ILP for very long ($l\ge10$) sequences in the experiments (otherwise the list Viterbi algorithm is applied).

% \subsection{SSVM for sequence recommendation: sequence decoding}
% \label{ssec:subtour}
% \label{ssec:SRinf}

% The SR model require two algorithmic components for inference and learning
% (missing from the SSVM algorithms).
% For inference, SR needs to predict a {\em path}, \ie a sequence whose elements are distinct from each other,
% that maximises the score function (Eq.~\ref{eq:jointfeature}).
% As described in Section~\ref{sec:intro}, this is desirable since users traversing a sequence (of locations or music)
% would not want to see repeated entries.
% Given desired sequence length $l$ among $m$ possible points, the Viterbi algorithm~\cite{tsochantaridis2005large}
% will generate the length-$l$ sequence $\y^*$ with the best score. %i.e. $\y^* = \argmax_\y f(\x,\y)$.
% One may then use the well-known
% Christofides algorithm~\cite{christofides1976} on $\y^*$ to eliminate loops in the sequence.
% However, the resulting sequence will have less than the desired number of points, and the resulting score will not be optimal in general.

% For learning an SR model, loss-augmented inference (\ref{eq:loss_aug_inf}) needs to be done by excluding multiple known sequences.
% As described in Section~\ref{ssec:sr}, %this involves %we would want to maximize the loss-augmented objective function
% however, the Viterbi algorithm uses back-tracking to find the best sequence,
% and cannot easily exclude known sequences.
% The rest of this section describes two algorithms, each intuitively aimed to address %one of 
% the two requirements above, which can be applied in novel contexts of the SR model.


% \textbf{Finding paths with integer programming}.
% For inference in the SR model, 
% we consider the setting of finding the best path $\y^*$ that starts from a specific point $y_1^*$ and traverses exactly $l$ of $m$ candidate points.
% This can be seen as a variant of the travelling salesman problem (TSP), or the best of ${m \choose l}$ TSPs.
% Such a point traversal problem can be solved by incorporating
% sub-tour elimination constraints of the TSP~\cite{ijcai15,cikm16paper}.
% In particular, the integer linear program (ILP) formulation (Eq.~\ref{eq:obj} to \ref{eq:cons5})
% will solve SR inference for path $\y^*$ of length $l$ by decoding an optimal solution.

% Given a set of points $\{p_j\}_{j=1}^m$, 
% consider binary decision variables $u_{jk}$ that are true if and only if
% the transition from $p_j$ to $p_k$ is in the resulting path,
% and binary decision variables $z_j$ that are true iff $p_j$ is the last point in $\y^*$.
% %Suppose that $l$ is the number of candidate POIs.
% For brevity, we index the points such that $y_1^* = p_1$.
% Firstly, the desired path should start from $y_1^*$ (Constraint~\ref{eq:cons1}).
% In addition, only $l\!-\!1$ transitions between points are permitted as the path length is $l$ (Constraint~\ref{eq:cons2}).
% Moreover, any point could be visited at most once (Constraint~\ref{eq:cons3}).
% The last constraint, where $v_j \in \mathbf{Z}$ is an auxiliary variable,
% enforces that $\y^*$ is a single sequence of points without sub-tours.
% %\TODO{LX: i do not understand the last contraint, $v_j$ did not seem to have been defined? are they binary or something else?}
% % DW: v_j defined in the first constraint, which is a natural number
% We rewrite Eq.~(\ref{eq:jointfeature}) into a linear function of decision variables $u_{jk}$
% (dropping the unary term corresponding to $y_1^*$ as it has been observed), which results in ~(\ref{eq:obj}):

% \begin{equation}
% \label{eq:obj}
% \max_{\bu} ~\sum_{k=1}^m \w_k^\top \psi_k(\x, p_k) \sum_{j=1}^m u_{jk} +
%             \sum_{j,k=1}^m u_{jk} \w_{jk}^\top \psi_{j, k}(\x, p_j, p_k)
% \end{equation}

% {$\displaystyle
% \begin{minipage}{0.45\linewidth}
% \begin{alignat}{2}
% s.t. \, 
% & \sum_{k=2}^m u_{1k} = 1, \, \sum_{j=2}^m u_{j1} = z_1=0                 \label{eq:cons1} \\
% & \sum_{j=1}^m \sum_{k=1}^m u_{jk} = l-1, \, \sum_{j=1}^m u_{jj}=0        \label{eq:cons2}
% \end{alignat}
% \end{minipage}
% \qquad
% \begin{minipage}{0.5\linewidth}
% \begin{alignat}{3}
% \sum_{j=1}^m u_{ji} = z_i + \sum_{k=2}^m u_{ik} \le 1, \, i = 2,\cdots,m  \label{eq:cons3} \\
% v_j - v_k + 1 \le (m-1) (1-u_{jk}),                                       \nonumber \\
% j,k = 2,\cdots,m~                                                         \label{eq:cons4}
% \end{alignat}
% \end{minipage}
% $}

% We can use the above formulation to find the top-$K$ scored paths in a sequential manner. 
% In particular, given the $K\!-\!1$ top scored paths $\{\y^{(k)}\}_{k=1}^{K-1}$, 
% the $K\!$-th best scored path can be found by solving the above ILP with additional constraints:
% $\sum_{j=1}^{l-1} u_{y_j, y_{j+1}} \le l-2, ~\forall \y \in \{\y^{(k)}\}_{k=1}^{K-1}$.
% This constraint can also be used to exclude observed labels when learning an SR model, 
% \ie solving the loss-augmented inference~(\ref{eq:loss_aug_inf}),
% together with constraints~(\ref{eq:cons1})-(\ref{eq:cons4}),
% as long as the loss $\Delta(\y,\bar\y)$ can be represented as a linear function of $u_{jk}$,
% \eg the number of mis-predicted POIs disregarding the order $\Delta(\y, \bar\y) = \sum_{j=2}^l (1 - \sum_{k=1}^m u_{k, y_j})$.
% However, we note that Hamming loss, the most common loss function for sequence prediction tasks,
% cannot be used here, as $\Delta(\y,\bar\y) = \sum_{j=1}^l (1 - \llb y_j = \bar y_j \rrb)$
% cannot be expressed as a linear function of $u_{jk}$.

% \textbf{Finding $k$-best sequences}.
% An alternative approach to solve the inference and learning of the SR model is making use of 
% several extensions of the Viterbi algorithm that aim to find more than one high-scoring sequences.
% The \emph{parallel list Viterbi} algorithm~\cite{seshadri1994list} finds the top $k$ sequences
% by keeping $k$ backtrack pointers for each possible state in each position of the sequence.
% However, it is difficult to apply to the SR inference scenario,
% since we do not know $k$ beforehand.
% This is because it is generally impossible to foresee
% the rank (according to the score) of the first path among all valid sequences (that may have repeated points).

% We resort to the \emph{serial list Viterbi} algorithm~\cite{seshadri1994list,nilsson2001sequentially}.
% These algorithms sequentially find the $k$-th best sequence given the best, 2nd best, \dots, $(k \!-\! 1)$-th best sequences.
% The key insight here is that the 2nd best sequence deviates from the best sequence
% for one continuous segment, and then finally merges back to the best sequence without deviating again
% -- otherwise replacing one of the continuous segments with those from the best sequence will increase the score.
% Subsequently, the $k$th best sequence can be the 2nd best sequence relative to the $(k \!-\! 1)$-th sequence
% at the point of final merge, or the 2nd or 3rd best sequence to the $(k \!-\! 2)$-th sequence at the point of final merge, \ldots, and so on.
% %The version of serial list Viterbi presented by~\cite{nilsson2001sequentially} accommodates
% The serial list Viterbi allows exclusion of sequences with repeats, %by checking whether or not the current $k$th best sequence has a repeat, if it does, discard and proceed to the $(k \!+\! 1)$-th.
% %and also allows 
% as well as
% excluding an arbitrary number of known sequences.
% %by partitioning the remaining space
% The list Viterbi algorithm is fully detailed in the supplement.

% The serial list Viterbi algorithm is particularly versatile
% for the structured recommendation problem: %the serial list Viterbi algorithm 
% it can be used for inference by sequentially getting the next best sequence, until the required number of %one or more 
% paths are found;
% it can also be used for loss-augmented inference with Hamming loss,
% since it only requires the loss function be decomposable with respect to the label elements.

% List Viterbi algorithms are polynomial time given the list depth $k$~\cite{nilsson2001sequentially},
% but in reality $k$ is unknown a priori and can be very large for long sequences.
