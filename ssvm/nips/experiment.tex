% !TEX root = ./main.tex

\secmoveup
\section{Experimental results}
\label{sec:experiment}
\textmoveup

We now present evaluations assessing the viability of our structured prediction approach
for the trajectory recommendation task discussed in Section~\ref{sec:trajrec}.
%of trajectory recommendation.
On a real-world dataset of photo tours, our methods are shown to significantly outperform
a number of non-structured baselines or those that do not take into account the recommendation setting.
%, over which we demonstrate significant improvements.


\secmoveup
\subsection{Photo trajectory datasets}
\label{sec:dataset}
\textmoveup

% experiment protocol: Nested cross-validation with Monte-Carlo cross-validation for the inner loop
We assess the recommendation performance %methods developed in Section~\ref{sec:trajrec}
%%of various methods
on trajectories extracted from Flickr photos
for the cities of Glasgow, Osaka and Toronto~\cite{thomee2016yfcc100m,ijcai15}.
Each dataset comprises of a
list of trajectories, being a sequence of points of interest (POI), as visited by various Flickr users
and recorded by the geotags in photos. Photos that are nearby in time and space are grouped and then segmented by eight hours of time gap, and visits that only include a single POI are excluded.
The statistics of datasets are shown in Table~\ref{tab:data}.
All three datasets are sparse in user activity,
i.e., each user has on average less than two trajectories.
This makes user-specific recommendation impractical, and also undesirable because of the domain being urban locations
(a user would want different recommendations given different starting locations, and not a static recommendation no matter where she is).
From Table~\ref{tab:data}, %%and Figure~\ref{fig:hist},
we see that each distinct query %, %comprising of start POI and a desired trip length,
has an average of 4-9, and a maximum of 30-60 trajectories.
Therefore, evaluation is carried out on the problem of recommending trajectories given a query.
%as posed in \S\ref{sec:trajrec}:
%given a query , can we recommend a trajectory of POIs that a visitor will enjoy?

%A characteristic of these datasets is that many queries are associated with multiple trajectories, or ground truths.
%recommendationThe histograms of the number of ground truths for queries are shown in Figure~\ref{fig:hist}.

% dataset stats
\begin{table}[!h]
	\begin{minipage}[t]{\linewidth}
		\resizebox{\linewidth}{!}{
		\setlength{\tabcolsep}{4pt} % tweak the space between columns
		\small
		\begin{tabular}{l*{9}{c}} \hline
		\textbf{Dataset} & \textbf{\#Traj.} & \textbf{\#POIs} & \textbf{\#Users} & \textbf{\#Queries} & \textbf{\#GT=1} & \textbf{\#GT$\in$[2,5]} & \textbf{\#GT$>$5} & \textbf{\#Traj.(short)} & \textbf{\#Traj.(long)} \\ \hline
		Osaka            & 186              & 26              & 130              & 47                 & 17              & 22                      & 8                 & 178                     & 8  \\
		Glasgow          & 351              & 25              & 219              & 64                 & 23              & 22                      & 19                & 336                     & 15 \\
        Toronto          & 977              & 27              & 454              & 99                 & 30              & 33                      & 36                & 918                     & 59 \\
		\hline
		\end{tabular}%
		}
		\captionof{table}{Statistics of trajectory dataset,
        including the number of trajectories/POIs/users/queries (\ie \textbf{\#Traj.}, \textbf{\#POIs}, \textbf{\#Users} and \textbf{\#Queries}); 
        the number of queries with a single/2-5/more than 5 ground truths (\ie \textbf{\#GT=1}, \textbf{\#GT$\in$[2,5]} and \textbf{\#GT$>$5}); 
        the number of trajectories with less than 5 (\textbf{\#Traj.(short)}) and more than 5 POIs (\textbf{\#Traj.(long)}).
        }
		\label{tab:data}
	\end{minipage}%
	%%\quad
	%%\begin{minipage}[t]{0.45\linewidth}
	%%	\includegraphics[scale=0.25]{hist_query.pdf}
	%%    \captionof{figure}{\# of trajectories per query.}
	%%    \label{fig:image}
    %%\end{minipage}
\end{table}

%\newsavebox\tmpbox

% % dataset stats
% \begin{table}[t]
% 	\sbox\tmpbox{%
% 		\resizebox{0.4\linewidth}{!}{
% 		\setlength{\tabcolsep}{4pt} % tweak the space between columns
% 		\small
% 		\begin{tabular}{l*{5}{c}} \hline
% 		\textbf{Dataset} & \textbf{\#Traj.} & \textbf{\#POIs} & \textbf{\#Users} & \textbf{\#Queries} & \textbf{AvgLenth} \\ \hline
% 		Osaka            & 186              & 26              & 130              & 47                 & 2.4 \\
% 		Glasgow          & 351              & 25              & 219              & 64                 & 2.5 \\
% 		\hline
% 		\end{tabular}%
% 		}
% 	}%
%   \renewcommand*{\arraystretch}{0}
%   \begin{tabular*}{\linewidth}{@{\extracolsep\fill}p{\wd\tmpbox}p{40mm}@{}}
%     \usebox\tmpbox &
%     \includegraphics[scale=0.4]{hist.pdf} \\
%     \caption{Statistics of trajectory dataset.}
%     \label{tab:data}
%     &
%     \captionof{figure}{Histograms of the number of trajectories per query.}
%     \label{fig:image}
%   \end{tabular*}
% \end{table}


\secmoveup
\subsection{Methods compared}
\label{ssec:methods}
\textmoveup

We compare the performance of our methods to the following three baselines:
\begin{itemize}[leftmargin=0.125in]\itemmoveup
\parskip -.05em
\item The \textsc{Random} baseline simply recommends a sequence of POIs by sampling uniformly at random from the whole set of POIs 
      (without utilising any POI or query related features).

\item The stronger \textsc{Popularity} baseline simply recommends the top-$l$ most popular POIs, 
      \ie, the POIs that have been visited by the most number of users in the training set.

\item \textsc{PoiRank}~\cite{cikm16paper} is a generalisation of \textsc{Popularity} 
      which considers a number of POI-query features\footnote{See supplement for details of features.\label{fn:suplement}} in addition to the popularity,
      and trains a RankSVM model to learn a score for each POI. The top-$l$ scored POIs are then used to construct a trajectory.
\end{itemize}\itemmoveup

To assess the viability of our structured prediction approach, and the necessity of our two extensions (normalising the loss per query and disallowing loops), we consider the following structured methods: %the following versions of our structured prediction methods:
\begin{itemize}[leftmargin=0.125in]\itemmoveup
\parskip -.05em
%\item The structured prediction ({\sc SP}) method employs the vanilla structured SVM framework in order to learn a score for trajectories given a query.
\item The SP and SR methods described in Section~\ref{ssec:sr} with both POI-query features and pairwise features\footref{fn:suplement}.

%\item The structured recommendation ({\sc SR}) method extends the {\sc SP} method by additionally incorporating multiple ground truths into
%      forming the constraints and adding them in cutting-plane algorithm,
%      described in Section~\ref{ssec:sr}.
	%performing normalisation of the loss function per query,
	%so that we do not attempt to distinguish between multiple ground truths for the same query.

\item The variants {\sc SPpath} and {\sc SRpath} described in Section~\ref{ssec:training} with the same features as the SP and SR methods.
\end{itemize}\itemmoveup
All above methods take into account the specified start location properly.

\secmoveup
\subsection{Evaluation procedure}
\textmoveup

% leave-one-out evaluation (with query aggregation)
%As described in Section~\ref{sec:queryagg},
%To evaluate the performance of the various methods under comparison,
%we first group the trajectories  %according to queries that they conform to.
We then evaluate the performance of each algorithm using leave-one-query-out cross validation. 
That is, holding out all the relevant trajectories for each query $\x\pb{i}$ (\ie $\{\y\pb{ij}\}_{j=1}^{n_i}$) in each round,
where in each iteration of this procedure,
%one query and its associated trajectories serves as a test point, with all other trajectories for training.
%(Note that without this query aggregation, there will be considerable overlap between the train and test set, and simple nearest neighbour methods will be hard to outperform.)
% model selection (Monte Carlo CV) (with query aggregation): 90/10 random split for 5 times
the hyper-parameter (\eg the regularisation constant $C$) are tuned using Monte Carlo cross validation~\cite{burman1989comparative} on the training set.

We use three different measures to compare algorithm performances.
The {\bf F$_1$ score on points}~\cite{ijcai15} computes F$_1$ on the predicted versus seen points
without considering their relative order.
The {\bf F$_1$ score on pairs}~\cite{cikm16paper} is proposed to mitigate this by computing F$_1$ on all ordered pairs in the predicted versus groundtruth sequence. %%It is 1 iff both sequences agree completely.
The well-known rank correlation {\bf Kendall's $\tau$} (version $b$)~\cite{agresti2010analysis} 
computes the ratio of concordant (correctly ranked) pairs minus discordant pairs, over all possible pairs after taking care of ties.
%$\frac{1}{2}l(l-1)$) pairs  %DW: this is not correct

Structured recommendation problems aims to perform ranking on a very large $m^l$ labelset.
We adopt the practice of {\em best of top $k$}~\cite{russakovsky2015imagenet} for reporting results: %\footnote{$k=10$ in our experiments}:
that is, for all methods described in Section~\ref{ssec:methods},
we predict the top $k$ trajectories
\footnote{We use the list Viterbi algorithm to perform top $k$ predictions for \textsc{Popularity} and \textsc{PoiRank}, see supplement for details.}
and then report the best match of any in the top $k$ to any trajectory in the ground truth set.
We also explored the effect of different values for $k$ (\eg $k=1,\cdots\!,10$).


\eat{
As described previously, our methods are capable of recommending not merely a single trajectory,
but rather a list of trajectories.
While one can simply take the top recommended trajectory as the prediction,
this ignores the fact that there are likely multiple plausible trajectories for any given query.
Thus, for each performance measure $\mathrm{perf}$,
we take the maximum over all trajectories,
i.e.,
\begin{equation*}
%\tau_b^{(i)} =
\mathrm{perf}^{(i)}( \mathbf{y}, \hat{\mathbf{y}} ) =
\max_{(\mathbf{y}, \hat{\mathbf{y}}) \in \{\mathbf{y}^{(ij)}\}_{j=1}^{N_i} \times \{\hat{\mathbf{y}}^{(ij)}\}_{j=1}^k}
%\tau_b(r_\mathbf{y}, r_{\hat{\mathbf{y}}}),
\mathrm{perf}(\mathbf{y}, {\hat{\mathbf{y}}}),
\end{equation*}
where $\{\mathbf{y}^{(ij)}\}_{j=1}^{N_i}$ are the ground truths for query $\mathbf{x}^{(i)}$ and
$\{\hat{\mathbf{y}}^{(ij)}\}_{j=1}^k$ are the top-$k$ recommendations.
}

\secmoveup
\subsection{Results and discussion}
\label{sec:result}
\textmoveup

\input{tab_experiment}


%\begin{minipage}[!t]{0.8\linewidth}
\begin{figure}[!t]
		\centering
		\includegraphics[scale=0.35]{tau_topk.pdf}
	    \captionof{figure}{Average $\tau$ on short/long trajectories w.r.t. top-k recommendation.}
	    \label{fig:topk}
	    \vspace{-0.2in}
\end{figure}

% experimental results
%The performance of %three baselines and four variants based on structured prediction on two datasets
%all methods are shown in Table~\ref{tab:result}.
The results in Table~\ref{tab:result} and Figure~\ref{fig:topk} validate our contributions as follows:

\textbf{Exploiting the structure of sequences helps}.
All variants of our structured prediction methods achieve better performance than existing baselines.
Thus, the basis of our approach -- reducing sequence recommendation to a structured prediction problem -- is sensible, and has empirical benefit.

\textbf{Accounting for multiple ground truths helps}.
\textsc{SR} always performs better than \textsc{SP},
and similarly for the {\sc path} variants of both methods.
This indicates that our first extension -- explicitly modelling multiple ground truths helps recommendation -- is important to achieve good performance.
(We note that even without this correction, our structured methods outperform baselines.)

\textbf{Eliminating loops during training helps}.
{\sc SRpath} improves performance further of the {\sc SR} method,
as indicated by the F$_1$ score on pairs.
This indicates that our second extension -- explicitly performing sub-tour elimination in training -- is important to further improve performance.
Interestingly,
this advantage does \emph{not} take effect if the multiple ground truths are not modelled explicitly,
with the performance of the {\sc SP} method largely unaffected.

\textbf{An illustrative example}. Figure~\ref{fig:example} presents an example illustrating the differences among three algorithms. The query requires the trajectory to start from the middle point and be of length 3.
\textsc{PoiRank} regards points at the lower right and upper left be of the highest rank, but did not consider their compatibility (\ie pairwise features). {\sc SP} and {\sc SR} hits one edge (green edge) out of the two ground truths, while {\sc SRpath} hits both edges for two valid trajectories by exploiting all factors.

Overall, these results indicate that our structured prediction approach to the problem has
benefits over non-structured approaches,
and that our extensions to the vanilla structured approach are important to further improve performance.

%\subsection{Example}
%\label{sec:example}

\begin{figure*}[t]
	\centering
	\includegraphics[scale=0.5]{example.pdf}
	\caption{Example of structured recommender versus baseline on a query with two ground truths. %as shown in Figure (c).
             (a) \textsc{PoiRank} cannot make a recommendation related to any of the ground truths;
             (b) \textsc{SP} and \textsc{SR} recommend a better trajectory than \textsc{PoiRank}, but not fully consistent with the ground truths;
             (c) \textsc{SRpath} hits both ground truths at rank 3 (green edges) and 5 (red edges) respectively.}
	\label{fig:example}
\end{figure*}
