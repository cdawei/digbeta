\documentclass[9pt]{extarticle}
\usepackage[a4paper,top=0.79in,left=0.79in,bottom=0.79in,right=0.79in]{geometry} % A4 paper margins in LibreOffice
\usepackage[numbers,compress]{natbib}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{bbm}
%\usepackage{ulem}
\usepackage{stmaryrd}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[sc]{mathpazo}
\linespread{1.05}       % Palladio needs more leading (space between lines)
\usepackage[T1]{fontenc}
\usepackage{footmisc}   % \footref, refer the same footnote at different places
\usepackage{subcaption} % sub-figures
\usepackage{setspace}   % set space between lines
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{xcolor}
\usepackage{graphicx}
\graphicspath{{fig/}}   % Location of the graphics files

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\eat}[1]{}
\newcommand{\given}{\mid}
\newcommand{\llb}{\llbracket}
\newcommand{\rrb}{\rrbracket}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\h}{\mathbf{h}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\1}{\mathbf{1}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\p}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\q}{\mathbf{q}}
\newcommand{\LCal}{\mathcal{L}}
\newcommand{\XCal}{\mathcal{X}}
\newcommand{\YCal}{\mathcal{Y}}
\newcommand{\alphat}{\tilde{\alpha}}
\newcommand{\betat}{\tilde{\beta}}
\newcommand{\gammat}{\tilde{\gamma}}
\newcommand{\phit}{\tilde{\phi}}
\newcommand{\alphabm}{\bm{\alpha}}
\newcommand{\betabm}{\bm{\beta}}
\newcommand{\nubm}{\bm{\nu}}
% madeness: suPer-script in Brackets
\newcommand{\pb}[1]{^{({#1})}}

\newcommand{\eg}{e.g.\ }
\newcommand{\ie}{i.e.\ }
\newcommand{\downto}{\,\textbf{downto}\,}
\newcommand{\blue}[1]{{\color{blue}{#1}}}

\setlength{\columnsep}{1.5em} % spacing between columns

\title{Multi-label Classification, Bipartite Ranking and Playlist Generation}

\author{Dawei Chen}

\date{\today}

\begin{document}

\maketitle

\section{Multi-label classification}
\label{sec:mlc}

%1. Brief summary of reference
\paragraph{Summary}
\citet{dembczynski:2010} formalised the multi-label classification problem, 
and claimed that if Hamming loss or rank loss is used,
multi-label classification methods, in theory, could not benefit from modelling label dependence.
On the other hand, modelling correlation between labels was necessary if one chose to use the subset 0/1 loss.
Further, a probabilistic classifier chains (PCC), which generalised the classifier chains (CC) from a probabilistic perspective,
was proposed to modelling label correlation. 

Theoretically, the order of labels does not affect the model, 
in practice, however, using different order of labels will result in different model parameters (we do not have infinity data).
To alleviate this issue, an ensemble of PCC (EPCC) was proposed, which made a prediction by averaging over predictions by a number of PCCs, 
each model was trained using a randomly chosen permutation of the labels.
PCC (and EPCC) was empirically shown to outperform a number of baselines that did not model label correlations when label dependence existed in data.


\noindent
\paragraph{Definition}
Let $\LCal = \{\lambda_1,\dots,\lambda_l\}$ be a finite set of class labels,
and example $(\x,\y) \in \XCal \times \YCal$, 
where $\YCal \in \{0,1\}^m$ is the set of all possible labels,
and $\y=y_{1:m}$ is a binary vector where $y_i = 1$ \emph{iff} $\lambda_i$ is a label of $\x$.
A multi-label classifier is a mapping $\h: \XCal \to \YCal$.

\noindent
\paragraph{Label dependence}
Suppose examples are independent and identically distributed (iid) according to a joint probability distribution $\p(\X,\Y)$ on $\XCal \times \YCal$,
where $\X$ is a random variable and $\Y=Y_{1:l}$ is a random vector,
Let $\p\pb{i}(Y_i |\x)$ be the marginal distribution of $Y_i$, then
\begin{equation*}
\p\pb{i}(Y_i=b |\x) = \sum_{\y \in \YCal:y_i = b} \p(\Y = \y |\x),
\end{equation*}
where $\p(\Y = \y |\x)$ is the posterior distribution given observation $\x$.
We note that the labels are not independent if 
\begin{equation*}
\p(\Y |\x) \ne \prod_{i=1}^l \p\pb{i}(Y_i |\x),
\end{equation*}
and the degree of dependence could be quantified in terms of measures such as cross entropy and KL divergence.

\noindent
\paragraph{Learning}
Given a loss function $\ell(\cdot)$, 
we can learn a multi-label classifier by find a model $\h^*$ that minimise the expected loss over the joint distribution $\p(\X,\Y)$:
\begin{equation*}
\h^* 
= \argmin_{\h} \, \E_{\X\Y} \, \ell(\Y,\h(\X))
= \argmin_{\h} \, \E_{\X} \, \E_{\Y|\X} \, \ell(\Y,\h(X))
= \argmin_{\h} \, \sum_{\x} \p(\x) \, \E_{\Y|\X} \, \ell(\Y,\h(\x)),
\end{equation*}
thanks to the summation, fix $\x$, we have
\begin{equation*}
\h^*(\x) = \argmin_{\y} \, \E_{\Y|\X} \, \ell(\Y,\y).
\end{equation*}
Frequently used loss functions in the context of multi-label classification including Hamming loss, rank loss and subset 0/1 loss~\cite{dembczynski:2010},
here we focus on a rank loss (taking care of ties):
\begin{equation}
\label{eq:loss_rank}
\ell(\y, \h(\x)) = \sum_{(i,j): y_i > y_j} \left( \llb h_i < h_j \rrb + \frac{1}{2} \llb h_i = h_j \rrb \right).
\end{equation}
\emph{Theorem 3.1 in~\cite{dembczynski:2010} here.}

\noindent
\paragraph{Probabilistic classifier chains}
Given a query $\x$, the posterior probability of a label $\y$ can be computed using the product rule of probability:
\begin{equation*}
\p(\y |\x) = \p(y_1) \cdot \prod_{i=2}^l \p(y_i |\x, y_{1:i-1}),
\end{equation*}
and we further define a function:
\begin{equation*}
f_i = 
\begin{cases}
\p(y_i = 1 |\x), & i = 1 \\
\p(y_i = 1 |\x, y_{1:i-1}), & 1 < i \le l
\end{cases}
\end{equation*}
then we have
\begin{equation*}
\p(\y |\x) = f_1 \cdot \prod_{i=2}^l f_i,
\end{equation*}
where $f_i$ uses $\x$ and $y_{1:i-1}$ as the input features. 
Theoretically, the results of the product rule does not depend on the order of variables, 
however, in practice, different order of variables will result in different model parameters (\ie the order of features depend on the order of variables). \\
\emph{Greedy approach -- classifier chain; assuming Markov property, we can use the Viterbi algorithm; with Neural net, we can build an order agnostic model.}


\section{Bipartite ranking}
\label{sec:birank}

%1. Brief summary of reference
\paragraph{Summary}
\citet{li:2014} proposed a new algorithm (\ie \emph{TopPush}) for bipartite ranking to optimise the ranking accuracy at the top.
This algorithm has a linear time complexity at each iteration of the optimisation process.

The key observation was that the loss used in~\cite{agarwal:2011} (when indicator function is replaced with a convex surrogate)
can be equivalently transformed to a new form 
which can be optimised in linear time (w.r.t the size of training set).


\paragraph{Definition} 
Bipartite ranking is to learn a real-valued ranking function that places positive examples above negative examples~\cite{li:2014}.
Formally, given training examples $S = S_+ \cup S_-$ with $m$ positive examples $S_+ = \{\x_i^+\}_{i=1}^m$ and $n$ negative examples $S_- = \{\x_i^-\}_{i=1}^n$, 
bipartite ranking aims to learn a ranking function $f: \XCal \to \R$ that is likely ranks positive examples higher than negative examples.

\paragraph{Loss function}
AUC is a widely used as an evaluate metric for bipartite ranking, and it turns out that AUC can be optimised by minimising a loss defined as~\cite{cortes:2004}
\begin{equation}
\label{eq:loss_auc}
\ell_\text{rank}(f; S) = \frac{1}{mn} \sum_{i=1}^m \sum_{j=1}^n \llb f(\x_i^+) \le f(\x_j^-) \rrb,
\end{equation}
and this loss can be easily optimised (\eg by gradient descent) if we replace the indicator function with a convex surrogate such as the truncated quadratic loss 
$\ell(z) = (1+z)_+^2$, the exponential loss $\ell(z) = e^z$ and logistic loss $\ell(z) = \log(1+e^z)$.
One drawback of this loss function is enumerating all the positive-negative pairs, which is computationally expensive for large dataset. \\
\emph{Theorem 3.1 in~\cite{dembczynski:2010} for this loss function here.}

Alternatively, one may interested in optimising the ranking accuracy only at the top, 
or equivalently, we would like to minimize the number of positive examples that ranked below the highest-ranking negative instance~\cite{agarwal:2011,li:2014}:
\begin{equation}
\label{eq:loss_inf}
\begin{aligned}
\ell_{\infty}(f; S) 
&= \max_{1 \le j \le n} \frac{1}{m} \sum_{i=1}^m \, \llb f(\x_i^+) < f(\x_j^-) \rrb \\
&= \frac{1}{m} \sum_{i=1}^m \max_{1 \le j \le n} \llb f(\x_i^+) < f(\x_j^-) \rrb,
\end{aligned}
\end{equation}
by replace the indicator function in (\ref{eq:loss_inf}) with a convex surrogate $\ell(\cdot)$, we have
\begin{equation}
\label{eq:loss_inf1} 
\begin{aligned}
\tilde{\ell}_{\infty}(f; S) 
&= \frac{1}{m} \sum_{i=1}^m \max_{1 \le j \le n} \ell\left( f(\x_j^-) - f(\x_i^+) \right) \\
&= \frac{1}{m} \sum_{i=1}^m \ell\left( \max_{1 \le j \le n} f(\x_j^-) - f(\x_i^+) \right),
\end{aligned}
\end{equation}
which can be optimised more efficiently than (\ref{eq:loss_auc})~\cite{li:2014}.

\paragraph{Dual formulation}
Consider a linear ranking function $f(\x) = \w^\top \x$ and loss function~\ref{eq:loss_inf1}.
Table~\ref{tab:symbol} summarises some notation we will use.
\begin{table}[!h]
\caption{Glossary of commonly used symbols}
\label{tab:symbol}
\renewcommand{\arraystretch}{1.5} % tweak the space between rows
\setlength{\tabcolsep}{1pt} % tweak the space between columns
\centering
\begin{tabular}{llll}
\hline \hline
\multicolumn{3}{l}{\textbf{Symbol}} & \textbf{Quantity} \\ \hline 
$d$              &  $\in$  &  $\Z^+$  & The number of features for each example \\
$\w$             &  $\in$  &  $\R^d$  & The vector of model parameters \\
$\mathbf{1}_m$   &  $\in$  &  $\R^m$  & The $m$ dimensional vector of $1$'s \\
$\X^+$           &  $\in$  &  $\R^{m \times d}\quad$  & Matrix of features of positive examples \\
$\X^-$           &  $\in$  &  $\R^{n \times d}$       & Matrix of features of negative examples \\
$\alphabm$       &  $\in$  &  $\R^m$  &  Dual variables for positive examples \\
$\betabm, \nubm$ &  $\in$  &  $\R^n$  &  Dual variables for negative examples \\ \hline
\end{tabular}
\end{table}

We can learn the model parameters $\w$ by risk minimisation with L2 regularisation:
\begin{equation}
\label{eq:minrisk}
\min_{\w} \, \frac{\lambda}{2} \w^\top \w + \frac{1}{m} \sum_{i=1}^m \ell\left( \max_{1 \le j \le n} \w^\top \x_j^- - \w^\top \x_i^+ \right),
\end{equation}
where $\lambda > 0$ is a regularisation constant.
Problem (\ref{eq:minrisk}) is hard to optimise in general due to the maximum term in loss function, one widely used trick is to form its dual problem.
Let 
\begin{equation*}
\begin{aligned}
f_0 (\w, \xi) &= \frac{\lambda}{2} \w^\top \w + \frac{1}{m} \sum_{i=1}^m \ell\left( \xi - \w^\top \x_i^+ \right), \\
f_j (\w, \xi) &= \w^\top \x_j^- - \xi, \ j \in \{1,\dots,n\}.
\end{aligned}
\end{equation*}
Then problem (\ref{eq:minrisk}) is equivalent to
\begin{equation}
\label{eq:minrisk_lg}
\begin{aligned}
\min_{\w, \xi} \quad & f_0 (\w, \xi) \\
s.t. \quad & f_j (\w, \xi) \le 0, \ j \in \{1,\dots,n\}.
\end{aligned}
\end{equation}
The \emph{Lagrangian} of (\ref{eq:minrisk_lg}) is
\begin{align*}
L(\w, \xi, \nubm) 
&= f_0 (\w, \xi) + \sum_{j=1}^n \nu_j \cdot f_j(\w, \xi) \\
&= \frac{\lambda}{2} \w^\top \w + \frac{1}{m} \sum_{i=1}^m \ell\left( \xi - \w^\top \x_i^+ \right) + \sum_{j=1}^n \nu_j \cdot \left( \w^\top \x_j^- - \xi \right)
\end{align*}
Note that the conjugate of the conjugate of a convex function is itself, \eg $f(\z) = f^{**}(\z) = \sup_{\y} \left( \z^\top \y - f^*(\y) \right)$, we have
\begin{align*}
\frac{1}{m} \sum_{i=1}^m \ell\left( \xi - \w^\top \x_i^+ \right)
&= \frac{1}{m} \sum_{i=1}^m \sup_{\alpha_i} \left( (\xi - \w^\top \x_i^+) \cdot \alpha_i - \ell^*(\alpha_i) \right) \\
&= \sup_{\alphabm} \left[ \frac{1}{m} \sum_{i=1}^m (\xi - \w^\top \x_i^+) \cdot \alpha_i - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right] \\
&= \sup_{\alphabm} \left[ \frac{\xi}{m} \1_m^\top \alphabm - \frac{1}{m} \alphabm^\top \X^+ \w - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right] \\
\end{align*}
where $\ell^*(\cdot)$ is the conjugate of $\ell(\cdot)$.
Further, 
$$\sum_{j=1}^n \nu_j \cdot \left( \w^\top \x_j^- - \xi \right) = \nubm^\top \X^- \w - \xi \1_n^\top \nubm$$
Then
\begin{align*}
L(\w, \xi, \alphabm, \nubm) 
&= \frac{\lambda}{2} \w^\top \w + 
   \sup_{\alphabm} \left[ \frac{\xi}{m} \1_m^\top \alphabm - \frac{1}{m} \alphabm^\top \X^+ \w - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right] +
   \nubm^\top \X^- \w - \xi \1_n^\top \nubm \\
&= \sup_{\alphabm} \left[ 
   \frac{\lambda}{2} \w^\top \w + 
   \frac{\xi}{m} \1_m^\top \alphabm - \frac{1}{m} \alphabm^\top \X^+ \w - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) +
   \nubm^\top \X^- \w - \xi \1_n^\top \nubm \right] \\
&= \sup_{\alphabm} \left[ g(\w, \xi) - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right]
\end{align*}
where
$$g(\w, \xi) = \frac{\lambda}{2} \w^\top \w + \frac{\xi}{m} \1_m^\top \alphabm - \frac{1}{m} \alphabm^\top \X^+ \w + \nubm^\top \X^- \w - \xi \1_n^\top \nubm$$
The \emph{Lagrangian dual function} of (\ref{eq:minrisk_lg}) is
\begin{align*}
\inf_{\w, \xi} \, L(\w, \xi, \alphabm, \nubm) 
&= \inf_{\w, \xi}  \, \sup_{\alphabm} \left[ g(\w, \xi) - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right] \\
&= \sup_{\alphabm} \, \inf_{\w, \xi} \left[ g(\w, \xi) - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right] ~~ \text{(assuming strong duality)} \\
&= \max_{\alphabm} \, \min_{\w, \xi} \left[ g(\w, \xi) - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right] ~~ \text{(Equation (\ref{eq:minrisk}) is L2 regularised~\cite{shalev:2007})} \\
&= \max_{\alphabm} \left[ \min_{\w, \xi} g(\w, \xi) - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right]
\end{align*}
To solve the unconstrained inner minimisation, let
\begin{align*}
\frac{\partial g}{\partial \w}  &= \lambda \w - \frac{1}{m} \alphabm^\top \X^+ + \nubm^\top \X^- = 0 \\
\frac{\partial g}{\partial \xi} &= \frac{1}{m} \1_m^\top \alphabm - \1_n^\top \nubm = 0
\end{align*}
Then we have
$$
\w^* 
= \frac{1}{\lambda m} \left( \alphabm^\top \X^+ - m \nubm^\top \X^- \right) 
= \frac{1}{\lambda m} \left( \alphabm^\top \X^+ - \betabm^\top \X^- \right) 
$$
and
$$\1_m^\top \alphabm = m \1_n^\top \nubm = \1_n^\top \betabm$$
where $\betabm = m \nubm$, and
\begin{align*}
\min_{\w, \xi} g(\w, \xi) 
&= \frac{\lambda}{2} \w^{*\top} \w^* + \frac{\xi}{m} \1_m^\top \alphabm - \frac{1}{m} \alphabm^\top \X^+ \w^* + \nubm^\top \X^- \w^* - \xi \1_n^\top \nubm \\
&= \frac{\lambda}{2} \w^{*\top} \w^* + 
   \frac{\xi}{m} \left( \1_m^\top \alphabm - m \1_n^\top \nubm \right) - 
   \frac{1}{m} \left( \alphabm^\top \X^+ - m \nubm^\top \X^- \right)^\top \w^* \\
&= \frac{\lambda}{2} \w^{*\top} \w^* + 0 - \lambda \w^{*\top} \w^* \\
&= -\frac{\lambda}{2} \w^{*\top} \w^* \\
&= -\frac{1}{2 \lambda m^2} \left\| \alphabm^\top \X^+ - \betabm^\top \X^- \right\|^2
\end{align*}
Lastly, the \emph{Lagrangian dual problem} of (\ref{eq:minrisk_lg}) is
\begin{align*}
\max_{\nubm} \, \inf_{\w, \xi} \, L(\w, \xi, \alphabm, \nubm) 
= \max_{\alphabm, \nubm} \, \left[ \min_{\w, \xi} g(\w, \xi) - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right]
= \max_{\alphabm, \nubm} \, \left[ -\frac{1}{2 \lambda m^2} \left\| \alphabm^\top \X^+ - \betabm^\top \X^- \right\|^2 - \frac{1}{m} \sum_{i=1}^m \ell^*(\alpha_i) \right]
\end{align*}
subject to $\1_m^\top \alphabm = m \1_n^\top \nubm$,
which is equivalent to
\begin{equation}
\label{eq:minrisk_dual}
\begin{aligned}
\min_{\alphabm, \betabm} \quad & \frac{1}{2 \lambda m} \left\| \alphabm^\top \X^+ - \betabm^\top \X^- \right\|^2 + \sum_{i=1}^m \ell^*(\alpha_i) \\
s.t. \quad & \1_m^\top \alphabm = \1_n^\top \betabm.
\end{aligned}
\end{equation}


\section{Playlist generation}
\label{sec:playlist}

%2. Formal problem statement (e.g. input, output)
%\subsection{Problem formulation}

Given $n$ playlists where songs in each playlist are from a music library with $m$ songs $\{s_j\}_{j=1}^m$.
%For a query $q$ (\eg a seed song) and each song $s_i, i=\{1,\dots,m\}$,
%we have a scoring function $f: (q, s_i) \to \R$ (\eg $f(q, s_i) = \w^\top \Psi(q, s_i)$ 
%where $\w$ is the model parameters and $\Psi(q, s_i)$ is a feature map).
%Further, for a query related to a particular playlist, we want the songs in the playlist to be ranked higher (get higher scores)
%than songs that are not in the playlist.
%The prediction given a query can be simply taking the top-$k$ ranked songs.

For the $i$-th playlist, %we derive a set of examples $\{\x\pb{i}, \y\pb{i}\}$ where
we derive a vector of labels $\y\pb{i} = y_{1:m}\pb{i}$ where
$$
y_j\pb{i} = 
\begin{cases}
+1, & \text{song $s_j$ occurred in the $i$-th playlist} \\
-1, & \text{otherwise}
\end{cases}
$$
and a set of training examples
$$\left\{ \left( \x_j\pb{i}, \y\pb{i} \right) \colon y_j\pb{i} = +1 \right\}_{i=1}^n$$
where $\x_j\pb{i} = \phi_j$ which denotes the feature vector of song $s_j$.

%The playlist generation problem can be stated as follows:
%\begin{mdframed}[innertopmargin=3pt,innerbottommargin=3pt,skipbelow=5pt,roundcorner=8pt,backgroundcolor=red!3,topline=false,rightline=false,leftline=false,bottomline=false]
%	\begin{tabular}{ll}
%		{\sc Input}:  & training set $\left\{ \left( \x^{(i)}, \y^{(i)} \right) \right\}_{i = 1}^n \in ( \XCal \times \YCal )^n$ \\
%		{\sc Output}: & a playlist generator $g \colon \XCal \to \YCal$ \\
%	\end{tabular}
%\end{mdframed}

Let $m_i$ be the number of songs in the $i$-th playlist, \ie 
$$
m_i = \sum_{j=1}^m \llb y_j\pb{i} = +1 \rrb,
$$
and $\Psi(\x, s_j)$ be a feature map of song $s_j$ given query $\x$ (\ie the feature vector of a seed song).

Suppose we learn a playlist generator using the probabilistic classifier chains~\cite{dembczynski:2010},
in particular, we assume a logistic regression classifier for each label $y_j$, \ie
\begin{align*}
\p(y_1 |\x) &= \sigma\left( \w_1^\top \Psi(\x, s_1) \right) \\
\p(y_j |\x, y_{1:j-1}) &= \sigma\left( \w_j^\top \Psi(\x, s_j, y_{1:j-1}) \right), \, j=2,\dots,m
\end{align*}
where $\sigma(z) = \frac{1}{1+\exp(-z)}$ is the logistic function.

\emph{$\Psi(\x, s_j, y_{1:j-1})$ looks weird!}

The scoring function
$$
f_j(\x) = 
\begin{cases}
\p(y_j |\x), & j=1 \\
\p(y_j |\x, y_{1:j-1}), & j=2,\dots,m
\end{cases}
$$

The empirical risk is
$$
R(\f) 
= \frac{1}{n} \sum_{i=1}^n 
  \frac{1}{m_i} \sum_{q=1}^m \llb y_q\pb{i} = +1 \rrb 
  \cdot \left[
  \frac{1}{m_i} \sum_{j=1}^m 
  \llb y_j\pb{i} = +1 \rrb \cdot \ell \left( \max_{1 \le k \le m} \llb y_k\pb{i} = -1 \rrb \cdot f_k(\phi_q) - f_j(\phi_q) 
  \right) \right]
$$ 


\eat{
%3. Baseline approaches (e.g. song-song matrix factorisation, model each label independently, model label pairs with a number of ranking losses)
\subsection{Baselines}

\paragraph{Matrix factorisation}
Let $\phi(s_i)$ be the (hidden) features of song $s_i$, we can construct a $N \times N$ matrix of song co-occurrences (normally very sparse),
the missing entries can be recovered by factorising this matrix into two low rank matrices.
The scoring function of this approach is $f(q, s_i) = \phi(q)^\top \phi(s_i)$.

\paragraph{Logistic regression}
We can model the probability of song $s_i$ in a playlist induced by query $q$ using logistic regression,
\ie the scoring function is $f(q, s_i) = \sigma(\w^\top \Psi(q, s_i))$ where $\sigma(z) = \frac{1}{1+\exp(-z)}$ is the logistic function.
To estimate the model parameters $\w$, for each observed playlist, we derive a set of positive examples $\{((q, s_j), +1)\}$ and 
a set of negative examples $\{(q, s_k), -1)\}$ where $s_j$ are song in the playlist and $s_k$ are songs not in it.

\paragraph{Bipartite ranking by optimising AUC}
Similar to the logistic regression approach, we derive a set of positive/negative examples for each observed playlist, 
inspired by bipartite ranking, we can optimise AUC by minimise a loss similar to Equation~\ref{eq:loss_auc}:
\begin{equation}
\label{eq:loss_auc_pl}
\ell_\text{rank}(f; S) = \frac{1}{N} \sum_{i=1}^N \frac{1}{m_i n_i} \sum_{j=1}^{m_i} \sum_{k=1}^{n_i} \llb f(q_i, s_j) \le f(q_i, s_k) \rrb,
\end{equation}
where $m_i$ (and $n_i$) is the number of songs (not) in the $i$-th playlist, and $m_i + n_i = M$.
\\ \emph{describe pros and cons}

\subsection{Proposed approach}
Inspired by bipartite ranking by optimising accuracy at the top, we can optimise a loss similar to Equation~\ref{eq:loss_inf1}:
\begin{equation}
\label{eq:loss_inf_pl}
\hat{\ell}_{\infty}(f; S) = \frac{1}{N} \sum_{i=1}^N \frac{1}{m_i} \sum_{j=1}^{m_i} \ell\left( \max_{1 \le k \le n_i} f(q_i, s_k) - f(q_i, s_j) \right),
\end{equation}
where $m_i$ and $n_i$ is define as before.

\paragraph{Training set}
We can derive a set of positive/negative examples for each observed playlist as described before, however,
for the $i$-th playlist, there are $m_i$ songs that can be used as a \emph{seed}, so we have multiple queries for each observed playlist.
Formally, we have a training set:
\begin{equation*}
\left\{ \left\{ 
        \left\{\left((q_{il}, s_j), +1\right) \right\}_{j=1}^{m_i}, \,
        \left\{\left((q_{il}, s_k), -1\right) \right\}_{k=1}^{n_i}  \right\}_{l=1}^{m_i} \right\}_{i=1}^N
\end{equation*}
\emph{describe approach that considers label correlations}

%The playlist generation problem can be formulated as a multi-label classification problem (we have a label for each song in library).
%The idea is to replace the loss function (\ref{eq:loss_rank}) in multi-label classification with loss~(\ref{eq:loss_auc}) or (\ref{eq:loss_inf}). \\
%We can build a probabilistic classifier chains with approximate inference (greedy, Viterbi, Neural net).
}


%4. Evaluation measure (e.g. Precision@k, Average precision, Reciprocal precision)
\subsection{Evaluation measure}
Evaluation measure such as Precision@k, Average precision, Reciprocal precision can be used.
\\ \emph{described details of the above measures}

%\bibliographystyle{ieeetr}
%\bibliographystyle{apalike}
\bibliographystyle{plainnat}
\bibliography{ref_mlc}

\end{document}
