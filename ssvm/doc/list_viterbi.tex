\section{List Viterbi algorithm}
\label{sec:lva}

There are many variants of generalising the Viterbi algorithm (VA) to find the 
$1$st, $2$nd, $\dots$, $k$-th best sequences~\cite{seshadri1994list,nilsson2001sequentially},
here we call them the list Viterbi algorithm (LVA).

We compare three typical variants of the LVA, mainly their time/space complexity.
Firstly, we define notations that will be used in this section, we denote:
\begin{itemize}
\item $n$: the (maximum) number of states of (all) random variables in the HMM.
\item $l$: the length of sequence, \ie, the number of random variables in the chain.
\item $k$: the total number of best sequences that are going to be identified.
\end{itemize}
We will also use $i$ to index the possible states of random variables, \ie, $i \in \{1, \dots, n\}$,
and use $t$ to index the locations/(time instants) in the chain/sequence, \ie, $t \in \{1, \dots, l\}$.

\subsection{Parallel LVA}
\label{sec:plva}

The parallel LVA finds the $k$ best sequences simultaneously by computing the $k$ best sequences in each state at each time instant.
The basic idea of parallel LVA is storing all the $k$ best scores instead of the $1$st best score in VA at each time instant,
which means we need to find the $k$ best scores among the $n\cdot k$ accumulated scores at each state and time instant 
(Eq. (6) in~\cite{seshadri1994list}).

The time complexity for this step (Eq. (6) in~\cite{seshadri1994list})
is $O \left( nk\cdot \log(nk) \right)$ if the $k$ best scores are found by sorting the $n\cdot k$ accumulated scores,
or $O \left( k \cdot nk \right)$ if the $k$ best scores are found by order statistics algorithms~\cite{clrs2009}, 
which is more expensive if $k$ is very large (\ie when $k > \log(nk)$),
so for this analysis, we assume this step is implemented by sorting.
As we are doing this step at each state and time instant, the total time complexity is $O \left( n \cdot l \cdot nk \cdot \log(nk) \right)$.
Intuitively, we are computing elements in a cube $n \times l \times k$, 
the third dimension needs a sorting to get the $k$ best elements in $nk$ elements.

The space complexity is $O ( nlk + nk )$, \ie $O(nlk)$ for all sequence states and $O(nk)$ for $nk$ accumulated scores at each time instant.


\subsection{Serial LVA}
\label{sec:slva}

In practice, we usually do not know the value of $k$ beforehand, this is where the serial LVA comes into play.
The serial LVA finds the $k$ best sequences one at a time, which is more flexible than the parallel LVA described in Section~\ref{sec:plva}.
This algorithm makes use of the observation that the globally $2$nd best sequence diverges from the globally $1$st best sequence 
at some time instant, and it merges with the globally $1$ best sequence at a later time instant and never diverges again, 
since any subsequent divergence will result in a higher cost (lower priority) sequence.
Similarly, the $3$rd globally best sequence can be identified from the locally $2$nd best sequences w.r.t. the globally $1$ best sequence 
(except the globally $2$ best sequence) or the locally $2$nd best sequences w.r.t. the globally $2$nd best sequence.
This reasoning can be generalised to find the $k$-th best sequence given the $1$st, $2$nd, \dots, $(k-1)$-th best sequences.

The time complexity of the serial LVA is composed of three parts:
\begin{itemize}
\item using the Viterbi algorithm to find the globally best sequence: $O(n^2 l)$ as the $n$-by-$l$ table stores scores and 
      computing each cell is upper bounded by $O(n)$.
\item computing locally $2$nd best sequences after identifying the globally $j$-th $(j=1,\dots,k-1)$ best sequence, 
      which requires $n$ additions and comparisons (Eq. (9) in~\cite{seshadri1994list}) at each time instant $t \in \{1, \dots, l\}$, 
      which is upper bounded by $O(nl)$. Thus, for all $k$ best sequences, the time complexity is $O (k \cdot nl)$.
\item additional cost related to insert each candidate sequence and its score into data structure (\eg, priority queue), 
      and extract the best scored sequence among all candidates, upper bounded by $O \left( kl \log (kl) \right)$ 
      (the total number of sequences in priority queue is $kl$ at most, each insertion/extraction requires $\log (N)$ operations, 
       where $N$ is the number of elements in priority queue, 
       this bound is not tight\footnote{A tighter bound is 
       $O \left( \sum_{j=1}^k \log(j\cdot l) \right) = O( k\log(l) + \log(k\,!) )$, 
       which can be further simplified by using Stirling's formula~\cite{stirling}.\label{foot:bound}}).
      However, if we create a priority queue to store the $2$nd locally best sequence for each globally $j$-th $(j=1,\dots,k-1)$ best sequence,
      and use an additional priority queue to store the best sequence in all the $k-1$ priority queues, 
      we have another bound:
      \begin{itemize}
      \item insertion: $(k-1) \cdot \left( \log(1) + \log(2) + \cdots + \log(l) \right) = (k-1)\log(l\,!) \le kl\log(l)$
      \item extracting best: $\log(l) + 2\log(l) + \cdot + (k-1)\log(l) = \frac{k(k-1)}{2}\log(l) \le k^2 \log(l)$
      \item find the $j$-th $(j=2,\dots,k)$ globally best: $\log(2) + \log(3) + \cdots + \log(k) = \log(k\,!) \le k\log(k)$
      \end{itemize}
      which results in $O \left( kl\log(l) + k^2\log(l) + k\log(k) \right) = O ( k(k+l)\log(l) + k\log(k) ) $
      time complexity for insertions/extractions operations. If $k \gg l$, $O(kl \log(kl))$ is a better bound.
\end{itemize}
Adding up these terms, the time complexity of serial LVA is $O \left( n^2 l + nlk + kl\log(kl) \right)$.
The space complexity is $\Omega(nl + kl + nlk)$ (Eq. (9), path matrix and the "merge" count array in~\cite{seshadri1994list}).


\subsection{Nilsson and Goldberger algorithm}
\label{sec:nglva}

Another approach to find all the $k$ best sequences in a sequential manner was proposed by Nilsson and Goldberger~\cite{nilsson2001sequentially},
instead of focusing on when the $2$nd locally best sequence \textit{merges} with globally best sequence (Section~\ref{sec:slva}),
this algorithm focuses on when the $2$nd locally best sequence \textit{diverges}.
The algorithm makes use of results computed by the forward-backward algorithm~\cite{rabiner1989tutorial}.
The space of possible sequences is first cleverly partitioned into subsets, 
then the best sequence in each subset is computed and identified using the forward-backward results,
the procedure is repeated until all $k$ best sequences have been found.

The time complexity of this algorithm is composed of three parts:
\begin{itemize}
\item forward-backward procedure: $O(n^2 l)$.
\item partitioning and computing the scores of the best sequence in each subset/partition: $O(k \cdot nl)$.
\item insertion (and extraction) sequences into (and from) data structure (\eg, priority queue etc): $O(kl \log (kl))$\footref{foot:bound},
      we can also use the trick described in Section~\ref{sec:slva}, but here we care about the setting $k \gg l$.
\end{itemize}
Adding up and the time complexity of this algorithm: $O \left( n^2 l + nlk + kl\log(kl) \right)$.
The space complexity of this algorithm is $O (nl + kl \cdot (1 + l + 1 + n) ) = O(nl + kl^2 + nlk)$.


\subsection{Summary}
We summarise the time/space complexity of the three variants in Table~\ref{tab:variants}.

\begin{table}[ht]
\caption{Time and space complexities of three variants of the list Viterbi algorithm}
\label{tab:variants}
\centering
%\setlength{\tabcolsep}{28pt} % tweak the space between columns
\begin{tabular}{|l|l|l|} \hline
\textbf{Algorithm variants}  & \textbf{Time complexity} & \textbf{Space complexity} \\ \hline
Parallel LVA~\cite{seshadri1994list} & $O \left( n^2 lk \log(nk) \right)$ or $O \left( n^2 k^2 l \right)$ & $O(nlk + nk)$ \\ \hline
Serial LVA~\cite{seshadri1994list}   & $O \left( n^2 l + nlk + kl\log(kl) \right)$ & $\Omega(nl + kl + nlk)$ \\ \hline
Nilsson and Goldberger~\cite{nilsson2001sequentially} & $O \left( n^2 l + nlk + kl\log(kl) \right)$ & $O(nl + kl^2 + nlk)$ \\ \hline
\end{tabular}
\end{table}


\begin{algorithm}[htbp]
\caption{The list Viterbi algorithm for top-$K$ prediction~\cite{nilsson2001sequentially}}
\begin{algorithmic}[1]
\STATE \textbf{Input}: $\mathbf{x}=(s, l),~ \mathcal{P},~ \mathbf{w},~ \Psi, ~K$
%\STATE Initialise score matrices $\alpha,~ \beta,~ f_t,~ f_{t, t+1}$, a max-heap $H,~ k=0$.
\STATE Initialise score matrices $\alpha,~ \beta,~ f_{t, t+1}$, a max-heap $H$, result set $R$, $k=0$.
\STATE $\triangleright$ Do the forward-backward procedure \vspace{3pt}
\STATE $\forall p_j \in \mathcal{P},~ \alpha_t(p_j) =
        \begin{cases}
         0, & t = 1 \\
         \max_{p_i \in \mathcal{P}} \left\{ \alpha_{t-1}(p_i) + \mathbf{w}_{ij}^\top \psi_{ij}(\mathbf{x}, p_i, p_j) \right\} +
         \mathbf{w}_j^\top \psi_j(\mathbf{x}, p_j), & t=2,\dots,l
        \end{cases}$\vspace{3pt}
\STATE $\forall p_i \in \mathcal{P},~ \beta_t(p_i) =
         \begin{cases}
          0, & t = l \\
          \max_{p_j \in \mathcal{P}} \left\{ \mathbf{w}_{ij}^\top \psi_{ij}(\mathbf{x}, p_i, p_j) +
          \mathbf{w}_j^\top \psi_j(\mathbf{x}, p_j) + \beta_{t+1}(p_j) \right\}, & t = l-1,\dots,1
         \end{cases}$
%\STATE $\forall p_i \in \mathcal{P},~ f_t(p_i) = \alpha_t(p_i) + \beta_t(p_i),~ t = 1,\dots,K$
\STATE \vspace{3pt}$\forall p_i, p_j \in \mathcal{P},~ f_{t,t+1}(p_i, p_j) = \alpha_t(p_i) + \mathbf{w}_{ij}^\top \psi_{ij}(\mathbf{x}, p_i, p_j) +
                             \mathbf{w}_j^\top \psi_j(\mathbf{x}, p_j) + \beta_{t+1}(p_j),~ t = 1,\dots,l-1$

\STATE \vspace{3pt}$\triangleright$ Identify the best (scored) trajectory $\mathbf{y}^1=(y_1^1,\dots,y_l^1)$, it could be a trajectory that violates the desired condition.
\STATE $y_t^1 = \begin{cases}
                  s, & t = 1 \\
                  \argmax_{p \in \mathcal{P}} \left\{ f_{t-1,t}(y_{t-1}^1, p) \right\}, & t = 2,\dots,l
                 \end{cases}$

%\STATE $r^1 = \max_{p \in \mathcal{P}} \left\{ f_K(p) \right\}~~~ \triangleright$ $r^1$ is the score/priority of $\mathbf{y}^1$
\STATE \vspace{3pt}$r^1 = \max_{p \in \mathcal{P}} \left\{ \alpha_{l}(p) \right\}~~~ \triangleright$ $r^1$ is the score/priority of $\mathbf{y}^1$

\STATE \vspace{3pt}$H.\textit{push}\left(r^1,~ (\mathbf{y}^1, \textsc{nil}, \emptyset) \right)$
\STATE \vspace{3pt}Set $R=\emptyset$, the list of trajectories to be returned.\vspace{3pt}
\WHILE{$H \ne \emptyset$ \textbf{and} $k < \,|\mathcal{P}|^{l-1} - \prod_{t=2}^l (|\mathcal{P}|-t+1) + K$}
    \STATE \vspace{3pt}$r^k,~ (\mathbf{y}^k, I, S) = H.\textit{pop}()~~~ \triangleright$
           $r^k$ is the score of $\mathbf{y}^k=(y_1^k,\dots,y_l^k)$, $I$ is the partition index, and $S$ is the exclude set
    \STATE \vspace{3pt}$k = k + 1$
    \STATE \vspace{3pt}Add $\mathbf{y}^k$ to $R$ if it satisfies the desired property\vspace{3pt}
    \RETURN \vspace{3pt}$R$ if it contains the required number of trajectories
    \STATE \vspace{3pt}$I' = \begin{cases}
                  2, & I = \textsc{nil} \\
                  I, & \text{otherwise}
                 \end{cases}$\vspace{3pt}

    \FOR{$t = I',\dots,l$}
        \STATE \vspace{3pt}$S' = \begin{cases}
                      S \cup \{ y_t^k \}, & t = I' \\
                      \{ y_t^k \},        & \text{otherwise}
                     \end{cases}$\vspace{3pt}

        \STATE $y'_j = \begin{cases}
                            y_j^k,                                                                             & j=1,\dots,t-1 \\
                            \argmax_{p \in \mathcal{P} \setminus S'} \left\{ f_{t-1,t}(y_{t-1}^k, p) \right\}, & j=t \\
                            \argmax_{p \in \mathcal{P}} \left\{ f_{j-1, j}(y'_{j-1}, p) \right\},              & j=t+1,\dots,l
                \end{cases}$
        %\STATE $\bar{r} = \begin{cases}
        %                  f_{t-1,t}(y_{t-1}^k, \bar{y}_t),&I = \textsc{nil} \\
        %                  r^k + f_{t-1,t}(y_{t-1}^k, \bar{y}_t) - f_{t-1,t}(y_{t-1}^k, y_t^k), &\text{otherwise}
        %                  \end{cases}$
        \STATE \vspace{3pt}$r' = r^k + f_{t-1,t}(y_{t-1}^k, y'_t) - f_{t-1,t}(y_{t-1}^k, y_t^k)$\vspace{3pt}

        \STATE \vspace{3pt}$H.\textit{push}(r', (\y', t, S') )$ \vspace{5pt}
    \ENDFOR\vspace{3pt}
\ENDWHILE
\end{algorithmic}
\end{algorithm}
