{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Trajectory Recommendation - MEMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of contents:\n",
    "1. [MEMM with first order MC](#1.-MEMM-with-first-order-MC)\n",
    " 1. [Cost Function](#1.1-Cost-Function)\n",
    " 1. [Inference](#1.2-Inference)\n",
    " 1. [Cross Validation](#1.3-Cross-Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% matplotlib inline\n",
    "\n",
    "import os, sys, time, pickle\n",
    "import math, random, itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heapq as hq\n",
    "from scipy.optimize import minimize\n",
    "from scipy.misc import logsumexp\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shared import TrajData, evaluate, do_evaluation\n",
    "from ssvm import calc_node_features\n",
    "from ssvm import calc_node_features, calc_edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(1234567890)\n",
    "np.random.seed(1234567890)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_ix = 0\n",
    "data_dir = 'data/data-new'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_obj = TrajData(dat_ix, data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_JOBS = 6         # number of parallel jobs\n",
    "ABS_SCALER = False  # feature scaling, True: MaxAbsScaler, False: MinMaxScaler #False: StandardScaler\n",
    "C_SET = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000]  # regularisation parameter\n",
    "MC_PORTION = 0.1   # the portion of data that sampled by Monte-Carlo cross-validation\n",
    "MC_NITER = 5       # number of iterations for Monte-Carlo cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MEMM with first order MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Cost Function and its gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function and its gradient for MEMM with first order MC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "from scipy.misc import logsumexp\n",
    "cimport numpy as np\n",
    "\n",
    "cpdef MEMM_1MC_obj(w, X_node_all, X_edge, list Y, float C, long M):\n",
    "    \"\"\"\n",
    "    w - parameter vector\n",
    "    X_node_all - feature matrix of POIs for all training examples, (N x M) x n_node_features\n",
    "    X_edge - transition feature matrix of POIs, M x M x n_edge_features\n",
    "    Y - labels/trajectories for all training examples\n",
    "    C - regularisation constant\n",
    "    M - total number of POIs\n",
    "    return the cost and the gradient of cost\n",
    "    \"\"\"\n",
    "    #print('entering MEMM_obj')\n",
    "    assert(C > 0)\n",
    "    assert(M > 0)\n",
    "    cdef long N, D, i, j, pj, pk, pl\n",
    "    N = int(np.shape(X_node_all)[0]/M)\n",
    "    D = np.shape(X_node_all)[1] * 2 + np.shape(X_edge)[2]\n",
    "    assert(D == np.shape(w)[0])\n",
    "    assert(N == len(Y))\n",
    "    \n",
    "    cdef double cost, costi, denorminator\n",
    "    cost = 0.0\n",
    "    grad = np.zeros(D, dtype=np.float)\n",
    "    for i in range(N):\n",
    "        costi = 0.0\n",
    "        gradi = np.zeros(D, dtype=np.float)\n",
    "        for j in range(1, np.shape(Y[i])[0]):\n",
    "            pj = Y[i][j-1]  # index of feature vector for POI p_{j-1}\n",
    "            pk = Y[i][j]\n",
    "            phi_j = np.hstack([X_node_all[i*M + pj], X_edge[pj, pk], X_node_all[i*M + pk]])\n",
    "            costi -= np.dot(w, phi_j)\n",
    "            gradi = gradi - phi_j\n",
    "            cost_values = np.zeros(M, dtype=np.float)\n",
    "            denorminator = 0.0\n",
    "            numerator = np.zeros(D, dtype=np.float)\n",
    "            for pl in range(M):\n",
    "                phi_l = np.hstack([X_node_all[i*M + pj], X_edge[pj, pl], X_node_all[i*M + pl]])\n",
    "                term = np.dot(w, phi_l)\n",
    "                cost_values[pl] = term\n",
    "                expterm = np.exp(term)\n",
    "                numerator = numerator + phi_l * expterm\n",
    "                denorminator += expterm\n",
    "            costi += logsumexp(cost_values)\n",
    "            gradi = gradi + numerator / denorminator\n",
    "        cost += costi / N\n",
    "        grad = grad + gradi / N\n",
    "    cost = 0.5 * np.dot(w, w) + C * cost\n",
    "    grad = w + C * grad\n",
    "    #print('exit MEMM_obj')\n",
    "    return (cost, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M0, L0 = 5, 3\n",
    "f_u = np.array([2, 1, 1, 3, 1], dtype=np.float).reshape((M0, 1))\n",
    "f_p = np.array([1,2,1,1,1, 1,1,1,1,3, 2,1,1,1,1, 1,1,3,1,1, 1,1,1,2,1], dtype=np.float).reshape((M0, M0, 1))\n",
    "w0 = np.array([1, 1, 2], dtype=np.float)\n",
    "ps0 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M0, L0 = 6, 4\n",
    "f_u = np.array([2, 1, 1, 2, 1, 1], dtype=np.float).reshape((M0, 1))\n",
    "f_p = np.array([1,2,1,1,1,1, 1,1,1,1,3,2, 2,1,1,1,1,2, 1,1,3,1,1,1, 1,1,1,2,1,1, 2,1,1,2,1,1], \n",
    "               dtype=np.float).reshape((M0, M0, 1))\n",
    "w0 = np.array([1, 2, 1], dtype=np.float)\n",
    "ps0 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference for MEMM with first order MC by brute force search (for sanity check)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MEMM_inference_bruteForce(ps, L, M, w, X_node, X_edge):\n",
    "    assert(L > 1)\n",
    "    assert(M >= L)\n",
    "    assert(ps >= 0)\n",
    "    assert(ps < M)\n",
    "            \n",
    "    Q = []\n",
    "    for x in itertools.permutations([p for p in range(M) if p != ps], L-1):\n",
    "        #print([ps] + list(x))\n",
    "        y = [ps] + list(x)\n",
    "        score = 0\n",
    "        for j in range(1, L):\n",
    "            ss = y[j-1]\n",
    "            tt = y[j]\n",
    "            score += np.dot(w, np.hstack([X_node[ss, :], X_edge[ss, tt], X_node[tt, :]]))\n",
    "            score -= logsumexp([np.dot(w, np.hstack([X_node[ss,:], X_edge[ss,pp], X_node[pp,:]])) for pp in range(M)])\n",
    "        priority = -score\n",
    "        hq.heappush(Q, (priority, y))\n",
    "    \n",
    "    k = 20\n",
    "    while k > 0 and len(Q) > 0:\n",
    "        priority, pathwalk = hq.heappop(Q)\n",
    "        print(pathwalk, -priority)\n",
    "        k -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMM_inference_bruteForce(ps0, L0, M0, w0, f_u, f_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference for MEMM with first order MC using the Viterbi algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MEMM_inference_viterbi(ps, L, M, w, X_node, X_edge, debug=False):\n",
    "    assert(L > 1)\n",
    "    assert(M >= L)\n",
    "    assert(ps >= 0)\n",
    "    assert(ps < M)\n",
    "    assert(w.shape[0] == X_node.shape[1]*2 + X_edge.shape[2])\n",
    "    \n",
    "    A = np.zeros((L-1, M), dtype=np.float)     # scores matrix\n",
    "    B = np.ones((L-1, M), dtype=np.int) * (-1) # backtracking pointers\n",
    "    \n",
    "    for p in range(M): # ps--p\n",
    "        A[0, p] = np.dot(w, np.hstack([X_node[ps, :], X_edge[ps, p], X_node[p, :]])) - \\\n",
    "                  logsumexp([np.dot(w, np.hstack([X_node[ps,:], X_edge[ps,pp], X_node[pp,:]])) for pp in range(M)]) \\\n",
    "                  if ps != p else -np.inf\n",
    "        B[0, p] = ps\n",
    "\n",
    "    for t in range(0, L-2): # ps~~p1--p\n",
    "        for p in range(M):\n",
    "            scores = [A[t, p1] + \\\n",
    "                      np.dot(w, np.hstack([X_node[p1,:], X_edge[p1,p], X_node[p,:]])) - \\\n",
    "                      logsumexp([np.dot(w, np.hstack([X_node[p1,:], X_edge[p1,pp], X_node[pp,:]])) for pp in range(M)])\n",
    "                      if p1 not in {p,ps} else -np.inf for p1 in range(M)]\n",
    "            maxix = np.argmax(scores)\n",
    "            A[t+1, p] = scores[maxix]\n",
    "            B[t+1, p] = maxix\n",
    "            \n",
    "    if debug == True: print(A)\n",
    "    \n",
    "    y_hat = [np.argmax(A[L-2, :])]\n",
    "    p, t = y_hat[-1], L-2\n",
    "    while t >= 0:\n",
    "        y_hat.append(B[t, p])\n",
    "        p, t = y_hat[-1], t-1\n",
    "    y_hat.reverse()\n",
    "    \n",
    "    if debug == True: print(y_hat, np.max(A[L-2]))\n",
    "\n",
    "    return np.asarray(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMM_inference_viterbi(ps0, L0, M0, w0, f_u, f_p)#, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference using **the List Viterbi algorithm**, which *sequentially* find the (k+1)-th best path/walk given the 1st, 2nd, ..., k-th best paths/walks.\n",
    "\n",
    "Reference papers:\n",
    "- [*Sequentially finding the N-Best List in Hidden Markov Models*](http://www.eng.biu.ac.il/~goldbej/papers/ijcai01.pdf), Dennis Nilsson and Jacob Goldberger, IJCAI 2001.\n",
    "- [*A tutorial on hidden Markov models and selected applications in speech recognition*](http://www.cs.ubc.ca/~murphyk/Bayes/rabiner.pdf), L.R. Rabiner, Proceedings of the IEEE, 1989.\n",
    "\n",
    "Implementation is adapted from the above references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HeapItem0:  # an item in heapq (min-heap)\n",
    "    def __init__(self, priority, task):\n",
    "        self.priority = priority\n",
    "        self.task = task\n",
    "        self.string = str(priority) + ': ' + str(task)\n",
    "        \n",
    "    def __lt__(self, other):\n",
    "        return self.priority < other.priority\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.string\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MEMM_inference_listViterbi0(ps, L, M, w, X_node, X_edge, debug=False):\n",
    "    assert(L > 1)\n",
    "    assert(M >= L)\n",
    "    assert(ps >= 0)\n",
    "    assert(ps < M)\n",
    "            \n",
    "    # forward-backward procedure: adapted from the Rabiner paper\n",
    "    Alpha = np.zeros((L, M), dtype=np.float)  # alpha_t(p_i)\n",
    "    Beta  = np.zeros((L, M), dtype=np.float)  # beta_t(p_i)\n",
    "    lognorm0 = logsumexp([np.dot(w, np.hstack([X_node[ps,:], X_edge[ps,pp], X_node[pp,:]])) for pp in range(M)])\n",
    "    for pj in range(M):\n",
    "        Alpha[1,pj] = np.dot(w, np.hstack([X_node[ps, :], X_edge[ps, pj], X_node[pj, :]])) - lognorm0 \\\n",
    "                      if pj != ps else -np.inf\n",
    "    for t in range(2, L):\n",
    "        for pj in range(M): # pi-->(pj fixed)\n",
    "            Alpha[t, pj] = np.max([Alpha[t-1, pi] + \\\n",
    "                                   np.dot(w, np.hstack([X_node[pi, :], X_edge[pi, pj], X_node[pj, :]])) - \\\n",
    "                                   logsumexp([np.dot(w, np.hstack([X_node[pi,:], X_edge[pi,pp], X_node[pp,:]])) \\\n",
    "                                              for pp in range(M)]) \\\n",
    "                                   if pi not in {pj, ps} else -np.inf for pi in range(M)])\n",
    "    for t in range(L-1, 1, -1):\n",
    "        for pi in range(M): # (fixed pi)-->pj\n",
    "            lognorm = logsumexp([np.dot(w, np.hstack([X_node[pi,:], X_edge[pi,pp], X_node[pp,:]])) for pp in range(M)])\n",
    "            Beta[t-1, pi] = np.max([Beta[t, pj] + \\\n",
    "                                    np.dot(w, np.hstack([X_node[pi, :], X_edge[pi, pj], X_node[pj, :]])) - lognorm \\\n",
    "                                    if pj not in {pi, ps} else -np.inf for pj in range(M)])\n",
    "    Beta[0, ps] = np.max([Beta[1, pj] + np.dot(w, np.hstack([X_node[ps,:], X_edge[ps,pj], X_node[pj,:]])) - lognorm0 \\\n",
    "                          if pj != ps else -np.inf for pj in range(M)])\n",
    "    \n",
    "    Fp = np.zeros((L-1, M, M), dtype=np.float)  # f_{t, t+1}(p, p')\n",
    "        \n",
    "    for t in range(L-1):\n",
    "        for pi in range(M):\n",
    "            lognormi = logsumexp([np.dot(w, np.hstack([X_node[pi,:], X_edge[pi,pp], X_node[pp,:]])) for pp in range(M)])\n",
    "            for pj in range(M):\n",
    "                Fp[t, pi, pj] = Alpha[t, pi] + Beta[t+1, pj] + \\\n",
    "                                np.dot(w, np.hstack([X_node[pi, :], X_edge[pi, pj], X_node[pj, :]])) - lognormi \\\n",
    "                                if pj not in {pi, ps} else -np.inf\n",
    "                \n",
    "    # identify the best path/walk: adapted from the IJCAI01 paper\n",
    "    y_best = np.ones(L, dtype=np.int) * (-1)\n",
    "    y_best[0] = ps\n",
    "    maxix = np.argmax(Fp[0, ps, :])  # the start POI is specified\n",
    "    y_best[1] = maxix\n",
    "    for t in range(2, L): \n",
    "        y_best[t] = np.argmax(Fp[t-1, y_best[t-1], :])\n",
    "        \n",
    "    Q = []  # priority queue (min-heap)\n",
    "    maxIter = 1e5\n",
    "    with np.errstate(invalid='raise'):  # deal with overflow\n",
    "        try: maxIter = np.power(M, L-1) - np.prod([M-kx for kx in range(1,L)]) + 1\n",
    "        except: maxIter = 1e5\n",
    "    if debug == True: maxIter = np.min([maxIter, 200]); print('#iterations:', maxIter) \n",
    "    else: maxIter = np.min([maxIter, 1e5])\n",
    "        \n",
    "    # heap item for the best path/walk\n",
    "    #priority, partition_index, exclude_set = -np.max(Fu[L-1, :]), None, set()  # -1 * score as priority\n",
    "    priority, partition_index, exclude_set = -np.max(Alpha[L-1, :]), None, set()  # -1 * score as priority\n",
    "    hq.heappush(Q, HeapItem0(priority, (y_best, partition_index, exclude_set)))\n",
    "    \n",
    "    if debug == True: histories = set()\n",
    "        \n",
    "    k = 0; y_last = None\n",
    "    while len(Q) > 0 and k < maxIter:\n",
    "        hitem = hq.heappop(Q)\n",
    "        k_priority, (k_best, k_partition_index, k_exclude_set) = hitem.priority, hitem.task\n",
    "        k += 1; y_last = k_best\n",
    "        \n",
    "        if debug == True: \n",
    "            #histories.add(''.join([str(x) + ',' for x in k_best]))\n",
    "            #print(k, len(histories))\n",
    "            #print('pop:', k_priority, k_best, k_partition_index, k_exclude_set)\n",
    "            print(k_best, -k_priority)\n",
    "        else:\n",
    "            if len(set(k_best)) == L: return k_best\n",
    "            \n",
    "        \n",
    "        # identify the (k+1)-th best path/walk given the 1st, 2nd, ..., k-th best: adapted from the IJCAI01 paper\n",
    "        partition_index_start = 1\n",
    "        if k_partition_index is not None:\n",
    "            assert(k_partition_index > 0)\n",
    "            assert(k_partition_index < L)\n",
    "            partition_index_start = k_partition_index\n",
    "            \n",
    "        for parix in range(partition_index_start, L):    \n",
    "            new_exclude_set = set({k_best[parix]})\n",
    "            if parix == partition_index_start:\n",
    "                new_exclude_set = new_exclude_set | k_exclude_set\n",
    "            \n",
    "            new_best = np.ones(L, dtype=np.int) * (-1)\n",
    "            for pk in range(parix):\n",
    "                new_best[pk] = k_best[pk]\n",
    "            \n",
    "            candidate_points = [p for p in range(M) if p not in new_exclude_set]\n",
    "            if len(candidate_points) == 0: continue\n",
    "            candidate_maxix = np.argmax([Fp[parix-1, k_best[parix-1], p] for p in candidate_points])\n",
    "            new_best[parix] = candidate_points[candidate_maxix]\n",
    "            \n",
    "            for pk in range(parix+1, L):\n",
    "                new_best[pk] = np.argmax([Fp[pk-1, new_best[pk-1], p] for p in range(M)])\n",
    "            \n",
    "            new_priority = Fp[parix-1, k_best[parix-1], new_best[parix]]\n",
    "            if k_partition_index is not None:\n",
    "                new_priority += (-k_priority) - Fp[parix-1, k_best[parix-1], k_best[parix]]\n",
    "            new_priority *= -1.0  # NOTE: -np.inf - np.inf + np.inf = nan\n",
    "                \n",
    "            #print(' '*3, 'push:', new_priority, new_best, parix, new_exclude_set)\n",
    "            \n",
    "            hq.heappush(Q, HeapItem0(new_priority, (new_best, parix, new_exclude_set)))\n",
    "            \n",
    "    if debug == True: print('#iterations: %d, #distinct_trajectories: %d' % (k, len(histories)))\n",
    "    sys.stderr.write('WARN: reaching max number of iterations, NO optimal solution found, return the last one.\\n')\n",
    "    return y_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMM_inference_listViterbi0(ps0, L0, M0, w0, f_u, f_p, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "#%%cython -a\n",
    "import numpy as np\n",
    "import heapq as hq\n",
    "import sys\n",
    "from scipy.misc import logsumexp\n",
    "cimport numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Inference using the List Viterbi algorithm, which sequentially find the (k+1)-th best path/walk given \n",
    "the 1st, 2nd, ..., k-th best paths/walks. \n",
    "Implementation is adapted from references:\n",
    "- Sequentially finding the N-Best List in Hidden Markov Models, Dennis Nilsson and Jacob Goldberger, IJCAI 2001.\n",
    "- A tutorial on hidden Markov models and selected applications in speech recognition, L.R. Rabiner, \n",
    "  Proceedings of the IEEE, 1989.\n",
    "\"\"\"\n",
    "\n",
    "cdef class HeapItem:  # an item in heapq (min-heap)\n",
    "    cdef readonly float priority\n",
    "    cdef readonly object task, string\n",
    "    \n",
    "    def __init__(self, float priority, task):\n",
    "        self.priority = priority\n",
    "        self.task = task\n",
    "        self.string = str(priority) + ': ' + str(task)\n",
    "        \n",
    "    #def __lt__(self, other):\n",
    "    #    return self.priority < other.priority\n",
    "    \n",
    "    def __richcmp__(self, other, int op):\n",
    "        if op == 2: # ==\n",
    "            return self.priority == other.priority\n",
    "        elif op == 3: # !=\n",
    "            return self.priority != other.priority\n",
    "        elif op == 0: # <\n",
    "            return self.priority < other.priority\n",
    "        elif op == 1: # <=\n",
    "            return self.priority <= other.priority\n",
    "        elif op == 4: # >\n",
    "            return self.priority > other.priority\n",
    "        elif op == 5: # >=\n",
    "            return self.priority >= other.priority\n",
    "        else:\n",
    "            assert False\n",
    "            \n",
    "    def __repr__(self):\n",
    "        return self.string\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.string\n",
    "\n",
    "            \n",
    "cpdef MEMM_inference_listViterbi(int ps, int L, int M,\n",
    "                                 np.ndarray[dtype=np.float64_t, ndim=1] w,\n",
    "                                 np.ndarray[dtype=np.float64_t, ndim=2] X_node,\n",
    "                                 np.ndarray[dtype=np.float64_t, ndim=3] X_edge):\n",
    "    \"\"\" Inference using the list Viterbi algorithm \"\"\"\n",
    "    assert(L > 1)\n",
    "    assert(M >= L)\n",
    "    assert(ps >= 0)\n",
    "    assert(ps < M)\n",
    "    \n",
    "    cdef int pi, pj, t, k, pk, parix, partition_index, partition_index_start, k_partition_index\n",
    "    cdef long nIter, maxIter = long(1e6)\n",
    "    cdef float loss, priority\n",
    "    \n",
    "    \n",
    "    # forward-backward procedure: adapted from the Rabiner paper\n",
    "    Alpha = np.zeros((L, M), dtype=np.float)  # alpha_t(p_i)\n",
    "    Beta  = np.zeros((L, M), dtype=np.float)  # beta_t(p_i)\n",
    "    lognorm0 = logsumexp([np.dot(w, np.hstack([X_node[ps, :], X_edge[ps, pp, :], X_node[pp, :]])) for pp in range(M)])\n",
    "    \n",
    "    for pj in range(M):\n",
    "        Alpha[1, pj] = np.dot(w, np.hstack([X_node[ps, :], X_edge[ps, pj, :], X_node[pj, :]])) - lognorm0 \\\n",
    "                       if pj != ps else -np.inf\n",
    "    for t in range(2, L):\n",
    "        for pj in range(M): # pi-->(pj fixed)\n",
    "            Alpha[t, pj] = np.max([Alpha[t-1, pi] + \\\n",
    "                                   np.dot(w, np.hstack([X_node[pi, :], X_edge[pi, pj, :], X_node[pj, :]])) - \\\n",
    "                                   logsumexp([np.dot(w, np.hstack([X_node[pi, :], X_edge[pi, pp, :], X_node[pp, :]])) \\\n",
    "                                              for pp in range(M)]) \\\n",
    "                                   if pi not in [pj, ps] else -np.inf for pi in range(M)])\n",
    "    \n",
    "    for t in range(L-1, 1, -1):\n",
    "        for pi in range(M): # (fixed pi)-->pj\n",
    "            lognorm = logsumexp([np.dot(w, np.hstack([X_node[pi,:], X_edge[pi,pp], X_node[pp,:]])) for pp in range(M)])\n",
    "            Beta[t-1, pi] = np.max([Beta[t, pj] + \\\n",
    "                                    np.dot(w, np.hstack([X_node[pi, :], X_edge[pi, pj], X_node[pj, :]])) - lognorm \\\n",
    "                                    if pj not in {pi, ps} else -np.inf for pj in range(M)])\n",
    "    Beta[0, ps] = np.max([Beta[1, pj] + np.dot(w, np.hstack([X_node[ps,:], X_edge[ps,pj], X_node[pj,:]])) - lognorm0 \\\n",
    "                          if pj != ps else -np.inf for pj in range(M)])\n",
    "    \n",
    "    Fp = np.zeros((L-1, M, M), dtype=np.float)  # f_{t, t+1}(p, p')\n",
    "        \n",
    "    for t in range(L-1):\n",
    "        for pi in range(M):\n",
    "            lognormi = logsumexp([np.dot(w, np.hstack([X_node[pi,:], X_edge[pi,pp], X_node[pp,:]])) for pp in range(M)])\n",
    "            for pj in range(M):\n",
    "                Fp[t, pi, pj] = Alpha[t, pi] + Beta[t+1, pj] + \\\n",
    "                                np.dot(w, np.hstack([X_node[pi, :], X_edge[pi, pj], X_node[pj, :]])) - lognormi \\\n",
    "                                if pj not in [pi, ps] else -np.inf\n",
    "    \n",
    "    # identify the best path/walk: adapted from the IJCAI01 paper\n",
    "    y_best = np.ones(L, dtype=np.int) * (-1)\n",
    "    y_best[0] = ps\n",
    "    y_best[1] = np.argmax(Fp[0, ps, :])  # the start POI is specified\n",
    "    for t in range(2, L): \n",
    "        y_best[t] = np.argmax(Fp[t-1, y_best[t-1], :])\n",
    "    \n",
    "    Q = []  # priority queue (min-heap)\n",
    "    with np.errstate(invalid='raise'):  # deal with overflow\n",
    "        try: nIter = np.power(M, L-1) - np.prod([M-kx for kx in range(1,L)]) + 1\n",
    "        except: nIter = maxIter\n",
    "    nIter = np.min([nIter, maxIter])\n",
    "    \n",
    "    # heap item for the best path/walk\n",
    "    priority = -np.max(Alpha[L-1, :])\n",
    "    partition_index = -1\n",
    "    exclude_set = set()  # -1 * score as priority\n",
    "    hq.heappush(Q, HeapItem(priority, (y_best, partition_index, exclude_set)))\n",
    "    \n",
    "    k = 0; y_last = None\n",
    "    while len(Q) > 0 and k < nIter:\n",
    "        hitem = hq.heappop(Q)\n",
    "        k_priority, (k_best, k_partition_index, k_exclude_set) = hitem.priority, hitem.task\n",
    "        k += 1; y_last = k_best\n",
    "        \n",
    "        if len(set(k_best)) == L: \n",
    "            #print(k_priority)\n",
    "            return k_best\n",
    "        \n",
    "        # identify the (k+1)-th best path/walk given the 1st, 2nd, ..., k-th best: adapted from the IJCAI01 paper\n",
    "        partition_index_start = 1\n",
    "        if k_partition_index > 0:\n",
    "            assert(k_partition_index < L)\n",
    "            partition_index_start = k_partition_index\n",
    "            \n",
    "        for parix in range(partition_index_start, L):    \n",
    "            new_exclude_set = set({k_best[parix]})\n",
    "            if parix == partition_index_start:\n",
    "                new_exclude_set = new_exclude_set | k_exclude_set\n",
    "            \n",
    "            # new_best[:parix]\n",
    "            new_best = np.zeros(L, dtype=np.int) * (-1)\n",
    "            new_best[:parix] = k_best[:parix]\n",
    "            if len(set(new_best[:parix])) < parix: # if there's sub-tour(s) in new_best[:parix]\n",
    "                break\n",
    "                \n",
    "            # new_best[parix]\n",
    "            candidate_points = [p for p in range(M) if p not in new_exclude_set]\n",
    "            if len(candidate_points) == 0: continue\n",
    "            candidate_maxix = np.argmax([Fp[parix-1, k_best[parix-1], p] for p in candidate_points])\n",
    "            new_best[parix] = candidate_points[candidate_maxix]\n",
    "            \n",
    "            # new_best[parix+1:]\n",
    "            for pk in range(parix+1, L):\n",
    "                new_best[pk] = np.argmax([Fp[pk-1, new_best[pk-1], p] for p in range(M)])\n",
    "            \n",
    "            new_priority = Fp[parix-1, k_best[parix-1], new_best[parix]]\n",
    "            if k_partition_index > 0:\n",
    "                new_priority += (-k_priority) - Fp[parix-1, k_best[parix-1], k_best[parix]]\n",
    "            new_priority *= -1.0   # NOTE: -np.inf - np.inf + np.inf = nan\n",
    "            \n",
    "            hq.heappush(Q, HeapItem(new_priority, (new_best, parix, new_exclude_set)))\n",
    "            \n",
    "    if k >= nIter: \n",
    "        sys.stderr.write('WARN: reaching max number of iterations, NO optimal solution found, return the last one.\\n')\n",
    "    if len(Q) == 0:\n",
    "        sys.stderr.write('WARN: empty queue, return the last one\\n')\n",
    "    return y_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMM_inference_listViterbi(ps0, L0, M0, w0, f_u, f_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEMM:\n",
    "    def __init__(self, dat_obj, inference_fun=MEMM_inference_listViterbi, C=1.0, poi_info=None, debug=False):\n",
    "        assert(C > 0)\n",
    "        self.C = C\n",
    "        self.inference_fun = inference_fun\n",
    "        self.dat_obj = dat_obj\n",
    "        self.debug = debug\n",
    "        self.trained = False\n",
    "        \n",
    "        if poi_info is None:\n",
    "            self.poi_info = None\n",
    "        else:\n",
    "            self.poi_info = poi_info\n",
    "        \n",
    "        if ABS_SCALER == True:\n",
    "            self.scaler = MaxAbsScaler(copy=False)\n",
    "        else:\n",
    "            self.scaler = MinMaxScaler(feature_range=(-1,1), copy=False)\n",
    "            #self.scaler = StandardScaler(copy=False)\n",
    "        \n",
    "        \n",
    "    def train(self, trajid_set_train):\n",
    "        if self.poi_info is None:\n",
    "            self.poi_info = calc_poi_info(list(trajid_set_train), traj_all, poi_all)\n",
    "        \n",
    "        # build POI_ID <--> POI__INDEX mapping for POIs used to train CRF\n",
    "        # which means only POIs in traj such that len(traj) >= 2 are included\n",
    "        poi_set = {p for tid in trajid_set_train for p in self.dat_obj.traj_dict[tid] \\\n",
    "                   if len(self.dat_obj.traj_dict[tid]) >= 2}\n",
    "        self.poi_ix = sorted(poi_set)\n",
    "        self.poi_id_dict, self.poi_id_rdict = dict(), dict()\n",
    "        for idx, poi in enumerate(self.poi_ix):\n",
    "            self.poi_id_dict[poi] = idx\n",
    "            self.poi_id_rdict[idx] = poi        \n",
    "\n",
    "        # generate training data       \n",
    "        train_traj_list = [self.dat_obj.traj_dict[x] for x in trajid_set_train if len(self.dat_obj.traj_dict[x]) >= 2]\n",
    "        node_features_list = Parallel(n_jobs=N_JOBS)\\\n",
    "                             (delayed(calc_node_features)\\\n",
    "                              (tr[0], len(tr), self.poi_ix, self.poi_info, self.dat_obj) for tr in train_traj_list)\n",
    "        edge_features = calc_edge_features(list(trajid_set_train), self.poi_ix, self.poi_info, self.dat_obj)       \n",
    "        assert(len(train_traj_list) == len(node_features_list))\n",
    "        \n",
    "        self.fdim = node_features_list[0].shape\n",
    "        \n",
    "        # feature scaling\n",
    "        # should each example be flattened to one vector before scaling?\n",
    "        # It seems the performance is better if we don't flatten before scaling\n",
    "        X_node_all = np.vstack(node_features_list)\n",
    "        #X_node_all = X_node_all.reshape(len(node_features_list), -1) # flatten every example to a vector\n",
    "        X_node_all = self.scaler.fit_transform(X_node_all)\n",
    "        #X_node_all = X_node_all.reshape(-1, self.fdim[1])\n",
    "        \n",
    "        self.X_edge = edge_features\n",
    "        y_train = [np.array([self.poi_id_dict[x] for x in tr]) for tr in train_traj_list]\n",
    "\n",
    "        if self.debug == True: print('C:', self.C)\n",
    "        w = np.random.rand(X_node_all.shape[1] * 2 + self.X_edge.shape[2])  # initial guess\n",
    "        opt_method = 'BFGS' # 'Newton-CG'\n",
    "        options = {'disp': True} if self.debug == True else dict()\n",
    "        #opt = minimize(MEMM_1MC_cost, w, args=(X_node_all, self.X_edge, y_train, self.C, len(self.poi_ix)), \\\n",
    "        #               method=opt_method, jac=MEMM_1MC_grad, options=options)\n",
    "        opt = minimize(MEMM_1MC_obj, w, args=(X_node_all, self.X_edge, y_train, self.C, len(self.poi_ix)), \\\n",
    "                       method=opt_method, jac=True, options=options)\n",
    "        if opt.success == True:\n",
    "            self.w = opt.x\n",
    "            self.trained = True\n",
    "        else:\n",
    "            sys.stderr.write('Optimisation failed, C=%f\\n' % self.C)\n",
    "            self.trained = False\n",
    "        return self.trained\n",
    "       \n",
    "    \n",
    "    def predict(self, startPOI, nPOI):\n",
    "        assert(self.trained == True)\n",
    "        if startPOI not in self.poi_ix: return None\n",
    "        X_node_test = calc_node_features(startPOI, nPOI, self.poi_ix, self.poi_info, self.dat_obj)\n",
    "        \n",
    "        # feature scaling\n",
    "        # should each example be flattened to one vector before scaling?\n",
    "        assert(X_node_test.shape == self.fdim)\n",
    "        #X_node_test = X_node_test.reshape(1, -1) # flatten test example to a vector\n",
    "        X_node_test = self.scaler.transform(X_node_test)\n",
    "        #X_node_test = X_node_test.reshape(self.fdim)\n",
    "        \n",
    "        y_hat = self.inference_fun(self.poi_id_dict[startPOI], nPOI, len(self.poi_ix), self.w, X_node_test, self.X_edge)\n",
    "        \n",
    "        return np.array([self.poi_id_rdict[x] for x in y_hat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested cross-validation with Monte-Carlo cross-validation as inner loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recdict_memm1m = dict()\n",
    "cnt = 1\n",
    "keys = sorted(dat_obj.TRAJID_GROUP_DICT.keys())\n",
    "\n",
    "# outer loop to evaluate the test performance by cross validation\n",
    "for i in range(len(keys)):\n",
    "    ps, L = keys[i]\n",
    "    \n",
    "    best_C = 1\n",
    "    #best_F1 = 0; best_pF1 = 0\n",
    "    best_Tau = 0\n",
    "    keys_cv = keys[:i] + keys[i+1:]\n",
    "    \n",
    "    # use all training+validation set to compute POI features, \n",
    "    # make sure features do NOT change for training and validation\n",
    "    trajid_set_i = set(dat_obj.trajid_set_all) - dat_obj.TRAJID_GROUP_DICT[keys[i]]\n",
    "    poi_info_i = dat_obj.calc_poi_info(list(trajid_set_i))\n",
    "    \n",
    "    poi_set_i = {p for tid in trajid_set_i for p in dat_obj.traj_dict[tid] if len(dat_obj.traj_dict[tid]) >= 2}\n",
    "    if ps not in poi_set_i: \n",
    "        sys.stderr.write('start POI of query %s does not exist in training set.\\n' % str(keys[i]))\n",
    "        continue\n",
    "    \n",
    "    # tune regularisation constant C\n",
    "    for logit_C in C_SET:\n",
    "        print('\\n--------------- try_C: %f ---------------\\n' % logit_C); sys.stdout.flush() \n",
    "        F1_memm = []; pF1_memm = []; Tau_memm = []        \n",
    "        \n",
    "        # inner loop to evaluate the performance of a model with a specified C by Monte-Carlo cross validation\n",
    "        for j in range(MC_NITER):\n",
    "            poi_list = []\n",
    "            while True: # make sure the start POI in test set are also in training set\n",
    "                rand_ix = np.arange(len(keys_cv)); np.random.shuffle(rand_ix)\n",
    "                test_ix = rand_ix[:int(MC_PORTION*len(rand_ix))]\n",
    "                assert(len(test_ix) > 0)\n",
    "                trajid_set_train = set(dat_obj.trajid_set_all) - dat_obj.TRAJID_GROUP_DICT[keys[i]]\n",
    "                for j in test_ix: \n",
    "                    trajid_set_train = trajid_set_train - dat_obj.TRAJID_GROUP_DICT[keys_cv[j]]\n",
    "                poi_set = {poi for tid in trajid_set_train for poi in dat_obj.traj_dict[tid]}\n",
    "                good_partition = True\n",
    "                for j in test_ix: \n",
    "                    if keys_cv[j][0] not in poi_set: good_partition = False; break\n",
    "                if good_partition == True:\n",
    "                    poi_list = sorted(poi_set)\n",
    "                    break\n",
    "            \n",
    "            # train\n",
    "            memm = MEMM(dat_obj, C=logit_C, poi_info=poi_info_i.loc[poi_list].copy(), debug=True)\n",
    "            if memm.train(trajid_set_train) == True:\n",
    "                for j in test_ix:  # test\n",
    "                    ps_cv, L_cv = keys_cv[j]\n",
    "                    y_hat = memm.predict(ps_cv, L_cv)\n",
    "                    if y_hat is not None:\n",
    "                        F1, pF1, tau = evaluate(dat_obj, keys_cv[j], y_hat)\n",
    "                        F1_memm.append(F1); pF1_memm.append(pF1); Tau_memm.append(tau)\n",
    "            else:  # if training is failed\n",
    "                for j in test_ix:\n",
    "                    F1_memm.append(0); pF1_memm.append(0); Tau_memm.append(0)\n",
    "        \n",
    "        #mean_F1 = np.mean(F1_memm); mean_pF1 = np.mean(pF1_memm)\n",
    "        mean_Tau = np.mean(Tau_memm)\n",
    "        print('mean_Tau: %.3f' % mean_Tau)\n",
    "        if mean_Tau > best_Tau:\n",
    "            best_Tau = mean_Tau\n",
    "            best_C = logit_C\n",
    "    print('\\n--------------- %d/%d, Query: (%d, %d), Best_C: %f ---------------\\n' % (cnt, len(keys), ps, L, best_C))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # train model using all examples in training+validation set and measure performance on test set\n",
    "    memm = MEMM(dat_obj, C=best_C, poi_info=poi_info_i, debug=True)\n",
    "    if memm.train(trajid_set_i) == True:\n",
    "        y_hat = memm.predict(ps, L)\n",
    "        print(y_hat)\n",
    "        if y_hat is not None:\n",
    "            recdict_memm1m[(ps, L)] = {'PRED': y_hat, 'W': memm.w, 'C': memm.C}\n",
    "        \n",
    "    cnt += 1; #print_progress(cnt, len(keys)); sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmemm = os.path.join(dat_obj.data_dir, 'memm-' + dat_obj.dat_suffix[dat_ix] + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(recdict_memm1m, open(fmemm, 'bw'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(recdict_memm1m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_evaluation(dat_obj, recdict_memm1m)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
